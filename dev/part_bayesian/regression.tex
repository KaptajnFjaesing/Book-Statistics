\chapter{Regression}
\label{chp:regression}
Regression involves the Robot building a model, $f: \Omega_W\times \Omega_X\mapsto\mathbb{R}$, with associated parameters $w\in \Omega_W$, that estimates Nature's actions $S$ based on observed data $X$. Note that the output of $f$ is $\mathbb{R}$ implying that $S$ is assumed continuos. The model $f$ acts as a proxy for the Robot in that it on behalf of the Robot estimates the action of Nature given an input. Hence, in providing an estimate, the model must make a choice, similar to the Robot and thus the Robot must pick a cost function for the model. In this study, the quadratic cost function from \dfref{def:quadratic_cost} will be considered to review the subject. From \thref{theorem:expectation_decision_rule} the best action for the Robot can be written
\begin{equation}
	U^*(x) = \int ds s p(s|x,D,I)
	\label{eq:q1}
\end{equation}
Assuming the actions of Nature follow a normal distribution with the function $f$ as mean and an unknown variance, $\xi\in \Omega_W$
\begin{equation}
	p(s|x,w,\xi,I)=\sqrt{\frac{\xi}{2\pi}} e^{-\frac{\xi}{2}(f(w,x)-s)^2}.
	\label{f_dist}
\end{equation}
Using \EQref{f_dist} and marginalizing over $\xi,w$
\begin{equation}
	\begin{split}
		p(s|x,D,I) &= \int p(s,w,\xi|x,D,I) dw d\xi\\
		& = \int p(s|x,w,\xi,D,I)  p(w,\xi|x,D,I)dw d\xi\\
		& = \int p(s|x,w,\xi,I)  p(w,\xi|D,I)dw d\xi,\\
	\end{split}
	\label{eq:q2}
\end{equation}
where it has been used that $p(s|w,\xi,x,D,I) = p(s|w,\xi,x,I)$ since by definition $f$ produce a $1-1$ map of the input $x$ (\EQref{f_dist}) and $p(w,\xi|x,D,I) = p(w,\xi|D,I)$ from \axref{ax:observation_relevance}. Using \EQref{eq:q2} in \EQref{eq:q1}\footnote{Note that a function of a random variable is itself a random variable, so $f$ is a random variable.}
\begin{equation}
	\begin{split}
		U^*(x) & = \int f(w,x)  p(w,\xi|D,I) dw d\xi,\\
		& = \mathbb{E}[f|x,D,I]
	\end{split}
	\label{eq:q3}
\end{equation}	
where it has been used that
\begin{equation}
	\begin{split}
		\mathbb{E}[S|x,w,\xi,I] &= \int s p(s|x,w,\xi,I) dy\\
		&= f(w,x)
	\end{split}
\end{equation}
according to \EQref{f_dist}. Using Bayes theorem (\thref{theorem:bayes_theorem})
\begin{equation}
	p(w,\xi|D,I) = \frac{p(D_s|D_x,w,\xi,I)p(w,\xi|D_x,I)}{p(D_s|D_x,I)}
	\label{eq:bayes2}
\end{equation}
where from marginalization (\thref{theorem:law_of_total_probability})
\begin{equation}
	p(D_s|D_x,I) = \int p(D_s|D_x,w,\xi,I)p(w,\xi|D_x,I) dw d\xi.
\end{equation}
Assuming the past actions of Nature are independent and identically distributed, the likelihood can be written (using equation \EQref{f_dist})
\begin{equation}
	p(D_s|D_x,w,\xi,I) = \bigg(\frac{\xi}{2\pi}\bigg)^\frac{n}{2}\prod_{i=1}^n e^{-\frac{\xi}{2}(f(w,x_i)-s_i)^2}
	\label{reg:likelihood}
\end{equation}
From the chain rule (see \thref{theorem:chain_rule}) and \thref{ax:observation_relevance}
\begin{equation}
		p(w,\xi|D_x,I) = p(w|\xi,I)p(\xi|I).
\end{equation}
Assuming the distributions of the $w$'s are i) independent of $\xi$ and ii) normally distributed\footnote{The normally distributed prior is closely related to weight decay~\citep{Plaut1986}, a principle conventionally used in frequentist statistics to avoid the issue of overfitting.} with zero mean and a precision described by a hyperparameter, $\lambda$. 	 
\begin{equation}
	\begin{split}
		p(w|\xi,I) & = p(w|I)\\
		& = \int p(w|\lambda,I)p(\lambda|I)d\lambda
	\end{split}
	\label{eq:prior1}
\end{equation}
The precision is constructed as a wide gamma distribution\index{Gamma distribution} so as to approximate an objective prior
\begin{equation}
	p(w|\lambda,I)p(\lambda|I)
	= \prod_{q=1}^{\tilde{n}} \frac{\lambda_q^\frac{n_q}{2}}{(2\pi)^\frac{n_q}{2}}e^{-\frac{\lambda_q}{2}\sum_{l=1}^{n_q}w_l^2}\frac{\beta_q^{\alpha_q}}{\Gamma(\alpha_q)}\lambda_q^{\alpha_q-1}e^{-\beta_q \lambda_q}
	\label{eq:prior}
\end{equation}
where $\alpha_q,\beta_q$ are prior parameters (a part of the background information) and $\tilde{n}$ is the number of hyper parameters. In the completely general case $\tilde{n}$ would equal the number of parameters $w$, such that each parameter has an independent precision. In practice, the Robot may consider assigning some parameters the same precision, e.g. for parameters in the same layer in a neural network. Since $p(\xi|I)$ is analogous to $p(\lambda|I)$ -- in that both are prior distributions for precision parameters -- $p(\xi|I)$ is assumed to be a wide gamma distribution, then
\begin{equation}
	\begin{split}
		p(\xi|I) & = \text{Ga}(\xi|\tilde{\alpha},\tilde{\beta})\\
		& =\frac{\tilde{\beta}^{\tilde{\alpha}}}{\Gamma(\tilde{\alpha})}\xi^{\tilde{\alpha}-1}e^{-\tilde{\beta} \xi}.
	\end{split}
	\label{p7}
\end{equation}
At this point equation \EQref{eq:q1} is fully specified (the parameters $\alpha,\beta,\tilde{\alpha},\tilde{\beta}$ and the functional form of $f(w,x)$ are assumed specified as part of the background information) and can be approximated by obtaining samples from $p(w,\xi,\lambda|D,I)$ via HMC~\citep{Hammersley1964,Duane:1987de,Neal:1996,Neal2012} (see \appref{app:HMC} for a review of HMC). The centerpiece in the HMC algorithm is the Hamiltonian defined viz~\citep{Neal:1996,Neal2012}
\begin{equation}
	H \equiv  \sum_{q=1}^{\tilde{n}}\sum_{l=1}^{n_q}\frac{p_{l}^2}{2m_{l}}-\ln[p(w,\xi,\lambda|D,I)]+const,
	\label{eqh}
\end{equation}
where 
\begin{equation}
	p(w,\xi|D,I) = \int d\lambda p(w,\xi,\lambda|D,I).
	\label{eq:ss}
\end{equation}
Besides its function in the HMC algorithm, the Hamiltonian represent the details of the Bayesian model well and should be a familiar sight for people used to the more commonly applied frequentist formalism\index{Frequentist statistics} (since, in this case, it is in form similar to a cost function comprised of a sum of squared errors, weight decay on the coefficients and further penalty terms~\citep{hastie_09,murphy2013machine,Goodfellow2016}). Using \EQref{eq:bayes2}-\EQref{eq:ss} yields
\begin{equation}
	\begin{split}
		H&=\sum_{q=1}^{\tilde{n}}\sum_{l=1}^{n_q}\frac{p_{l}^2}{2m_{l}}+\frac{n}{2}[\ln(2\pi)-\ln(\xi)] +\frac{\xi}{2}\sum_{i=1}^{n}(f(w,x_i)-s_i)^2\\
		&\quad+\sum_{q=1}^{\tilde{n}}\bigg(\ln(\Gamma(\alpha_q))-\alpha_q\ln(\beta_q)+(1-\alpha_q)\ln(\lambda_q)+\beta_q\lambda_q\\
		&\qquad\qquad+\frac{n_q}{2}(\ln(2\pi)-\ln(\lambda_q))+\frac{\lambda_q}{2}\sum_{l=1}^{n_q}w_l^2\bigg)\\
		&\quad+\ln(\Gamma(\tilde{\alpha}))-\tilde{\alpha}\ln(\tilde{\beta})+(1-\tilde{\alpha})\ln(\xi)+\tilde{\beta}\xi+const.
	\end{split}
	\label{eqh2}
\end{equation}
