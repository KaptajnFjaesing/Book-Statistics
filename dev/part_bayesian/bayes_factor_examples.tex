\begin{example}
	\index{Example: Bayes factor}
	\emph{Suppose the robot considers a coin is experiment\index{Coin experiment}. The Robot is told that a coin has been tossed $n=10$ times and $h=9$ heads have been observed. Let the null hypothesis be that the coin is fair, and the alternative hypothesis be that the coin can have any bias, so $p(\theta) = \text{Unif}(\theta|a=0,b=1)$.}\newline
	
	\begin{enumerate}
		\item \emph{Derive Bayes factor}
		\begin{equation}
			\text{BF}\equiv \frac{p(D|M_\text{biased coin},I)}{p(D|M_\text{fair coin},I)}.
		\end{equation}
		
		\begin{equation}
			\begin{split}
				p(D_s|M_\text{biased coin},I) &= p(D_s|\text{biased coin},I)\\
				&=\int p(h|n,\theta,I)p(\theta|I)\\
				&=\frac{1}{N+1},\\
				p(D|M_\text{fair coin},I) &= p(D_s|\text{fair coin},I)\\
				&=\int p(h|n,\theta,I)\delta(\theta-\frac{1}{2})\\
				&=\begin{pmatrix}
					n\\
					h
				\end{pmatrix}\theta^h(1-\theta)^{n-h}|_{\theta=\frac{1}{2}}\\
				& = \begin{pmatrix}
					n\\
					h
				\end{pmatrix}\frac{1}{2^n},
			\end{split}
		\end{equation}
		where $\begin{pmatrix}
			n\\
			h
		\end{pmatrix}$ is the binomial coefficient,	hereby
		\begin{equation}
			\begin{split}
				\text{BF} &= \frac{2^n}{(n+1)\begin{pmatrix}
						n\\
						h
				\end{pmatrix}}\\
				&\simeq 9.3
			\end{split}
		\end{equation}
		
		\item \emph{What if $n=100$ and $h=90$?}
		\begin{equation}
			\begin{split}
				\text{BF}	&\simeq 7.25\cdot 10^{14}
			\end{split}
		\end{equation}
		The coin is unlikely to be fair in either case, but increasingly unlikely when there is more evidence in favor of this hypothesis.
	\end{enumerate}
\end{example}

\begin{example}
	\index{Example: Bayes factor2}
	Consider the case where $x \in [0,1]$ and the distirbution of $\tilde{x}_j$ follow a truncated (since the uncerainty $\delta x$ is symmetric about $x_j$) normal distribution\index{Normal distribution} with mean $x_j$ and uncertainty $\delta x_j$\label{ex:BF1}
	\begin{equation}
		p(x_j|A,\delta x_j, \tilde{x}_j, I) = \begin{cases}
			\frac{1}{\sqrt{2\pi}\delta x_j}e^{-\frac{1}{2}\big(\frac{x_j-\tilde{x}_j}{\delta x_j}\big)^2} & \text{for  } x_j\in [0,1]\\
			0 & \text{Otherwise} 
		\end{cases}.
	\end{equation} 
	Hereby
	\begin{equation}
		I = \prod_{j=1}^n\int p(x_j|A,\delta x_j, \tilde{x}_j, I)p(\tilde{x}_j|A,\theta,\delta x, I)p(\theta|A,\delta x^{(1:n)},I) d\tilde{x}_j d\theta,
	\end{equation}
	where
	\begin{equation}
		\begin{split}
			p(\theta|A,\delta x^{(1:n)},I)
			& =  p(\theta|A,I)
		\end{split}.
	\end{equation}
	A prerequisite for evaluating the integral is assigning probabilities $p(\theta|A,I)$ and $p(\tilde{x}_j|A,\theta,\delta x, I)$. $p(\theta|A,I)$ represents the prior belief about the parameters $\theta$ whereas $p(\tilde{x}_j|A,\theta,\delta x, I)$ has to capture the nature of the data given the hypothesis, parameters, uncertainty of $x$ and background information. 
	
	\paragraph{Gaussian Approximation:} As a first approximation to the scenario in example \ref{ex:BF1} the Gaussian approximation can be considered. In this approximation the limited support of $x$ is neglected and the distribution of $\kappa$ is assumed to be symmetric around a single peak defined by the mean $\mu$, standard deviation $\sigma$. Example \ref{ex:gauss} show how the Gaussian distribution can be derived from these assumptions using the principle of maximum entropy\index{Maximum entropy}, meaning\label{ex:gauss2}
	\begin{equation}
		p(\tilde{x}_j|A,\theta,\delta x, I) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\big(\frac{\tilde{x}_j-\mu}{\sigma}\big)^2}
	\end{equation}
	where $\theta = \{\sigma,\mu\}$. Since the Gaussian distribution is the conjugate prior distribution of the Gaussian distribution
	\begin{equation}
		\begin{split}
			p(x^{(1:n)}|A,\theta,\delta x^{(1:n)}, I) &\equiv \prod_{j=1}^{n}\int p(\tilde{x}_j|A,\theta,\delta x, I)p(x_j|A,\delta x_j, \tilde{x}_j, I)d \tilde{x}_j\\
			&= \prod_{j=1}^n\frac{1}{(\sigma+\delta x_j)\sqrt{2\pi}}e^{-\big(\frac{x_j-\mu}{\sqrt{2}(\sigma+\delta x_j)}\big)^2}
		\end{split}
		\label{likelihood1}
	\end{equation}
	in this case. Assuming a uniform prior on both the mean and standard deviation
	\begin{equation}
		p(\theta|A,I) = \frac{1}{(\mu_{max}-\mu_{min})(\sigma_{max}-\sigma_{min})}.
		\label{prior1}
	\end{equation}
	Using equations \eqref{prior1} and \eqref{likelihood1} the integral, $I$ can be written
	\begin{equation}
		I = \frac{\int \prod_{j\in x^{(1:n)}}\frac{1}{(\sigma+\delta x_j)\sqrt{2\pi}}e^{-\big(\frac{x_{j}-\mu}{\sqrt{2}(\sigma+\delta x_j)}\big)^2} d\mu d\sigma}{(\mu_{max}-\mu_{min})(\sigma_{max}-\sigma_{min})}.
	\end{equation}
	The likelihood can be evaluated numerically via importance sampling. Alternatively an analytical solution can be obtained by i) assuming $\delta x_j\ll \sigma$ and ii) Taylor expanding the logarithm of the likelihood around a maximum defined by $\mu_0,\sigma_0$~\citep{Sivia2006}
	\begin{equation}
		L = L(\mu_0,\sigma_0)-\frac{1}{2}\begin{bmatrix}
			\Delta\mu & \Delta\sigma
		\end{bmatrix}
		\begin{bmatrix}
			\alpha & \gamma\\
			\gamma & \beta \\
		\end{bmatrix}\begin{bmatrix}
			\Delta\mu\\ \Delta\sigma
		\end{bmatrix}
		+\mathcal{O}(\Delta\mu^2),
		\label{eq:li}
	\end{equation}
	where $\Delta \mu = \mu-\mu_0$ and $\Delta\sigma = \sigma-\sigma_0$. $\mu_0$ and $\sigma_0$ are determined from requiring $\frac{\partial L}{\partial \mu}=\frac{\partial L}{\partial \sigma}=0$
	\begin{equation}
		\begin{split}
			\mu_0 &= \frac{1}{\tilde{N}}\sum_{j}x_j,\\
			\sigma_0 &= \frac{1}{\tilde{N}}\sum_j(x_j-\mu_0)^2.
		\end{split}
		\label{eq:s}	
	\end{equation}
	$\alpha,\beta$ and $\gamma$ can be determined by evaluating the second order partial derivatives at the maximum
	\begin{equation}
		\begin{split}
			\alpha & = \frac{\tilde{N}}{\sigma_0^2}\\
			\beta & = 2\alpha\\
			\gamma & = 0.
		\end{split}
		\label{eq:a}
	\end{equation} 
	The results of equations \eqref{eq:li},\eqref{eq:s} and \eqref{eq:a} are understood to apply for all distributions.
	Hereby
	\begin{equation}
		\begin{split}
			I &\approx \frac{e^{L(\mu_0,\sigma_0)}\tilde{I}}{(\mu_{max}-\mu_{min})(\sigma_{max}-\sigma_{min})} \\
			&\approx \frac{e^{L(\mu_0,\sigma_0)}}{(\mu_{max}-\mu_{min})(\sigma_{max}-\sigma_{min})}\frac{2\pi}{\sqrt{\alpha\beta}}\\
			&=\frac{(\sigma_0 \sqrt{2\pi})^{2-\tilde{N}}e^{-\frac{\tilde{N}}{2}}}{\tilde{N}\sqrt{2}(\mu_{max}-\mu_{min})(\sigma_{max}-\sigma_{min})}
		\end{split}
	\end{equation}
	where
	\begin{equation}
		\tilde{I} = \int_{\mu_{min}}^{\mu_{max}}d\mu\int_{\sigma_{min}}^{\sigma_{max}}d\sigma e^{-\frac{1}{2}(\alpha\Delta\mu^2+\beta\Delta\sigma^2)}.
	\end{equation}
	The integrals related to hypothesis B are computed analogously yielding the Bayes factor
	\begin{equation}
		\begin{split}
			\rm BF &\sim  \frac{(\mu_{max}-\mu_{min})(\sigma_{max}-\sigma_{min})}{\pi \sqrt{2}}\frac{\tilde{N}_1\tilde{N}_2}{\tilde{N}}\frac{\sigma_0^{2-\tilde{N}}}{\sigma_{0,1}^{2-\tilde{N}_1}\sigma_{0,2}^{2-\tilde{N}_2}},
		\end{split}
	\end{equation}
	where the second index in $\sigma_{0,s}$ denotes the value of $s$.
	
	\paragraph{Beta Approximation:} The advantage of the Gaussian approximation in example \ref{ex:gauss2} is that it yields an analytical soltuion. The disadvantage is that it does not properly account for the support and possible morphology of the distribution of $x$. An improved approximation is to take the limited support of $x$ into account and allow for asymmetry in the distribution. Example \ref{ex:beta} show how the beta distribution can be derived from these assumptions using the principle of maximum entropy\index{Maximum entropy}, meaning\label{sec:beta}
	\begin{equation}
		p(\tilde{x}_j|A,\theta, I) = \frac{1}{B(\alpha,\beta)} \tilde{x}_j^{\alpha-1}(1-\tilde{x}_j)^{\beta-1}
		\label{eq:p2}
	\end{equation}
	where $\theta =\{\alpha,\beta\}$ and $B(\alpha,\beta)$ denotes the beta function. Assuming again a uniform prior on the parameters of the beta distribution ($\alpha,\beta$)
	\begin{equation}
		p(\theta|A,I) = \frac{1}{(\alpha_{max}-\alpha_{min})(\beta_{max}-\beta_{min})}.
		\label{prior2}
	\end{equation}
	Given equations \eqref{prior2} and \eqref{eq:p2} the integral, $I$, can be evaluated numerically via importance sampling. The dimensionality of the integral in $I$ is $n+2$, meaning the complexity is dominated by (and scale with) the dimensionality of data. For this reason, it is worth considering approximating $p(x^{(1:n)}|A,\theta,\delta x^{(1:n)}, I)$. In the case of the beta distribution 
	\begin{equation}
		p(x^{(1:n)}|A,\theta,\delta x^{(1:n)}, I) =\frac{1}{B(\alpha,\beta)^n}\prod_{j=1}^{n}\frac{I_1}{\sqrt{2\pi \delta} x_j}
		\label{likelihood2}
	\end{equation}
	with
	\begin{equation}
		I_1 = \int_0^1e^{-\frac{1}{2}\big(\frac{x_j-\tilde{x}_j}{\delta x_j}\big)^2+\ln(B(\alpha,\beta) p(\tilde{x}_j|A,\theta, I))}d\tilde{x}_j.
	\end{equation}
	The exponent\footnote{The reason why the exponent is expanded rather than the integrand itself is that the latter will allow for a negative value of the integral.} can be Taylor expanded
	\begin{equation}
		\begin{split}
			-\frac{1}{2}\big(\frac{x_j-\tilde{x}_j}{\delta x_j}\big)^2+\ln(B(\alpha,\beta) p(\tilde{x}_j|A,\theta, I)) = \sum_{i=0}^{1} a_i\Delta x_j^i+\mathcal{O}(\Delta x_j^2)
		\end{split}
	\end{equation}
	with $\Delta x_j \equiv \tilde{x}_j-x_j$ and
	\begin{equation}
		\begin{split}
			a_0 & = (\alpha-1)\ln(x_j)+(\beta-1)\ln(1-x_j),\\
			a_1 & = \frac{\alpha-1}{x_j}+\frac{\beta-1}{x_j-1}.\\
		\end{split}
	\end{equation}
	Hereby
	\begin{equation}
		\begin{split}
			I_1 &\approx e^{a_0-a_1x_j}\int_0^1e^{a_1\tilde{x}_j}d\tilde{x}_j\\
			&= \frac{e^{a_0-x_j a_1}(e^{a_1}-1)}{a_1}
		\end{split}
	\end{equation}
	and
	\begin{equation}
		p(x^{(1:n)}|A,\theta,\delta x^{(1:n)}, I) \approx\frac{1}{B(\alpha,\beta)^n}\prod_{j=1}^{n}\frac{1}{\sqrt{2\pi \delta} x_j}\frac{e^{a_0-x_j a_1}(e^{a_1}-1)}{a_1}.
		\label{likelihood3}
	\end{equation}
\end{example}