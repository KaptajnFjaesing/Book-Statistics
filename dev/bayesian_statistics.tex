\chapter{Bayesian Statistics}
\label{chp:bay}
Bayesian statistics is based on definition \ref{def:bayesian_statistics}, where probability is viewed as a subjective degree of belief in the likelihood of an event occurring and parameters are modeled as random variables. The Bayesian framework originally come from the work of \citet{Bayes:63} and \citet{laplace_thorie_1812} with much of the modern discussions and formalism created later by \citet{Finetti1937LaP,Jeffreys1940} and \citet{Savage1954}. Although Bayesian statistics adopts a subjectivistic interpretation of probability, given established information the derived probabilities follow deductively from probability theory. To see this, consider a statistical experiment described by the statistical model defined by the tuple $(\Omega, \mathcal{F}, \mathbb{P},\mathbb{W},\mathcal{P})$, where $(\Omega, \mathcal{F}, \mathbb{P})$ defines the probability space (see chapter \ref{chp:probaiblity_theory}), $\mathbb{W}$ is the parameter space and $\mathcal{P}=\{p: w \in \mathbb{W}\}$ (unified notation, see section \ref{sec:notation}) is the set of distributions (see figure \ref{fig:statistics_theory_venn_diagram}). Associated to the sample space $\Omega$ are a set of random variables that map the outcomes of the experiment to numerical values, $X:\Omega \mapsto \mathbb{X}$ with $x\in \mathbb{X}$, $Y:\Omega \mapsto \mathbb{Y}$ with $y\in \mathbb{Y}$,... Given $n$ observations $X=x_1, X=x_2,\dots X = x_n$ and the background information\index{Background information} $I$, the probability of the $j$'th parameter $w_j$ taking a specific value can be written
\begin{equation}
	p(W_j=w_j|X=x_1, X=x_2,\dots X = x_n, I),
	\label{eq:pa1}
\end{equation}
where $W_j$ denotes that $j$'th element in the random variable vector $W$. Let $W_j=w_j\rightarrow w_j$ and $D$ denote any collection of observations, then 
\begin{equation}
	p(W=w_j|X=x_1, X=x_2,\dots X = x_n, I) = p(w_j|D,I)
\end{equation}
In order to evaluate $p(w_j|D,I)$, the probability needs to be unpacked. This is done by using a combination of marginalization, Bayes theorem and the chain rule (see chapter \ref{chp:probaiblity_theory})
\begin{equation}
		p(w_j|D,I) = \frac{p(D|w_j,I)p(w_j|I)}{p(D|I)},
	\label{eq:pa2}
\end{equation}
where $p(D|I)$ can be expanded via marginalization, $p(w_j|I)$ is called the prior distribution for $w_j$, $p(D|w_j,I)$ the likelihood and $p(w_j|D,I)$ the posterior distribution. In case $w_j$ is discrete $p(D|I) = \sum_{i}p(D|w_{j,i},I)p(w_{j,i}|I)$ and in case $w_j$ is continuous $p(D|I) = \int dw_j p(D|w_j,I)p(w_j|I)$. The prior distribution depends on parameters that has to be specified and by definition cannot be learned from data (since the prior is the belief before observing data). The parameters are included in the background information, $I$.\newline
From equations \eqref{eq:pa2} it is clear that, given assigned and specified probability distributions the probability of a parameter taking a specific value follow deductively from probability theory. The subjectivity comes from the assignment and specification of probability distributions. 

\begin{example}
	Consider a case where a coin is flipped and the probability for heads, $\theta\in \mathbb{W}$, is analyzed. Suppose there has been collected data, $D$, from $n$ coin flips with $h$ heads, the probability of heads given observed data and background information can then be written
	\begin{equation}
		p(\theta|D,I) = \frac{p(D|\theta,I)p(\theta|I)}{\int d\theta p(D|\theta,I)p(\theta|I)}.
	\end{equation} 
	Take $p(D|\theta,I)=\text{Bin}(h|n,\theta)$ and $p(\theta|I)=\text{Beta}(\theta|a,b)$, meaning
	\begin{equation}
		\begin{split}
			p(\theta|D,I)&= \frac{\frac{\theta^{a-1}(1-\theta)^{b-1}}{\text{B}(a,b)} \begin{pmatrix}
					n\\
					h\\
				\end{pmatrix}\theta^h(1-\theta)^{n-h}}{\begin{pmatrix}
					n\\
					h\\
				\end{pmatrix}\frac{1}{B(a,b)}\int d\theta \theta^{h+a-1}(1-\theta)^{n-h+b-1}}\\
			& = \text{Beta}(\theta|a+h,b+n-h)
		\end{split}
		\label{eq:beta14}
	\end{equation}
	From equation \eqref{eq:beta14} it is clear that a) the posterior distribution and prior distributions are of the same type but with different parameters and b) the parameters of the prior enter as fake counts of heads ($a$) and tails ($b$). In general the posterior is not the same type of distribution as the prior. In cases where this is true, the prior and posterior are said to be conjugate\index{Conjugate prior}. If additional data, $D'$, with $n'$ coin flips and $h'$ heads are collected, the entire computation can either be started over with $D\rightarrow D+D'$
	\begin{equation}
			p(\theta|D,D',I) = \frac{p(D,D'|\theta,I)p(\theta|I)}{\int d\theta p(D,D'|\theta,I)p(\theta|I)}
	\end{equation}
	This is equivalent to increasing $n$ and $h$ in equation \eqref{eq:beta14} (intuitively, this is clear since data can be sliced arbitrarily) with the amounts specified by $D'$.	Hence, for a fixed and finite choice of prior (fixed and finite $a$ and $b$), the prior will gradually loose impact as data is collected. Given the ratio of $n$ to $h$ stay fixed, the beta distribution will narrow as more data is gathered, representing decreased uncertainty of the true value.
\end{example}

\begin{example}
	\index{Example: Posterior of coin toss experiment}
	\emph{Suppos we toss a coin\index{Coin experiment} $n=5$ times. Let $h$ be the number of heads. We observe fewer than $3$ heads, but we don't know exactly how many. Let the prior probability of heads be $p(\theta)=\text{Beta}(\theta|a=1,b=1)$. Compute the posterior $p(\theta|h<3, n=5)$.}
	
	\begin{equation}
		p(\theta|h<3,n=5) = \frac{p(h<3,n=5|\theta)p(\theta)}{p(h<3,n=5)}
	\end{equation}
	where $p(\theta)=p(\theta|a=1,b=1)\propto [\theta^{a-1}(1-\theta)^{b-1}]|_{a=1,b=1}=1$. Now
	\begin{equation}
		\begin{split}
			p(h<3,n=5|\theta) & =\sum_{i=0}^2p(h=i,n=5|\theta)\\
			&= \begin{pmatrix}
				5 \\ 0 
			\end{pmatrix}(1-\theta)^5+\begin{pmatrix}
				5 \\ 1 
			\end{pmatrix}\theta ^1(1-\theta)^4+\begin{pmatrix}
				5 \\ 2 
			\end{pmatrix}\theta ^2(1-\theta)^3
		\end{split}
	\end{equation}
	Since $\int_0^1d\theta \begin{pmatrix}
		n \\ k 
	\end{pmatrix}\theta^k(1-\theta)^{n-k} = 1$
	\begin{equation}
		\begin{split}
			p(h<3,n=5) &= \int_0^1d\theta p(h<3,n=5|\theta)p(\theta)\\
			&= \frac{1}{2}.
		\end{split}
	\end{equation}
	Using $\begin{pmatrix}
		5 \\ 0 
	\end{pmatrix}=1$, $\begin{pmatrix}
		5 \\ 1 
	\end{pmatrix} = 5$, $\begin{pmatrix}
		5 \\ 0 
	\end{pmatrix} = 10$
	\begin{equation}
		p(\theta|h<3,n=5) = 2\bigg((1-\theta)^5+5\theta ^1(1-\theta)^4+10\theta ^2(1-\theta)^3\bigg)
	\end{equation} 
\end{example}

\begin{example}
	\index{Example: Marginalization}
	\emph{Derive}
	\begin{equation}
		p(\theta|D,I) = \sum_k p(z=k|D,I)p(\theta|D,z=k,I)
	\end{equation}
	\begin{equation}
		\begin{split}
			p(\theta|D,I) &= \sum_kp(\theta,z=k|D,I)\\
			&=\sum_kp(\theta|z=k,D,I)p(z=k|D,I)
		\end{split}
	\end{equation}
	where marginalization\index{Marginalization} over $z$ and the chain rule have been applied.
\end{example}

\begin{example}
	\index{Example: Maximum likelihood}
	\index{Example: Posterior mean}
	\emph{A lifetime $X$ of a machine is modeled by an exponential distribution\index{Exponential distribution} with unknown parameter $\theta$. The pdf is $p(x|\theta,I)=\theta e^{-\theta x}$ for $x\geq 0,\theta >0$.}
	\begin{enumerate}
		\item \emph{Show that the MLE is $\hat{\theta}=\bar{x}^{-1}$, where $\bar{x}=\frac{1}{n}\sum_{j=1}^nx_j$.}
		
		\begin{equation}
			\begin{split}
				p(D|\theta,I) &= \prod_{j=1}^{n}(\theta e^{-\theta x_j})\\
				&= \theta^ne^{-\theta \sum_{j=1}^nx_j}.
			\end{split}
		\end{equation}
		\begin{equation}
			\frac{d\ln(p(D|\theta,I))}{d\theta }\bigg|_{\theta = \hat{\theta}_{\text{MLE}}} = \frac{n}{\hat{\theta}_{\text{MLE}}} -\sum_{j=1}^nx_j =0 \Rightarrow \hat{\theta}_{\text{MLE}} = \frac{n}{\sum_{j=1}^nx_j}
		\end{equation}
		
		\item \emph{Suppose we observe $x_1 = 5$, $x_2=6$, $x_3=4$. What is the MLE then?}
		
		\begin{equation}
			\begin{split}
				\hat{\theta}_{\text{MLE}} & = \frac{1}{5}
			\end{split}
		\end{equation}
		
		\item \emph{Assume that an expert believes $\theta$ should have a prior distribution that is also exponential, meaning $p(\theta|I)=\lambda e^{-\lambda \theta}$. Choose the prior parameter, call it $\hat{\lambda}$, such that $\mathbb{E}_{p(\theta|I)}[\theta|\lambda,I]=\frac{1}{3}$.}
		
		\begin{equation}
			\mathbb{E}_{p(\theta|I)}[\theta|\lambda,I] = \int d\theta \theta\lambda e^{-\lambda \theta} = \frac{1}{\lambda}
		\end{equation}
		meaning $\hat{\lambda}=3$ in order for $\mathbb{E}_{p(\theta|I)}[\theta|\lambda,I] =\frac{1}{3}$.
		
		\item \emph{What is the posterior, $p(\theta|D,\hat{\lambda},I)$?}
		
		\begin{equation}
			\begin{split}
				p(\theta|D,\hat{\lambda},I) &= \frac{p(D|\theta,\hat{\lambda},I)p(\theta,\hat{\lambda}|I)}{p(D,\hat{\lambda}|I)}\\
				&= \frac{p(D|\theta,\hat{\lambda},I)p(\theta|\hat{\lambda},I)p(\hat{\lambda}|I)}{p(D|\hat{\lambda},I)p(\hat{\lambda}|I)}\\
				&= \frac{\prod_{j=1}^{n} (\theta e^{-\theta x_j})\hat{\lambda}e^{-\hat{\lambda}\theta}}{\int d\theta' \prod_{j=1}^{n} (\theta' e^{-\theta' x_j})\hat{\lambda}e^{-\hat{\lambda}\theta'}}\\
				&\propto \theta^n\hat{\lambda}e^{-\theta(\hat{\lambda}+\sum_{j=1}^nx_j)}\\
			\end{split}
		\end{equation}
		
		\item \emph{What is the posterior expectation?}\newline
		
		Since $\hat{\lambda}$ is given, $p(\theta|D,\hat{\lambda},I)\sim \text{Ga}(\theta|n+1,\sum_{j=1}^nx_j+\hat{\lambda})$, meaning the expectation is
		\begin{equation}
			\mathbb{E}[\theta|D,\hat{\lambda},I] = \frac{n+1}{\sum_{j=1}^nx_j+\hat{\lambda}} 
		\end{equation}
		
		\item \emph{Explain why the MLE and posterior expectation differ. Which is more reasonable?}\newline
		
		\begin{equation}
			\begin{split}
				\hat{\theta}_{\text{MLE}} &= \frac{2}{10}\\
				\mathbb{E}[\theta|D,\hat{\lambda},I] &= \frac{3+1}{15+3}\\
				&= \frac{2}{9}\\
			\end{split}
		\end{equation}
		The difference is in the prior information used in the posterior expectation. It adds an artificial datapoint with value $\hat{\lambda}$. If you trust the expert, this is good. If you do not, it is not.
	\end{enumerate}
\end{example}

\begin{example}
	\index{Example: Maximum likelihood}
	\emph{Show that $\hat{\theta}_{MLE}=\frac{N_1}{N}$ follows from the likelihood $p(D|\theta)=\theta^{N_1}(1-\theta)^{N_0}$, where $N=N_0+N_1$.}
	
	\begin{equation}
		\frac{dp(D|\theta)}{d\theta}\bigg|_{\theta=\hat{\theta}_{MLE}} =0 \Rightarrow N_1\hat{\theta}_{MLE}^{N_1-1}(1-\hat{\theta}_{MLE})^{N_0}+\hat{\theta}_{MLE}^{N_1}N_0(1-\hat{\theta}_{MLE})^{N_0-1}=0
	\end{equation}
	Isolating $\hat{\theta}_{MLE}$ yields
	\begin{equation}
		\hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1}.
	\end{equation}
\end{example}

\begin{example}
	\label{ex1}
	\index{Example: Maximum likelihood}
	\emph{The Poisson\index{Poisson distribution} pmf is defined as $\text{Poi}(x|\lambda)=e^{-\lambda}\frac{\lambda^x}{x!}$ for $x\in\{0,1,\dots\}$ where $\lambda>0$ is the rate parameter. Derive the MLE of $\lambda$.}
	
	\begin{equation}
		\begin{split}
			&p(D|\lambda,I)  = \prod_{j=1}^n \bigg(\frac{\lambda^{x_j}e^{-\lambda}}{x_j!}\bigg) \Rightarrow \ln(p(D|\lambda,I)) = \sum_{j=1}^n[x_j\ln(\lambda)-\ln(x_j!)]-\lambda n,\\
			&\frac{d\ln(p(D|\lambda,I))}{d\lambda}\bigg|_{\lambda=\hat{\lambda}_{\text{MLE}}} = -n+\frac{1}{\hat{\lambda}_{\text{MLE}}}\sum_{j=1}^nx_j=0 \Rightarrow \hat{\lambda}_{\text{MLE}} = \frac{1}{n}\sum_{j=1}^nx_j.
		\end{split}
	\end{equation}
\end{example}
\begin{example}
	\index{Example: Posterior mean}
	\emph{In example \ref{ex1} the Poisson distribution was defined with rate parameter $\lambda$ and its MLE was derived. Here a conjugate Bayesian analysis is performed.}
	\begin{enumerate}
		\item \emph{Derive the posterior $p(\lambda|D,I)$ assuming a conjugate prior $p(\lambda|I)=\text{Ga}(\lambda|a,b)\propto \lambda^{a-1}e^{-\lambda b}$.}
		
		\begin{equation}
			p(\lambda|D,I) = \frac{p(D|\lambda,I)p(\lambda|I)}{p(D|I)}
		\end{equation}
		with
		\begin{equation}
			\begin{split}
				p(D|\lambda,I) & = \prod_{j=1}^n\bigg(\frac{e^{-\lambda}\lambda^{x_j}}{x_j!}\bigg)\\
				& = \frac{e^{-n\lambda}\lambda^{\sum_{j=1}^nx_j}}{\prod_{j=1}^nx_j!}
			\end{split}
		\end{equation}
		Hence
		\begin{equation}
			\begin{split}
				p(\lambda|D,I)&\propto e^{n\lambda}\lambda^{\sum_{j=1}^nx_j}\lambda^{a-1}e^{-\lambda b}\\
				&=e^{-\lambda(n+b)}\lambda^{a+\sum_{j=1}^n-1}
			\end{split}
		\end{equation}
		
		\item \emph{What does the posterior mean tend to as $a,b\rightarrow 0$?}\newline
		
		$p(\lambda|D,I)\propto \text{Ga}(\lambda|a+\sum_{j=1}^n, b+n)$ and the posterior mean of $\text{Ga}(\lambda|\alpha,\beta)$ is $\frac{\alpha}{\beta}$. Hence
		
		\begin{equation}
			\mathbb{E}[\lambda|D,I] = \frac{a+\sum_{j=1}^nx_j}{b+n},
		\end{equation}
		meaning
		\begin{equation}
			\begin{split}
				\lim\limits_{a,b\rightarrow 0}(\mathbb{E}[\lambda|D,I]) &= \bar{x}\\
				& = \hat{\lambda}_{\text{MLE}}
			\end{split}
		\end{equation}
		where $\bar{x}\equiv \frac{1}{n}\sum_{j=1}^nx_j$.
	\end{enumerate}
\end{example}

\section{Assigning probabilities}
The axioms of probability theory can be used to define and relate probabilities, however, they are not sufficient to conduct inference because, ultimately, the "numerical values" of the probabilities must be known. Thus the rules for manipulating probabilities must be supplemented by rules for assigning numerical values to probabilities. The historical lack of these supplementary rules is one of the major reasons why probability theory, as formulated by Laplace, was rejected in the late part of the 19th century. To assign any probability there is ultimately only one way, logical analysis, i.e., non-self-contradictory analysis of the available information. The difficulty is to incorporate only the information one actually possesses without making gratuitous assumptions about things one does not know. A number of procedures have been developed that accomplish this task: Logical analysis may be applied directly to the sum and product rules to yield probabilities~\citep{jaynes_11}. Logical analysis may be used to exploit the group invariances of a problem~\citep{jaynes_16}. Logical analysis may be used to ensure consistency when uninteresting or nuisance parameter are marginalized from probability distributions~\citep{jaynes_21}. And last, logical analysis may be applied in the form of the principle of maximum entropy to yield probabilities \cite{zellner_bayesian_inference, jaynes_16,jaynes_19, shore_17,shore_18}. Of these techniques the principle of maximum entropy is probably the most powerful.

\subsection{The Principle of Maximum Entropy}
\label{sec:maxent}
The principle of maximum entropy\index{Maximum entropy}, first proposed by \citet{Jaynes1957}, considers the issue of assigning a probaility distribution to a random variable. Let $Z$ be a generic random variable that describes an abstract experiment and has a distribution $p(z|., I)$. The "$.$" denote any generic conditioning since the formalism refer to a generic distribution. The principle propose that the probability distribution, $p(z|., I)$, which best represents the current state of knowledge about a system is the one with largest constrained entropy~\citep{Sivia2006}, defined by the Lagrangian
\begin{equation}
	\mathcal{L} = \int\bigg[\underbrace{-p(z|., I)\ln\bigg(\frac{p(z|., I)}{m(z)}\bigg)-\lambda_0 p(z|., I)-\sum_{j=1}^{n}\lambda_jC_j(z)}_{\equiv F}\bigg]dz  ,
	\label{eq:Q}
\end{equation}
where $m$ -- called the Lebesgue measure -- ensures the entropy, given by $-\int p(z|., I)\ln\big(\frac{p(z|., I)}{m(z)}\big) dz$, is invariant under a change of variables and $C_j(z)$ represent the constraints beoynd normalization. The constraint beyond normality depend on the background information related to the random variable, $X$. In variational calculus the Lagrangian is optimized via solving the Euler-Lagrange equation
\begin{equation}
	\frac{\partial F}{\partial p(z|., I)}-\frac{d}{dx}\frac{\partial F}{\partial p(z|., I)'}=0,
\end{equation}
where $\frac{\partial p(z|., I)}{\partial x} = p(z|., I)'$ for shorthand. Since $p(z|., I)'\notin F$, the Euler-Lagrange equation simplify to simply
\begin{equation}
	\frac{\partial F}{\partial p(z|., I)}=0.
	\label{eq:f}
\end{equation}
Combining equations \eqref{eq:Q} and \eqref{eq:f}
\begin{equation}
	\begin{split}
		\frac{\partial F}{\partial p(z|., I)}&= -\ln\bigg(\frac{p(z|., I)}{m(z)}\bigg)-1-\sum_{j}\lambda_{j}C_j(z)\\
		&=0
	\end{split}
\end{equation}
and so
\begin{equation}
	\begin{split}
		p(z|., I)&=m(z)e^{-1-\sum_{j}\lambda_{j}C_j(z)}\\
		&=\tilde{m}(z)e^{-\sum_{j}\lambda_{j}C_j(z)},
	\end{split}
\end{equation}
where $\tilde{m}(z)\equiv m(z)e^{-1}$. Using that $\int p(z|., I) dx =1$
\begin{equation}
	p(z|., I)=\frac{\tilde{m}(z)e^{-\sum_{j}\lambda_{j}C_j(z)}}{\int \tilde{m}(z')e^{-\sum_{j}\lambda_{j}C_j(z')}dz'},
\end{equation}
where $m$ is a reference distribution that is invariant under parameter transformations. $\lambda_j$ are determined from the additional constraints, e.g. on the mean or variance.

\begin{example}
	\index{Example: Maximum entropy normal distribution}
	Consider a random variable, $Z$, with unlimited support, $z\in [-\infty,\infty]$, assumed to be symmetric around a single peak defined by the mean $\mu$, standard deviation $\sigma$. In this case $F$ can be written\label{ex:gauss}
	\begin{equation}
		F = -p(z|.,I)\ln\bigg(\frac{p(z|.,I)}{m(z)}\bigg)-\lambda_0p(z|.,I)-\lambda_1p(z|.,I)z-\lambda_2p(z|.,I)z^2
	\end{equation}
	with the derivative
	\begin{equation}
		\begin{split}
			\frac{\partial F}{\partial p(z|.,I)} &= -1-\ln\bigg(\frac{p(z|.,I)}{m(z)}\bigg)-\lambda_1z-\lambda_2z^2\\
			&=0,
		\end{split}
	\end{equation}
	meaning
	\begin{equation}
		p(z|.,I)=m(z)e^{-1-\lambda_0-\lambda_1z-\lambda_2z^2}.
	\end{equation}
	Taking a unifoirm measure ($m= const$) and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\int p(z|.,I) dz &= me^{-1-\lambda_0}\int e^{-\lambda_1z-\lambda_2z^2}dz\\
			&= me^{-1-\lambda_0}\sqrt{\frac{\pi}{\lambda_2}}e^{\frac{\lambda_1^2}{4\lambda_2}}\\
			&=1.
		\end{split}
	\end{equation}
	Defining $K^{-1} = me^{-1-\lambda_0}$ yields
	\begin{equation}
		\begin{split}
			p(z|.,I) &= \frac{e^{-\lambda_1x-\lambda_2x^2}}{K}\\
			&= \sqrt{\frac{\lambda_2}{\pi}}e^{-\frac{\lambda_1^2}{4\lambda_2}-\lambda_1z-\lambda_2z^2}\\
		\end{split}.
	\end{equation}
	Now, imposing the mean constraint
	\begin{equation}
		\begin{split}
			\int zp(z|.,I) dz &= \frac{\int ze^{-\lambda_1z-\lambda_2z^2}dz}{K}\\
			&= -\frac{\lambda_1}{2\lambda_2}\\
			&=\mu.
		\end{split}
	\end{equation}
	Hereby
	\begin{equation}
		\begin{split}
			p(z|.,I) &= \sqrt{\frac{\lambda_2}{\pi}}e^{-\mu^2\lambda_2+2\mu \lambda_2z-\lambda_2z^2}\\
			&= \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2}\big(\frac{\mu-z}{\sigma}\big)^2},\\\\
		\end{split}
	\end{equation}
	where $\sigma\equiv \frac{1}{2\lambda_2}$ has been defined. Hence, it is clear that the normal distribution\index{Normal distribution} can be derived from general constraints via the principle of maximum entropy\index{Maximum entropy}.
\end{example}

\begin{example}
	\index{Example: Maximum entropy beta distribution}
	Consider a random variable, $Z$, with limited support, $z\in [0,1]$. In order to impose the limited support, require that $\ln(z)$ and $\ln(1-z)$ be well defined. In this case $F$ can be written\label{ex:beta}
	\begin{equation}
		F = -p(z|.,I)\ln\bigg(\frac{p(z|.,I)}{m(z)}\bigg)-\lambda_0p-\lambda_1p(z|.,I)\ln(z)-\lambda_2p(z|.,I)\ln(1-z)
	\end{equation}
	with the derivative
	\begin{equation}
		\begin{split}
			\frac{\partial F}{\partial p(z|.,I)} &= -1-\ln\bigg(\frac{p(z|.,I)}{m(z)}\bigg)-\lambda_1\ln(z)-\lambda_2\ln(1-z)\\
			&=0,
		\end{split}
	\end{equation}
	meaning
	\begin{equation}
		p(z|.,I)=m(z)e^{-1-\lambda_0-\lambda_1\ln(z)-\lambda_2\ln(1-z)}.
	\end{equation}
	Taking a unifoirm measure ($m= const$) and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\int p(z|.,I) dz &= me^{-1-\lambda_0}\int z^{-\lambda_1}(1-z)^{-\lambda_2}dz\\
			&= me^{-1-\lambda_0}\frac{\Gamma(1-\lambda_1)\Gamma(1-\lambda_2)}{\Gamma(2-\lambda_1-\lambda_2)}\\
			&=1.
		\end{split}
	\end{equation}
	Now define $\alpha \equiv 1-\lambda_1\wedge \beta \equiv 1-\lambda_2$. Hereby
	\begin{equation}
		p(z|.,I) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1},
	\end{equation}
	which is the beta distribution\index{Maximum entropy}. 
\end{example}


\section{Bayes Factor}
The Bayes factor is the ratio between the probabilities of two hypothesis being true. Let $I$ denote the background information and $D$ the generic data available, then~\citep{Sivia2006,murphy2013machine}
\begin{equation}
	\begin{split}
		\text{BF} &\equiv \frac{p(A|D,I)}{p(B|D,I)}\\
		&= \frac{p(D|A,I)}{p(D|B,I)}\frac{p(A|I)}{p(B|I)},
	\end{split}
\end{equation}
where Bayes theorem has been used for the second equality and the normalization $p(D|I)$ cancels out between the denominator and nominator. Given there is no a priori bias toward any hypothesis , so $\frac{p(A|I)}{p(B|I)}=1$, meaning 
\begin{equation}
	\text{BF}= \frac{p(D|A,I)}{p(D|B,I)}.
\end{equation}
$p(D|A,I)$ and $p(D|B,I)$ can then be expanded via marginalization, the chain rule and Bayes theorem until they can be evaluated either analytically or numerically.

\begin{example}
	\index{Example: Bayes factor}
	\emph{Suppose we toss a coin\index{Coin experiment} $n=10$ times and observe $h=9$ heads. Let the null hypothesis be that the coin is fair, and the alternative hypothesis be that the coin can have any bias, so $p(\theta) = \text{Unif}(\theta|a=0,b=1)$.}
	
	\begin{enumerate}
		\item \emph{Derive Bayes factor}
		\begin{equation}
			\text{BF}\equiv \frac{p(D|M_\text{biased coin},I)}{p(D|M_\text{fair coin},I)}.
		\end{equation}
		
		\begin{equation}
			\begin{split}
				p(D|M_\text{biased coin},I) &= p(D|\text{biased coin},I)\\
				&=\int p(h|n,\theta,I)p(\theta|I)\\
				&=\frac{1}{N+1},\\
				p(D|M_\text{fair coin},I) &= p(D|\text{fair coin},I)\\
				&=\int p(h|n,\theta,I)\delta(\theta-\frac{1}{2})\\
				&=\begin{pmatrix}
					n\\
					h
				\end{pmatrix}\theta^h(1-\theta)^{n-h}|_{\theta=\frac{1}{2}}\\
				& = \begin{pmatrix}
					n\\
					h
				\end{pmatrix}\frac{1}{2^n},
			\end{split}
		\end{equation}
		where $\begin{pmatrix}
			n\\
			h
		\end{pmatrix}$ is the binomial coefficient,	hereby
		\begin{equation}
			\begin{split}
				\text{BF} &= \frac{2^n}{(n+1)\begin{pmatrix}
						n\\
						h
				\end{pmatrix}}\\
				&\simeq 9.3
			\end{split}
		\end{equation}
		
		\item \emph{What if $n=100$ and $h=90$?}
		\begin{equation}
			\begin{split}
				\text{BF}	&\simeq 7.25\cdot 10^{14}
			\end{split}
		\end{equation}
		The coin is unlikely to be fair in either case, but increasingly unlikely when there is more evidence in favor of this hypothesis.
	\end{enumerate}
\end{example}



\section{Parameter Estimation}
The posterior distirbution for a parameter, $p(w_j|D,I)$ hold all the information about this parameter. However, often a condensation of this information into a single best parameter estimate\index{Parameter estimation} with associated uncertainty is desired. As can be imagined, there exist a range of different ways to perform such a condensation of information. The most popular include the maximum of the posterior distribution (MAP), the expectation of the posterior and a credible interval on the posterior.

\subsection{MAP}
The MAP\index{MAP estimate} provides the value of a $w_j$ that has the highest probability, meaning this is the singular value that has the greatest belief. Let $w_j=\theta$  denote a generic parameter and denote the value of $\theta$ that correspond to the maximum of the posterior distribution by $\theta_0$, then by definition
\begin{equation}
	\theta_0 \equiv \argmax_{\theta}(p(\theta|D,I)).
\end{equation}
The maximum is defined a vanishing gradient and negative curvature, meaning
\begin{equation}
	\frac{d p(\theta|D,I)}{d\theta}\bigg|_{\theta = \theta_0} = 0 \quad \wedge \frac{d^2 p(\theta|D,I)}{d\theta^2}\bigg|_{\theta = \theta_0} < 0.
	\label{eq:four}
\end{equation}
$\theta_0$ can be determined from equation \eqref{eq:four}. An associated unceratinty can be determined by Taylor expanding around the maximum of the logarithm of the posterior, viz 
\begin{equation}
	\begin{split}
		L &\equiv \ln(p(\theta|D,I))\\
		&= L(\theta_0) + \frac{1}{2}\frac{d^2L}{d\theta^2}\bigg|_{\theta=\theta_0}(\theta-\theta_0)^2+\mathcal{O}(\theta^3),
	\end{split}
	\label{eq:taylor}
\end{equation}
where the first order term vanish because $\frac{d p(\theta|D,I)}{d\theta}\big|_{\theta = \theta_0} = 0 \Leftrightarrow \frac{dL}{d\theta}\big|_{\theta = \theta_0} = 0$ ($L$ is a monotonic function of $p(\theta|D,I)$). Inverting equation \eqref{eq:taylor} yields
\begin{equation}
	p(\theta|D,I)|_{\theta \sim \theta_0} \propto e^{-\frac{1}{2}\frac{d^2L}{d\theta^2}\big|_{\theta=\theta_0}(\theta-\theta_0)^2},
	\label{eq:taylor2}
\end{equation}
where the notation $p(\theta|D,I)|_{\theta \sim \theta_0}$ mean this is valid for $\theta\sim \theta_0$, i.e. around the maximum of the posterior. Equation \eqref{eq:taylor2} can be identified with the normal distribution with variance $\big(-\frac{1}{2}\frac{d^2L}{d\theta^2}\big|_{\theta=\theta_0}\big)^{-\frac{1}{2}}$ where $-\frac{1}{2}\frac{d^2L}{d\theta^2}\big|_{\theta=\theta_0}>0$ according to equation \eqref{eq:four}. The MAP estimate can therefore be written
\begin{equation}
	\theta_{0}\pm \bigg(-\frac{1}{2}\frac{d^2L}{d\theta^2}\bigg|_{\theta=\theta_0}\bigg)^{-\frac{1}{2}}.
	\label{eq:thetaa}
\end{equation}

\subsection{Expectation}
\label{sec:expectation_value}
The expectation\index{Expectation value} of the posterior distribution provides the weighted average of the random variable, meaning (see theorem \ref{theorem:expectaion_continuous})
\begin{equation}
	\mathbb{E}[\theta|D,I] = \int d\theta \theta p(\theta|D,I).
	\label{eq:expectationq1}
\end{equation}
The associated uncertainty is the variance (see definition \ref{def:variance})
\begin{equation}
	\text{Var}[\theta|D,I] = \int d\theta \theta^2 p(\theta|D,I)-\mathbb{E}[\theta|D,I]^2.
	\label{eq:eq:varianceq1}
\end{equation}
In case the posterior is symmetric around the maximum $\theta_0$, the MAP and expectation will be identical~\cite{Sivia2006}. However, the associated uncertainty estimate will not be. Also, in case of an asymmetric posetrior, the two estimate will differ. The expectation will take the entire distribution into account whereas the MAP will refer to the most likely value only. 

\subsection{Credible Interval}
When dealing with an asymmetric posterior distribution, a symmetric condensation of the posterior information may be misleading. In such cases, a credible interval\index{Credible interval} may be used to produce the uncertainty associated to the best estimate. The credible interval is defined viz
\begin{equation}
	\begin{split}
		p(\theta_1\leq \theta \leq \theta_2|D,I) &= \int_{\theta_1}^{\theta_2}d\theta p(\theta|D,I) \\
												& \equiv C,
	\end{split}
\end{equation}
where $C$ the limits $\theta_1,\theta_2$ are defined by a) imposing a value for $C$ and b) choosing a convention for the interval definition. In relation to an uncertainty estimate related to a point, the convention is to impose $C=0.68$, corresponding to the probability mass within one standard deviation of the normal distribution. With this interval, there is alignment between the credible interval and the uncertainty of equation \eqref{eq:thetaa}. In relation to convention, there are several options including i) the narrowest interval, ii) the interval that has equal tails or iii) the interval that has the expectation value as the central point. 

\begin{example}
	\index{Example: Normal distribution}
	\emph{Consider samples $x_1,\dots x_n$ from a Gaussian random variable with known variance, $\sigma^2$, and unknown mean, $\mu$. We further assume a prior distribution (also Gaussian) over the mean, $\mu\sim N(\mu|m,s^2)$, with fixed mean, $m$, and variance $s^2$. Thus, the only unknown is $\mu$.}
	
	\begin{enumerate}
		\item \emph{Calculate the MAP estimate $\mu_0$.}
		
		\begin{equation}
			p(\mu|D,\sigma,m,s,I)=\frac{p(D|\mu,\sigma,m,s,I)p(\mu|\sigma,m,s,I)}{p(D|\sigma,m,s,I)}
		\end{equation}
		with
		\begin{equation}
			\begin{split}
				p(\mu|\sigma,m,s,I) &= p(\mu|m,s,I)\\
				&= \frac{1}{\sqrt{2\pi s^2}}e^{-\frac{1}{2}(\frac{\mu-m}{s})^2},\\
				p(D|\mu,\sigma,m,s,I) &= p(D|\mu,\sigma,I)\\
				&= \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{1}{2}(\frac{\mu-x_i}{\sigma})^2}\\
				&= \frac{1}{(2\pi \sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2}\sum_{i=1}^n(\frac{\mu-x_i}{\sigma})^2}.\\
			\end{split}
		\end{equation}
		\begin{equation}
			\begin{split}
				p(\mu|D,\sigma,m,s,I) &\propto e^{-\frac{1}{2}\sum_{i=1}^n(\frac{\mu-x_i}{\sigma})^2-\frac{1}{2}(\frac{\mu-m}{s})^2}
			\end{split}
		\end{equation}
		\begin{equation}
			\begin{split}
				\frac{d\ln(p(\mu|D,\sigma,m,s,I))}{d\mu}\bigg|_{\mu=\mu_0} & = -\sum_{i=1}^n\frac{\mu_0-x_i}{\sigma^2}-\frac{1}{2}\frac{\mu_0-m}{s^2}\\
				&=0\\
				\Downarrow\\
				\mu_0 & = \frac{s^2n\bar{x}+\sigma^2m}{s^2n+\sigma^2}
			\end{split}
		\end{equation}
		
		\item \emph{Show that as the number of samples, $n$, increase, the MAP estimate converges to the MLE estimate.}\newline
		
		As $n$ increase $s^2n+\sigma^2\rightarrow s^2n$ and $s^2n\bar{x}+\sigma^2m\rightarrow s^2n\bar{x}$, meaning $\lim\limits_{n\rightarrow \infty}(\mu_0) = \bar{x}$, which is the MLE estimate of the mean (i.e. $\frac{d\ln(p(D|\mu,\sigma,I))}{d\mu}\bigg|_{\mu=\mu_0}=0\Rightarrow \hat{\mu}_{\text{MLE}}=\bar{x}$). The difference between the MAP and MLE estimates is the prior information. It makes sense that the prior loose impact as data is obtained.
		
		\item \emph{Suppose $n$ is small and fixed. What does the MAP estimator converge to if we increase the prior variance, $s^2$?}\newline
		
		As $s$ increase $s^2n+\sigma^2\rightarrow s^2n$ and $s^2n\bar{x}+\sigma^2m\rightarrow s^2n\bar{x}$, meaning $\lim\limits_{n\rightarrow \infty}(\mu_0) = \bar{x}$, similar to increasing the sample size. It makes intuitive sense that the prior should loose impact with increasing uncertainty (or equivalently decreasing information).
		
		\item \emph{Suppose $n$ is small and fixed. What does the MAP estimator converge to if we decrease te prior variance?}\newline
		
		As $s$ decrease $s^2n+\sigma^2\rightarrow \sigma^2$ and $s^2n\bar{x}+\sigma^2m\rightarrow \sigma^2m$, meaning $\lim\limits_{n\rightarrow \infty}(\mu_0) = m$. It makes intuitive sense that the prior is made more and more dominant, the estimate approach the prior (less uncertainty of the prior variance means a higher certainty about the prior and hence a stronger and more dominating prior).
		
	\end{enumerate}
\end{example}



\section{Prediction}
So far inference related to the parameters of the statistical model has been considered. Suppose now, the statistical model should be used to predict a random variable of interest, $Y$. Let $X: \Omega \mapsto \mathbb{X}$ and $Y:\Omega \mapsto \mathbb{Y}$ be two random variables that reprensent the input and target of a statistical model, respectively. The probability of a specific target value given observations $D = \{(X=x_1, Y=y_1), (X=x_2, Y=y_2),\dots (X=x_n, Y=y_n)\}$ and background information $I$ can be written $p(y|D,I)$. $p(y|D,I)$ depend on the parameters of the statistical model, but in the context of prediction the parameters are only of interest as a means to predict the target value $y$, so therefore the parameters can be marginalized viz
\begin{equation}
	p(y|D,I) = \int dw p(y,w|D,I).
	\label{eq:prediction1}
\end{equation}
The job from here is to expand equation \eqref{eq:prediction1} into something that can be evaluated either analytically or numerically
\begin{equation}
	\begin{split}
	p(y,w|D,I) & = p(y|w,D,I)p(w|D,I)\\
	& = p(y|w,D,I)\frac{p(D|w,I)p(w|I)}{p(D|I)},\\
	\end{split}
	\label{eq:prediction2}
\end{equation}
where the chain rule (see theorem \ref{theorem:chain_rule}) has been used for the first equality and Bayes theorem for the second (see theorem \ref{theorem:bayes_theorem}). At this point $p(D|w,I)$ can be identified as the likelihood, $p(y|w,D,I)$ as a draw from the probability distribution of $Y$, $p(w|I)$ the prior and $p(D|I)$ can be expanded via marginalization. Depending on the statistical model, a further breakdown may or may not be needed to specify the different components.

\begin{example}
	\index{Example: Marginal likelihood}
	\emph{Suppose we toss a coin\index{Coin experiment} $n$ times and observe $h$ heads. Let $H\sim \text{Bin}(h|n,\theta)$ and $\theta\sim \text{Beta}(\theta |\alpha =1,\beta = 1)$. Show that the marginal likelihood is}
	\begin{equation}
		p(h|n)=\frac{1}{n+1}
	\end{equation}
	\begin{equation}
		\begin{split}
			p(h|n,I)&=\int d\theta p(h,\theta|n,I) \\
			&= \int d\theta p(h|n,\theta,I)p(\theta|n,I)
		\end{split}
	\end{equation}
	$n$ is the number of throws, so
	\begin{equation}
		\begin{split}
			p(\theta|n,I) &= p(\theta|I)\\
			&=\text{Beta}(\theta |\alpha =1,\beta = 1)\\
			&=1.
		\end{split}
	\end{equation}
	\begin{equation}
		\begin{split}
			p(h|n,\theta,I) &= \text{Bin}(h|n,\theta)\\
			&=
			\begin{pmatrix}
				n \\
				h
			\end{pmatrix} \theta^h(1-\theta)^{n-h}
		\end{split}
	\end{equation}
	is the likelihood of $h$ heads in $n$ throws, where $\begin{pmatrix}
		n \\
		h
	\end{pmatrix}$ is the binomial coefficient, meaning
	\begin{equation}
		p(h|n,I)= \begin{pmatrix}
			n \\
			h
		\end{pmatrix}\int d\theta \theta^h(1-\theta)^{n-h}.
		\label{qwww}
	\end{equation}
	By definition $\int d\theta \text{Beta}(\theta|a,b) \equiv 1$, so $\int d\theta \theta^{a-1}(1-\theta)^{b-1}= B(a,b)$ which is the normalization constant of the Beta distribution. This means $\int d\theta \theta^h(1-\theta)^{n-h}=B(h+1,n-h+1)$. Hence equation \eqref{qwww} can be written
	\begin{equation}
		\begin{split}
			p(h|n,I) &= \begin{pmatrix}
				n \\
				h
			\end{pmatrix}B(h+1,n-h+1)\\
			&=\frac{\Gamma(n+1)}{\Gamma(h+1)\Gamma(n-1+1)}\frac{\Gamma(h+1)\Gamma(n-1+1)}{\Gamma(n+2)}\\
			&= \frac{\Gamma(n+1)}{\Gamma(n+2)}\\
			&= \frac{1}{n+1}.
		\end{split}
		\label{qwww2}
	\end{equation}
	where for the last equality it has been used that $\Gamma(x)=(x-1)\Gamma(x-1)$. Intuitively, I would have expected $p(h|n)$ to be a function of both $n,h$. However, the dependency on $h$ is eliminated due to the symmetric nature of $\theta^h(1-\theta)^{n-h}$.
\end{example}

\begin{example}
	\index{Example: Fly or drive on vacation?}
	\label{example:travel}
	\emph{Suppose you are considering whether it is safest to fly or take the car on vacation. The trip to the destination can be done by two flights (one stopover) or traveling by car in two days. Which means of travel is safest?}\newline
	
	The probability of not dying is analytically identical in case of traveling by car or plane both cases, however, with different interpretations. Let $D$ denote the data, $I$ the background information, $\theta$ the unknown probability of dying on a trip and $a,b$ the parameters of the prior of $\theta$. With this setup
	\begin{equation}
		\begin{split}
			p(\text{not die}|D,I) &= \int d\theta p(\theta,\text{not die}|D,I)\\
			&=\int d \theta p(\text{not die}|\theta,D,I)p(\theta|D,I).
		\end{split}
		\label{eq_ref}
	\end{equation}
	From Bayes theorem
	\begin{equation}
		\begin{split}
			p(\theta|D,I) & = \frac{p(D|\theta,I)p(\theta|I)}{p(D|I)},
		\end{split}
	\end{equation}
	where
	\begin{equation}
		p(D|I) = \int d\theta p(D|\theta,I)p(\theta|I).
	\end{equation}
	Take $p(\theta|I)=\text{Beta}(\theta|a,b)$ and 
	\begin{equation}
		\begin{split}
			p(D|\theta,I) &= \text{Bin}(n_{\text{deaths}}|n_{\text{trips}},\theta),\\
			p(\text{not die}|\theta,D,I) 
			&= \text{Bin}(n_{\text{deaths}} = 0|n_{\text{trips}} = 2,\theta).\\
		\end{split}
	\end{equation}
	Combining the above means
	\begin{equation}
		p(\text{not die}|D,I) = \frac{(n_{\text{trips}}+b-n_{\text{deaths}})(n_{\text{trips}}+b+1-n_{\text{deaths}})}{(n_{\text{trips}}+a+b)(n_{\text{trips}}+a+b+1)}.
		\label{prior_b7}
	\end{equation}
	From equation \eqref{prior_b7}, it can be seen that the parameters of the prior ($a,b$), enter into the probability as fake counts that alter the observed counts. $a,b\rightarrow 0$ correspond to the absence of prior information, in which case the data speak for itself. In general
	\begin{equation}
		p(\text{not die}|D,I)\leq 100\%,
		\label{eq:abe1}
	\end{equation}
	where the exact value depends on the values of $a$ and $b$. Hence, regardless of the data that you have, the safest safest means of travel is determined by the prior information about the probability of dying on a trip. Since the prior is subjective, it means the safest means of transportation is subjective. This is only natural, since there may be factors not included that should alter the probability of dying on a trip. For example, if it is known that the you (the driver of the car) recently lost eye sight, then the probability of not dying during two days of driving should be significantly lower than $100\%$. Dispite the natural explanation of the inherent subjectiveness of Bayesian probabilities, it was uneasiness with this kind of subjectiveness that drove the development of frequentist statistics. The objective of frequentist statistics is to redefine probability as the long run frequency of events occuring and through this redefinition remove any subjectiveness. A frequentist would estimate $p(\text{not die }|D,I)$ via a technique called penalized maximum likelihood\index{Penalized maximum likelihood}, in which $\theta$ is estimated viz 
	\begin{equation}
		\begin{split}
			\hat{\theta} &= \argmax_{\theta}(p(D|\theta,I)p(\theta|I))\\
			& = \frac{n_{\text{deaths}}+a-1}{n_{\text{trips}}+a+b-2}
		\end{split}
		\label{pll}
	\end{equation}
	and then
	\begin{equation}
		\begin{split}
			p(\text{not die}|D,I)|_{\text{frequentist}} & = (1-\hat{\theta})^2\\
			&= \bigg(\frac{n_{\text{trips}}+b-n_{\text{deaths}}-1}{n_{\text{trips}}+a+b-2}\bigg)^2.
		\end{split}
		\label{eq:point_estimate}
	\end{equation}  
	In Bayesian statistics, the penalized maximum likelihood technique is called the maximum a posteriori (MAP)\index{MAP estimate} technique and has a clear interpretation as an approximation. Equation \eqref{eq:point_estimate} still depend on the prior and is thus still subjective. To address this, often $p(\theta|a,b,I)=1$ is taken in which case the dependency on the prior seemingly disappear and the penalized maximum likelihood reduce to the maximum likelihood. However, $p(\theta|I)=1$ is a specific prior, namely the one where $a=b=1$ and hence the maximum likelihood method implicitly impose a specific prior, namely the uniform one. In this case
	\begin{equation}
		\begin{split}
			p(\text{not die}|D,a=1,b=1,I)|_{\text{frequentist}} &= \bigg(\frac{n_{\text{trips}}-n_{\text{deaths}}}{n_{\text{trips}}}\bigg)^2,
		\end{split}
		\label{eq:point_estimate2}
	\end{equation}
	where $a,b$ have been dragged out of the background information to be explicit about their values for now, which is indeed the observed long run frequency -- as the definition of frequentist statistics dictate -- which will become increasingly accurate as the data approach infinite. Note (equation \eqref{prior_b7} with $a=b=1$ and equation \eqref{eq:point_estimate2})
	\begin{equation}
		p(\text{not die}|D,a=1,b=1,I) \neq p(\text{not die}|D,a=1,b=1,I)|_{\text{frequentist}}
	\end{equation}
	since the two formalisms define probability in different ways. However, $p(\text{not die}|D,a=1,b=1,I) \approx p(\text{not die}|D,a=1,b=1,I)|_{\text{frequentist}}$ which approach an exact equality with increasing data. This is to be expected, since $p(\text{not die}|D,a,b,I)|_{\text{frequentist}}$ via the MAP technique can be interpreted as an approximation to $p(\text{not die}|D,a,b,I)$.\newline
	Another interesting aspect which has so far not been touched upon is the intuitive interpretation of the probabilities in Bayesian and frequentist statistics, respectively. Since frequentist statistics redefine probability as the long run frequency of events occuring, it does not make sense to talk about the event of dying in some way. Your death is an event that by definition does not have a long run frequency and is thus not well defined in frequentist statistics. The proponents of frequentist statistics work around this by invoking parallel universes (...) where such events can happen more than once.\newline
	In summary it can be said that frequentist statistic does not succeed in it's life's mission of removing the subjectivity of prior knowledge, it only succeeds in obscuring it -- Which is worse. On top of that, it redefines probaiblity in a counterintuitive way and require convoluted obscure scenarios to explain some types of probabilities. Frequentist statistics also cannot be derived from a singular framework, like Bayesian statistics can be derived from probability theory. In my oppinion this is the most damning shortcomming of frequentist statistics, since the lack of a coherent theoretical framework means a lack of understanding and thus presents a massive disadvantage when trying to improve or adjust details of a model.  
\end{example}



\subsection{Bayesian Regression}
\label{sec:bayML}
Regression is a process that estimates the relationship, represented by some function $f$ with associated parameters $\theta\in \mathbb{W}$, between a set of input ($X$) and target ($Y$) variables, typically via some sort of point estimate and/or some measure of uncertainty. In this study the expectation and standard deviation will be used as a point estimate and associated uncertainty. Let $D = \{(X=x_1, Y=y_1), (X=x_2, Y=y_2),\dots (X=x_n, Y=y_n)\}$ denote the data, $I$ the background information $X=x, Y=y$ a generic input beyond the input data, then the point estimate and associated uncertainty can be written (see section \ref{sec:expectation_value})
\begin{equation}
	\mathbb{E}[y|x,D,I] \pm \sqrt{\text{Var}[y|x,D,I]}
	\label{eq:expec}
\end{equation}
where
\begin{equation}
	\begin{split}
		\mathbb{E}[y|x,D,I] & \equiv \int y p(y|x,D,I) dy,\\
		\text{Var}[y|x,D,I] & \equiv
		\int y^2 p(y|x,D,I) dy-\mathbb{E}[y|x,D,I]^2.
	\end{split}
	\label{eq:q1}
\end{equation}
For a function $f$ that maps a set of continuous input variables ($x$) to a set of continuous target variables ($y$), the data is typically assumed to follow a normal distribution with $f$ as mean and an unknown variance, $\xi$, such that
\begin{equation}
	p(y_i|x_i,\theta,\xi,I)=\sqrt{\frac{\xi}{2\pi}} e^{-\frac{\xi}{2}(f(\theta,x_i)-y_i)^2}.
	\label{f_dist}
\end{equation}
Using equation \eqref{f_dist} and marginalizing over $\xi,\theta$
\begin{equation}
	\begin{split}
		p(y|x,D,I) &= \int p(y,\theta,\xi|x,D,I) d\theta d\xi\\
		& = \int p(y|x,\theta,\xi,D,I)  p(\theta,\xi|x,D,I)d\theta d\xi\\
		& = \int p(y|x,\theta,\xi,I)  p(\theta,\xi|x,D,I)d\theta d\xi,\\
	\end{split}
	\label{eq:q2}
\end{equation}
where for the last equality it has been used that $p(y|\theta,\xi,x,D,I) = p(y|\theta,\xi,x,I)$ since by definition $f: x\rightarrow y$ (equation \eqref{f_dist}). Using equation \eqref{eq:q2} in equation \eqref{eq:q1}
\begin{equation}
	\begin{split}
		\mathbb{E}[y|x,D,I] & = \int f(\theta,x)  p(\theta,\xi|x,D,I) d\theta d\xi,\\
		& = \mathbb{E}[f|x,D,I]\\
		\text{Var}[y|x,D,I] & =
		\int (\xi^{-1}+f(\theta,x)^2) p(\theta,\xi|x,D,I)  d\theta d\xi-\mathbb{E}[f(x,\theta)|x,D,I]^2\\
		& =\mathbb{E}[f^2|x,D,I]+\mathbb{E}[\xi^{-1}|x,D,I]-\mathbb{E}[f|x,D,I]^2\\
		& = \text{Var}[f|x,D,I]+\mathbb{E}[\xi^{-1}|x,D,I]\\
	\end{split}
	\label{eq:q3}
\end{equation}	
where it has been used that
\begin{equation}
	\begin{split}
		\mathbb{E}[y|x,\theta,\xi,I] &= \int y p(y|x,\theta,\xi,I) dy\\
		&= f(\theta,x)\\
		\mathbb{E}[y^2|x,\theta,\xi,I] &= \int y^2 p(y|x,\theta,\xi,I) dy\\
		&= \text{Var}[y|x,\theta,\xi,I]+\mathbb{E}[y|x,\theta,\xi,I]^2\\
		& = \xi^{-1}+f(\theta,x)^2
	\end{split}
\end{equation}
according to equation \eqref{f_dist}. Equation \eqref{eq:q3} relates the expectation and variance of $y$ to functions of the parameters $\theta,\xi$ and input data $x$. Using Bayes theorem
\begin{equation}
	p(\theta,\xi|x,D,I) = \frac{p(D_y|x,D_x,\theta,\xi,I)p(\theta,\xi|x,D_x,I)}{p(D_y|D_x,I)}
	\label{eq:bayes2}
\end{equation}
where $D_y= \{Y=y_1, Y=y_2,\dots Y=y_n\}$ and $D_x = \{X=x_1, X=x_2,\dots X=x_n\}$. From equation \eqref{f_dist} the likelihood can be written
\begin{equation}
	p(D_y|x,D_x,\theta,\xi,I) = \bigg(\frac{\xi}{2\pi}\bigg)^\frac{n}{2}\prod_{i=1}^n e^{-\frac{\xi}{2}(f(\theta,x_i)-y_i)^2}
	\label{reg:likelihood}
\end{equation}
and
\begin{equation}
	p(D_y|x,D_x,I) = \int p(D_y|x,D_x,\theta,\xi,I)p(\theta,\xi|x,D_x,I) d\theta d\xi.
\end{equation}
From the chain rule
\begin{equation}
	\begin{split}
		p(\theta,\xi|x,D_x,I) &= p(\theta|\xi,x,D_x,I)p(\xi|x,D_x,I).
	\end{split}
\end{equation}
Assuming the distributions over $W_\theta$ is i) independent of both the input data and $\xi$ and ii) normally distributed\footnote{The normally distributed prior is closely related to weight decay~\citep{Plaut1986}, a principle conventionally used in frequentist statistics to avoid the issue of overfitting.} with zero mean and a precision described by a hyperparameter, $\lambda$. 	 
\begin{equation}
	\begin{split}
		p(\theta|\xi,x,D_X,I) & = p(\theta|I)\\
		& = \int p(\theta|\lambda,I)p(\lambda|I)d\lambda
	\end{split}
	\label{eq:prior1}
\end{equation}
The precision is constructed as a wide gamma distribution\index{Gamma distribution} so as to approximate an objective prior
\begin{equation}
	\begin{split}
		p(\theta|\lambda,I)p(\lambda|I) &=\prod_{q=1}^{\tilde{n}}\text{N}(\theta|\text{mean} = 0, \text{precision} = \lambda_q)\text{Ga}(\lambda_q|\alpha_q,\beta_q)\\
		&= \prod_{q=1}^{\tilde{n}} \frac{\lambda_q^\frac{n_q}{2}}{(2\pi)^\frac{n_q}{2}}e^{-\frac{\lambda_q}{2}\sum_{l=1}^{n_q}\theta_l^2}\frac{\beta_q^{\alpha_q}}{\Gamma(\alpha_q)}\lambda_q^{\alpha_q-1}e^{-\beta_q \lambda_q}
	\end{split}
	\label{eq:prior}
\end{equation}
where $\alpha_q,\beta_q$ are prior parameters and $\tilde{n}$ is the number of hyper parameters. In the completely general case $\tilde{n}$ would equal the number of parameters $\theta$, such that each parameter has an independent precision. In practice, one may consider assigning some parameters the same precision, e.g. for parameters in the same layer in a neural network. Assuming i) $p(\xi|x,D_x,I)$ does not depend on input data alone (i.e. without parameters $\theta$ included) and ii) since $p(\xi|x,D_x,I)$ is analogous to $p(\lambda|I)$ -- in that both are prior distributions for precision parameters -- $p(\xi|x,D_x,I)$ is assumed to be a wide gamma distribution, then
\begin{equation}
	\begin{split}
		p(\xi|x,D_x,I) & = \text{Ga}(\xi|\tilde{\tilde},\tilde{\beta})\\
		& =\frac{\tilde{\beta}^{\tilde{\alpha}}}{\Gamma(\tilde{\alpha})}\xi^{\tilde{\alpha}-1}e^{-\tilde{\beta} \xi}.
	\end{split}
	\label{p7}
\end{equation}
At this point equation \eqref{eq:expec} is fully specified (the parameters $\alpha,\beta,\tilde{\alpha},\tilde{\beta}$ and the functional form of $f(\theta,x_i)$ are assumed specified) and can be approximated by obtaining samples from $p(\theta,\xi,\lambda|x,D,I)$ via HMC~\citep{Hammersley1964,Duane:1987de,Neal:1996,Neal2012} (see appendix \ref{app:HMC} for a review of HMC). The centerpiece in the HMC algorithm is the Hamiltonian defined viz~\citep{Neal:1996,Neal2012}
\begin{equation}
	H \equiv  \sum_{q=1}^{\tilde{n}}\sum_{l=1}^{n_q}\frac{p_{l}^2}{2m_{l}}-\ln[p(\theta,\xi,\lambda|x,D,I)]+const,
	\label{eqh}
\end{equation}
where 
\begin{equation}
	p(\theta,\xi|x,D,I) = \int d\lambda p(\theta,\xi,\lambda|x,D,I).
	\label{eq:ss}
\end{equation}
Besides its function in the HMC algorithm, the Hamiltonian represent the details of the Bayesian model well and should be a familiar sight for people used to the more commonly applied \emph{frequentist formalism}\index{Frequentist statistics} (since, in this case, it is in form similar to a cost function comprised of a sum of squared errors, weight decay on the coefficients and further penalty terms~\citep{hastie_09,murphy2013machine,Goodfellow2016}). Using equations \eqref{eq:bayes2}-\eqref{eq:ss} yields
\begin{equation}
	\begin{split}
		H&=\sum_{q=1}^{\tilde{n}}\sum_{l=1}^{n_q}\frac{p_{l}^2}{2m_{l}}+\frac{n}{2}[\ln(2\pi)-\ln(\xi)] +\frac{\xi}{2}\sum_{i=1}^{n}(f(\theta,x_i)-y_i)^2\\
		&\quad+\sum_{q=1}^{\tilde{n}}\bigg(\ln(\Gamma(\alpha_q))-\alpha_q\ln(\beta_q)+(1-\alpha_q)\ln(\lambda_q)+\beta_q\lambda_q\\
		&\qquad\qquad+\frac{n_q}{2}(\ln(2\pi)-\ln(\lambda_q))+\frac{\lambda_q}{2}\sum_{l=1}^{n_q}\theta_l^2\bigg)\\
		&\quad+\ln(\Gamma(\tilde{\alpha}))-\tilde{\alpha}\ln(\tilde{\beta})+(1-\tilde{\alpha})\ln(\xi)+\tilde{\beta}\xi+const.
	\end{split}
	\label{eqh2}
\end{equation}

\begin{example}
	\index{Example: HMC Hamiltonian variable change}
	Let $\xi \equiv e^\zeta$, such that $\zeta\in [-\infty,\infty]$ maps to $\xi\in[0,\infty]$ and $\xi$ is ensured to be positive definite regardless of the value of $\zeta$. Using the differential $d\xi =  \xi d\zeta$ in equation \eqref{eq:q3} means $p(\theta,\xi,\lambda|x,D,I)$ is multiplied with $\xi$. Hence, when taking $-\ln(p(\theta,\xi,\lambda|x,D,I))$ according to equation \eqref{eqh}, a $-\ln(\xi)$ is added to the Hamiltonian. In practice this means
	\begin{equation}
		(1-\tilde{\alpha})\ln(\xi)\in H\Rightarrow -\tilde{\alpha}\ln(\xi).
	\end{equation} 	
\end{example}

\begin{example}
	\label{ex:LS}
	\index{Example: Least squares derived}
	The method of least squares\index{Least squares} (LS) is a widely known approach to data analysis often used in the frequentist formulation of statistics. This example will show how it is derived from Bayesian statistics and which assumptions it is built from. Consider a (univariate) regression problem with a model $f$, data $D = \{(X=x_1, \delta X = \delta x_1, Y=y_1, \delta y = \delta y_1), (X=x_2, \delta X = \delta x_2, Y=y_2, \delta y = \delta y_2),\dots (X=x_n, \delta X = \delta x_n, Y=y_n, \delta y = \delta y_n)\}$ with associated uncertainties. For a function $f$ that maps a set of continuous input variables ($x$) to a set of continuous target variables ($y$), the target variable is typically assumed to follow a normal distribution with $f$ as mean. This means the expectation of a new data point $y$ given a new input $x$ can be written
	\begin{equation}
		\begin{split}
			\mathbb{E}[y|D,I] &= \int y p(y|x,D,I)d y\\
			&= \int y p(y,\theta,\zeta|x,D,I)d yd\theta d\zeta\\
			&= \int y p(y|x,\theta,\zeta,D,I)p(\theta,\zeta|x,D,I)d yd\theta d\zeta\\
		\end{split}
		\label{eq1}
	\end{equation}
	where $\zeta$ is the mean of the input variable (the true value assumed for each measurement). Assuming $p(y|x,\theta,\zeta,D,I) = p(y|x,\theta,\zeta,I)$ equation \eqref{eq1} can be written
	\begin{equation}
		\begin{split}
			\mathbb{E}[y|D,I]&= \int \mathbb{E}[y|x,\theta,\zeta,I]p(\theta,\zeta|x,D,I)d\theta d\zeta\\
			&= \int f(\theta,\zeta)p(\theta,\zeta|x,D,I)d\theta d\zeta\\
			&= \mathbb{E}[f(\theta,\zeta)]\\
			&=\lim\limits_{N\rightarrow \infty}\bigg(\frac{1}{N}\sum_{j\in p(\theta,\zeta|x,D,I)}f(\theta_j,\zeta_j)\bigg).
		\end{split}
		\label{eq2}
	\end{equation}
	Approximating the sum in equation \eqref{eq2} by the one set of coefficients corresponding to the maximum in $p(\theta,\zeta|x,D,I)$, denoted by $\theta^*,\zeta^*$, is the maximum a posteriori (MAP) estimate. By taking all the priors to be uniform, the MAP reduce to the the maximum likelihood estimate (MLE) of frequentist statistics
	\begin{equation}
		\hat{y}|_{\text{frequentist}}\sim f(\theta^*,\zeta^*).
	\end{equation}
	This example will show how this can be derived analytically. Assuming $\theta$ and $\zeta$ are conditionally independent $p(\theta,\zeta|x,D,I)$ can, using Bayes theorem, be written
	\begin{equation}
		p(\theta,\zeta|x,D,I)=\frac{p(D_y|x,D_x,\theta,\zeta,I)p(\theta,\zeta|x,D_x,I)}{p(D_y|x,D_x,I)}.
	\end{equation}
	Assuming i) the prior over $\theta,\zeta$ is independent of input data and uniform ($p(\theta,\zeta|x,I)=p(\theta,\zeta|I)=const$)
	\begin{equation}
		p(D_y|x,D_x,\theta,\zeta,I) = \prod_{i=1}^n\frac{1}{\delta y_i\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{y_i-f(\theta,\zeta_i)}{\delta y_i}\big)^2}\frac{1}{\delta x_i\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{x_i-\zeta_i}{\delta x_i}\big)^2}.
		\label{eq:lik1}
	\end{equation}
	As mentioned, $\theta^*,\zeta^*$ are defined as the sets of coefficients corresponding to the maximum in $p(\theta,\zeta|x,D,I)$, meaning
	\begin{equation}
		\begin{split}
			\theta^* &=\arg\max_{\theta,\zeta}(p(\theta,\zeta|x,D,I))\\
			&=\arg\max_{\theta,\zeta}(\ln(p(\theta,\zeta|x,D,I)))\\
			&=\arg\min_{\theta,\zeta}(\chi^2),
		\end{split},
		\label{eq:chi}
	\end{equation}
	where $\chi^2$ is the familiar chi-square measure
	\begin{equation}
		\chi^2\equiv \sum_{i=1}^n\bigg[\bigg(\frac{y_i-f(\theta,\zeta_i)}{\delta y_i}\bigg)^2+\bigg(\frac{x_i-\zeta_i}{\delta x_i}\bigg)^2\bigg],
		\label{eq:chi1}
	\end{equation}
	and the constant (containing the normalization in equation \eqref{eq:lik1} and the factor $\frac{1}{2}$ from the exponent) does not influence the $\arg\max$ function and $\arg\max\rightarrow \arg\min$ due to the negative sign in the exponent. The sets of coefficients minimizing $\chi^2$ is defined as the least squares coefficients. As is clear from equation \eqref{eq:chi}; the maximum likelihood estimate is equal to the least squares estimate in case the dependent variable is assumed normally distributed. \newline
	The minimization over equation \eqref{eq:chi1} can be approximated as follows
	\begin{equation}
		\begin{split}
			\frac{\partial \chi^2}{\partial \zeta_i}= &=-2\frac{y_i-f(\theta,\zeta_i)}{\delta y_i^2}\frac{\partial f(\theta,\zeta_i)}{\partial \zeta_i}\delta_{ai}-2\frac{x_i-\zeta_i}{\delta x_i^2}\delta_{ai}.\\
			&=0
		\end{split}
		\label{eq3}
	\end{equation}
	Now, let
	\begin{equation}
		f(\theta,x_i)=f(\theta,\zeta_i)+(x_i-\zeta_i)f'+\mathcal{O}(\partial^2f),
		\label{eq4}
	\end{equation}
	with $f' \equiv \frac{\partial f(\theta,x_i)}{\partial x_i}\big|_{x_i=\zeta_i}$. Using equation \eqref{eq4} in \eqref{eq3} yields
	\begin{equation}
		\begin{split}
			-\frac{y_i-[f(\theta,x_i)-(x_i-\zeta_i)f']}{\delta y_i^2}f'-\frac{x_i-\zeta_i}{\delta x_i^2}\approx 0,
		\end{split}
	\end{equation}
	from which $\zeta_i$ can be isolated
	\begin{equation}
		\begin{split}
			\zeta_i\approx x_i+\frac{\delta x_i^2}{\delta_i^2}(y_i-f(\theta,x_i)),
		\end{split}
		\label{eq5}
	\end{equation}
	where
	\begin{equation}
		\delta_i^2=\delta y_i^2+\delta x_i^2(f')^2.
		\label{eq:d1}
	\end{equation}
	Using equations \eqref{eq4} and \eqref{eq5} in equation \eqref{eq:chi1}
	\begin{equation}
		\chi^2\approx\sum_{i=1}^n\bigg(\frac{y_i-f(\theta,x_i)}{\delta_i}\bigg)^2.
		\label{eq7}
	\end{equation}
	Often the uncertainty of the independent variable can be neglected. In this case $\delta x_i\simeq 0$ and equation \eqref{eq7} becomes
	\begin{equation}
		\chi^2 =\sum_{i=1}^n\bigg(\frac{y_i-f(\theta,x_i)}{\delta y_i}\bigg)^2,
		\label{eq8}
	\end{equation}
	which is the famous equation of least squares. In the case where $\delta x_i\neq 0$ equation \eqref{eq7} must however (in general) be applied. In doing so it should be noted that $f'$ depends on $\zeta$ which is an unknown quantity. The minimization of equation \eqref{eq7} therefore (in general) proceeds via an iterative procedure
	\begin{enumerate}
		\item Guess some value of $f'$.
		\item Calculate $\delta_i$.
		\item Minimize $\chi^2$ with respect to $\theta$.
		\item Use $\theta$ from step iii) with $\zeta_i\approx x_i$ to calculate $g'$.
		\item Go to step 2.\newline
	\end{enumerate}
\end{example}

\begin{example}
	\label{ex:chi1}
	\index{Example: Error weighted mean}
	Often times the error weighted mean of two populations are compared in order to gauge whether the two are identical. This procedure can be derived from the least squares\index{Least squares} principle with $f=\mu=const$ (the distribution is approximated by one point) as follows
	\begin{equation}
		\frac{\partial \chi^2}{\partial \mu} = -2\sum_{i=1}^n\bigg(\frac{y_{i}-\mu}{\delta y_i^2}\bigg)=0 \rightarrow \mu =\frac{\sum_i \frac{y_i}{\delta y_i^2}}{\sum_j \frac{1}{\delta y_j^2}},
		\label{eq9}
	\end{equation}
	with uncertainty (via error propagation)
	\begin{equation}
		\begin{split}
			\delta \mu & = \sqrt{\sum_n\bigg(\frac{\partial \mu}{\partial y_n}\delta y_n\bigg)^2}\\
			& = \frac{1}{\sqrt{\sum_j \frac{1}{\delta y_j^2}}}\\
		\end{split}.
	\end{equation}
	The difference between two distributions of $y$ can then be estimated via the $\chi^2$
	\begin{equation}
		\chi^2 = \bigg(\frac{\Delta \mu-0}{\delta(\Delta\mu)}\bigg)^2,
	\end{equation}
	where $0$ is the theoretical value $\Delta\mu$ is compared to. Assuming $\mu_1$ and $\mu_2$ are independent such that $\delta (\Delta \mu)=\sqrt{\delta \mu_1^2+\delta \mu_2^2}$
	\begin{equation}
		\chi^2 =\bigg(\frac{\mu_2-\mu_1}{\sqrt{\delta \mu_1^2+\delta \mu_2^2}}\bigg)^2.
	\end{equation}
	
\end{example}
\begin{example}
	\label{ex:chi2}
	\index{Example: Least squares}
	Consider the case where a constant function is fitted to data. In this case the $\chi^2$ of equation\index{Least squares} \eqref{eq7} should be applied with $f=a=const$, however, as is clear from equation \eqref{eq:d1}, this particular choice of $f$ yields the $\chi^2$ of equation \eqref{eq8} (the errors in $\zeta$ are not relevant when $f$ does not depend on $\zeta$). Hereby
	\begin{equation}
		\chi^2 =\sum_{i=1}^n\bigg(\frac{y_i-a}{\delta y_i}\bigg)^2,
		\label{eq10}
	\end{equation}
	where $a$ is determined from $\frac{\partial \chi^2}{\partial a}=0$. Comparing examples \ref{ex:chi1} and \ref{ex:chi2} it is clear that the procedure in example \ref{ex:chi2} is identical to the error weighted mean of all $y$. 
\end{example}

\subsection{Bayesian Classification}
\label{sec:baycl}
The formalism of Bayesian classification mirror that of Bayesian regression (section \ref{sec:bayML}), however with a few key differences related to the discrete nature of classification and the underlying distributions. In case of classifications, the task is to predict the correct class, $k\in K$ for a generic input $X=x$. Let $Y=y$ be a one hot random variable vector with dimension $K$ that denote the true class, $D = \{(X=x_1, Y=y_1), (X=x_2, Y=y_2),\dots (X=x_n, Y=y_n)\}$ denote the data, $I$ the background information $X=x, Y=y$ a generic input beyond the input data, then the point estimate and associated uncertainty can be written 
\begin{equation}
	\begin{split}
		\mathbb{E}[Y = y_k|X =x,D,I] & \equiv \sum_{y_k=0,1}y_k p(Y = y_k|X = x,D,I)\\
		&= p(Y_k = 1|x,D,I),\\
		\text{Var}[Y_k = y_k|X = x,D,I] & \equiv \sum_{y_k=0,1}y_k^2 p(Y_k =y_k|X = x,D,I)-\mathbb{E}[Y_k = y_k|X = x,D,I]^2\\
		&=\mathbb{E}[Y_k = y_k|X = x,D,I](1-\mathbb{E}[Y_k = y_k|X = x,D,I]).
	\end{split}
	\label{eq:q6}
\end{equation}	
In case of classification the function $f$ maps a set of continuous input variables ($x$) to a probability for a discrete class, meaning ~\citep{Saerens2002}
\begin{equation}
	p(Y_k = 1|x,\theta,I)= f_k(\theta,x).
	\label{f_dist2}
\end{equation}
Using equation \eqref{f_dist2} and marginalizing over $\theta$
\begin{equation}
	\begin{split}
		\mathbb{E}[Y_k = 1|x,D,I] &= \int p(Y_k = 1,\theta|x,D,I) d\theta \\
		& = \int p(Y_k = 1|x,\theta,D,I)  p(\theta|x,D,I)d\theta \\
		& = \int p(Y_k=1|x,\theta,I)  p(\theta|x,D,I)d\theta \\
		& = \int f_k(\theta,x)  p(\theta|x,D,I)d\theta \\
		& = \mathbb{E}[f_k|x,D,I],\\
	\end{split}
	\label{eq:q5}
\end{equation}
where for the second to last equality it has been used that $p(Y_k=1|\theta,x,D,I) = p(Y_k=1|\theta,x,I)$ since by definition $f: x\rightarrow y$ (equation \eqref{f_dist2}). From Bayes theorem
\begin{equation}
	p(\theta|x,D,I) =\frac{p(D_y|x,D_x,\theta,I)p(\theta|x,D_x,I)}{p(D_y|x,D_x,I)},
\end{equation}
where similar to equation \eqref{eq:prior1}
\begin{equation}
	\begin{split}
		p(\theta|x,D_x,I) & = p(\theta|x,D_x,I)\\
		& = \int p(\theta|\lambda,I)p(\lambda|I)d\lambda.
	\end{split}
\end{equation}
If it is assumed that the distribution over $\theta$ is i) independent of the input data and ii) normally distributed with zero mean and a precision described by a hyperparameter, $\lambda$, then $p(\theta|\lambda,I)p(\lambda|I)$ is given by equation \eqref{eq:prior}. Assuming the target values are independent, the likelihood can be written~\citep{Fischer1999} 
\begin{equation}
	\begin{split}
		p(D_y|x,D_x,\theta,I) &= p(D_y|D_x,\theta,I)\\
		&=\prod_{k=1}^K\prod_{i=1}^{n}p(Y_k = y_{k,i}|X = x_{i},D,\theta,I)^{y_{k,i}}\\
		&=\prod_{k=1}^K\prod_{i=1}^{n}f_k(\theta,x_i)^{y_{k,i}}
	\end{split},
	\label{lik}
\end{equation}
where for the last equality it has been used that $p(Y_k = y_{k,i}|X = x_{i},D,\theta,I)^{y_{k,i}} = p(Y_k = y_{k,i}|X = x_{i},D,\theta,I)$ if $y_{k,i} = 1$ and otherwise $p(Y_k = y_{k,i}|X = x_{i},D,\theta,I)^{y_{k,i}}=1$. At this point equation \eqref{eq:expec} (in case of classification) is fully specified and can be approximated by HMC similarly to the regression case. In this case, the model can be represented by the Hamiltonian 
\begin{equation}
	H \equiv  \sum_{q}\sum_{l}\frac{p_{l}^2}{2m_{l}}-\ln(p(\theta,\lambda|x,D,I))+const
	\label{ham3}
\end{equation}
where
\begin{equation}
	p(\theta|x,D,I) = \int d\lambda p(\theta,\lambda|x,D,I).
\end{equation}
Using equations \eqref{eq:q5}-\eqref{lik} in equation \eqref{ham3} yields the Hamiltonian
\begin{equation}
	\begin{split}
		H&=\sum_{q=1}^{\tilde{n}}\sum_{l=1}^{n_q}\frac{p_{l}^2}{2m_{l}}-\sum_{k=1}^{K}\sum_{i=1}^{n}y_{k,i}\ln(f_k(\theta,x_i))+\text{const}\\
		&\quad+\sum_{q=1}^{\tilde{n}}\bigg(\ln(\Gamma(\alpha_q))-\alpha_q\ln(\beta_q)+(1-\alpha_q)\ln(\lambda_q)+\beta_q\lambda_q\\
		&\qquad \qquad+\frac{n_q}{2}(\ln(2\pi)-\ln(\lambda_q))+\frac{\lambda_q}{2}\sum_{l=1}^{n_q}\theta_l^2\bigg)\\
	\end{split}.
	\label{ham2}
\end{equation}
Sampling equation \eqref{ham2} yields a set of coefficients which can be used to compute $\mathbb{E}[f_k|x,D,I]$ which in turn (see euation \eqref{eq:q5}) can be used to compute $\mathbb{E}[Y = y_k|X =x,D,I]$ and $\text{Var}[Y_k = y_k|X = x,D,I]$ (see equation \eqref{eq:q6}).

