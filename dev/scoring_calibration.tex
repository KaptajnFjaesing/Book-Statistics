\subsection{Proper scoring rules, calibration, and expected calibration error}
\label{sec:scoring_calibration}

\subsubsection{Proper scoring rules}
A \emph{scoring rule} assigns a numerical score to a probabilistic forecast.  Let \(\Omega_S\) be a finite (or countable / measurable) state space for \(S\) and let
\[
q = \big(q(\sigma)\big)_{\sigma\in\Omega_S},\qquad \sum_{\sigma\in\Omega_S} q(\sigma)=1,\; q(\sigma)\ge0,
\]
be a proposed predictive distribution (a forecast) for \(S\).  A scoring rule is a function
\[
\mathcal{S}:\; \{q\}\times \Omega_S \;\longrightarrow\; \mathbb{R},
\]
so that \(\mathcal{S}(q,\sigma)\) is the score assigned when the forecast \(q\) is issued and outcome \(\sigma\) occurs.

\begin{definition}[Proper and strictly proper scoring rules]
	A scoring rule \(\mathcal{S}\) is \emph{proper} if for every true distribution \(p\) over \(\Omega_S\),
	\[
	\mathbb{E}_{S\sim p}\big[\mathcal{S}(q,S)\big] \ge \mathbb{E}_{S\sim p}\big[\mathcal{S}(p,S)\big]
	\qquad\text{for all forecasts } q,
	\]
	and \(\mathcal{S}\) is \emph{strictly proper} if equality implies \(q=p\).  Equivalently, the expected score is minimised (uniquely in the strictly proper case) when the forecast equals the true distribution.
\end{definition}

Proper scoring rules are the canonical objective for fitting probabilistic models: if the model is optimised with respect to a strictly proper scoring rule (on held-out data), it is incentivised to return honest posterior predictive probabilities.

\paragraph{Two canonical examples.}
\begin{itemize}
	\item \textbf{Logarithmic (log) score / Negative log-likelihood (NLL).} For discrete \(\Omega_S\),
	\[
	\mathcal{S}_{\mathrm{log}}(q,\sigma) \;=\; -\log q(\sigma).
	\]
	This is strictly proper: \(\mathbb{E}_p[-\log q(S)]\) is minimised uniquely at \(q=p\).
	\item \textbf{Brier score (quadratic score).} For a finite \(\Omega_S\) identify each outcome \(\sigma\) with the one-hot vector \(e_\sigma\).  The (multiclass) Brier score is
	\[
	\mathcal{S}_{\mathrm{Brier}}(q,\sigma)\;=\; \|q - e_\sigma\|_2^2 = \sum_{\tau\in\Omega_S}\big(q(\tau)-\mathbf{1}\{\sigma=\tau\}\big)^2.
	\]
	In the binary case (write \(q\) as scalar \(q\equiv q(\sigma_2)\) and outcome indicator \(y\in\{0,1\}\)):
	\[
	\mathcal{S}_{\mathrm{Brier}}(q,y) = (q-y)^2.
	\]
	The Brier score is strictly proper as well.
\end{itemize}

\paragraph{Decision-theoretic connection.} If the Robot predicts \(q(x)=p(S\mid x,D,I)\) and the scoring rule is proper, then reporting \(q(\cdot)\) is Bayes-optimal for the score: no other forecast \(q'\) has strictly smaller (expected) score.

\subsubsection{Calibration}
Intuitively, a probabilistic forecast is \emph{calibrated} if predicted probabilities match observed frequencies.

\begin{definition}[Calibration function]
	Let \(\widehat p(x)\) denote the classifier score for some distinguished event, e.g. in binary classification
	\[
	\widehat p(x) \equiv p(S=\sigma_2\mid x, D, I).
	\]
	The \emph{calibration function} (or reliability function) \(r:[0,1]\to[0,1]\) is defined by
	\[
	r(q) \;=\; \mathbb{P}\big( S=\sigma_2 \mid \widehat p(X)=q \big),
	\]
	when the conditional distribution is well-defined.  The predictor \(\widehat p\) is \emph{perfectly calibrated} if \(r(q)=q\) for all \(q\in[0,1]\).
\end{definition}

In practice \(\widehat p(X)\) is continuous and the conditional on the event \(\{\widehat p(X)=q\}\) has zero probability.  Practically one estimates \(r(q)\) by binning predicted probabilities or by nonparametric smoothing.

\paragraph{Reliability diagram (calibration plot).}
A reliability diagram plots empirical event frequency versus predicted probability. Concretely, partition \([0,1]\) into bins \(B_1,\dots,B_M\). For bin \(B_m\) define
\[
\text{conf}(B_m) \;=\; \frac{1}{|B_m|}\sum_{i: \widehat p(x^{(i)})\in B_m} \widehat p(x^{(i)}),
\qquad
\text{acc}(B_m) \;=\; \frac{1}{|B_m|}\sum_{i: \widehat p(x^{(i)})\in B_m} \mathbf{1}\{s^{(i)}=\sigma_2\}.
\]
The reliability diagram is the plot \(\{(\text{conf}(B_m),\text{acc}(B_m))\}_{m=1}^M\).  Perfect calibration corresponds to the diagonal \(\text{acc}=\text{conf}\).

\paragraph{Parametric and nonparametric calibration methods.}
\begin{itemize}
	\item \textbf{Platt scaling} (Platt, 1999). Given a classifier score \(f(x)\) (logit, margin, SVM score, etc.) Platt scaling fits a sigmoid
	\[
	\tilde p(x) = \frac{1}{1+\exp(A f(x) + B)}
	\]
	on a held-out calibration set \((x^{(i)},s^{(i)})\), by minimising the negative log-likelihood (equivalently NLL / log score) w.r.t. \(A,B\).  For multiclass one can fit one-vs-rest or use vector extensions.
	\item \textbf{Temperature scaling.} For softmax neural classifiers with logits \(z(x)\), temperature scaling returns
	\[
	\tilde p(x;\,T) = \mathrm{softmax}\big(z(x)/T\big),
	\]
	where \(T>0\) is fitted to minimise NLL on a validation set. It is a simple, single-parameter, post-hoc calibration method commonly used with deep nets.
	\item \textbf{Isotonic regression (PAV).} A nonparametric (monotone) calibration map \(g:\mathbb{R}\to[0,1]\) is fit using the Pool-Adjacent-Violators algorithm to minimise squared error on a calibration set. Isotonic regression is flexible and is guaranteed to produce a monotone mapping of model scores into probabilities.
\end{itemize}

\paragraph{Sharpness (resolution).}
Calibration alone is not sufficient: a calibrated but uninformative classifier that always outputs the base rate \(p(S=\sigma_2)\) is calibrated but useless. \emph{Sharpness} (or \emph{resolution}) measures concentration of predictive distributions â€” forecasts should be as sharp as possible subject to calibration. Proper scoring rules (e.g. log score, Brier) jointly reward calibration and sharpness; the decomposition of the Brier score below makes this explicit.

\subsubsection{Brier score decomposition (Murphy decomposition)}
Let \(\widehat p(x^{(i)})\) be binary scores on a test set of size \(n\). Partition predictions into \(M\) disjoint bins \(B_1,\dots,B_M\) and let \(n_m=|B_m|\), \(\bar r_m = \frac{1}{n_m}\sum_{i\in B_m}\widehat p(x^{(i)})\) (mean predicted probability in bin \(m\)), and \(\bar o_m = \frac{1}{n_m}\sum_{i\in B_m}\mathbf{1}\{s^{(i)}=\sigma_2\}\) (empirical event frequency). Let \(\bar o\) be the global event frequency \(\bar o=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{s^{(i)}=\sigma_2\}\). The (empirical) Brier score is
\[
\mathrm{BS}(\widehat p) \;=\; \frac{1}{n}\sum_{i=1}^n \big(\widehat p(x^{(i)}) - \mathbf{1}\{s^{(i)}=\sigma_2\}\big)^2.
\]
Murphy's decomposition (using the bins) gives
\[
\mathrm{BS} \;=\; \underbrace{\sum_{m=1}^M \frac{n_m}{n}\big(\bar r_m - \bar o_m\big)^2}_{\text{reliability (calibration)}} \;-\; \underbrace{\sum_{m=1}^M \frac{n_m}{n}\big(\bar o_m - \bar o\big)^2}_{\text{resolution (sharpness)}} \;+\; \underbrace{\bar o(1-\bar o)}_{\text{uncertainty}}.
\]
Interpretation:
\begin{itemize}
	\item The \emph{reliability} term measures squared calibration error within bins; smaller is better.
	\item The \emph{resolution} term measures how much the bins' observed frequencies differ from the global event rate; larger resolution improves the score (note the minus sign).
	\item The \emph{uncertainty} is the intrinsic difficulty of the problem and does not depend on the forecast.
\end{itemize}

\subsubsection{Expected Calibration Error (ECE)}
The Expected Calibration Error (ECE) is a widely used scalar summary of calibration based on binning.  Let \(B_1,\dots,B_M\) be a partition of \([0,1]\) (uniform bins are common). For bin \(B_m\) define the empirical confidence and accuracy as above:
\[
\operatorname{conf}(B_m) \;=\; \frac{1}{n_m}\sum_{i\in B_m}\widehat p(x^{(i)}),\qquad
\operatorname{acc}(B_m) \;=\; \frac{1}{n_m}\sum_{i\in B_m}\mathbf{1}\{s^{(i)}=\sigma_2\}.
\]
The (standard) ECE is the weighted average absolute deviation
\begin{equation}
	\label{eq:ECE}
	\operatorname{ECE} \;=\; \sum_{m=1}^M \frac{n_m}{n}\,\big|\operatorname{acc}(B_m) - \operatorname{conf}(B_m)\big|.
\end{equation}
Variants and related quantities:
\begin{itemize}
	\item \(\operatorname{MCE}=\max_m |\operatorname{acc}(B_m)-\operatorname{conf}(B_m)|\) (maximum calibration error).
	\item \(\operatorname{ECE}_2\) using squared differences (root-mean-squared calibration error).
	\item \emph{Adaptive / data-dependent binning:} choose bins so that each \(n_m\) is approximately equal (reduces high variance in low-density regions).
	\item \emph{Kernel / continuous estimators:} replace binning by kernel smoothing to estimate \(r(q)\) and compute an integrated deviation.
\end{itemize}

\paragraph{Implementation notes and caveats}
\begin{itemize}
	\item \textbf{Binning bias / variance tradeoff.} ECE depends on choice and number of bins \(M\). Too few bins understate miscalibration (low resolution), too many bins give noisy estimates (high variance). Adaptive binning or smoothing mitigate this.
	\item \textbf{ECE is not a proper scoring rule.} ECE measures calibration only, not sharpness. A trivial constant predictor equal to the base rate has ECE \(=0\) but is useless; proper scoring rules are needed to judge overall predictive utility.
	\item \textbf{Choice of score for fitting.} When learning or calibrating a model, optimise a strictly proper scoring rule (log-loss or Brier) on a held-out calibration set. Platt scaling and isotonic regression are typically fitted by minimising NLL (for Platt) or squared error (for isotonic / PAV).
	\item \textbf{Separation of data.} Always fit calibration maps (Platt/temperature/isotonic) on a validation set not used for training the base classifier; evaluate calibration on a distinct test set to avoid optimistic bias.
	\item \textbf{Multiclass extension.} ECE and reliability diagrams generalise to multiclass by focusing on the top-class probability or by vectorial decompositions; more sophisticated multiclass calibration metrics exist (e.g. classwise ECE or matrix variants).
\end{itemize}

\subsubsection{Practical recommendations}
\begin{enumerate}
	\item Use a strictly proper scoring rule (log-loss) as the primary optimisation objective when probabilistic accuracy is desired.
	\item Report both a proper score (log-loss or Brier) and a calibration metric (ECE) together with reliability diagrams. The proper score captures calibration \emph{and} sharpness; ECE isolates calibration.
	\item Use Platt scaling or temperature scaling for simple parametric post-hoc calibration; use isotonic regression when sufficient calibration data is available and monotonicity between score and probability is plausible.
	\item When reporting ECE, state the binning scheme; consider adaptive binning or kernel estimators to check robustness.
\end{enumerate}

\subsubsection{Summary}
Proper scoring rules (log-loss, Brier) provide principled objectives for probabilistic prediction and are strictly proper so that the true predictive distribution is optimal. Calibration formalises the idea that predicted probabilities should match observed frequencies; Platt scaling, temperature scaling, and isotonic regression are standard post-hoc calibration procedures. ECE (and related diagnostics) provides a practical, though approximate, scalar summary of calibration; it should be interpreted together with proper scores and graphical diagnostics (reliability diagrams) because calibration alone is not sufficient for useful prediction (sharpness / resolution is also required).
