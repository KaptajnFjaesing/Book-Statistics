\subsection{Bayesian Model Selection}
\label{sec:model_selection}
In Bayesian model selection, the goal of the Robot is to identify which of several competing models best represents Nature’s true data-generating process. Each model corresponds to a distinct probabilistic hypothesis about how the data were generated, characterized by its own parameters and likelihood function. Let 
\begin{equation}
	M \colon \Omega \to \Omega_M = \{1, \dots, K\}
\end{equation}
be a random variable representing Nature’s choice of model, where each $m \in \Omega_M$ indexes a candidate model $\mathcal{M}_m $. The Robot’s action space is $\Omega_U = \Omega_M$, meaning it must choose a model index $u \in \{1, \dots, K\}$. Given the observed dataset $D = \{(x_i, y_i)\}_{i=1}^n$, the Robot aims to minimize the expected cost (similar to \thref{theorem:opt_decision_rule})
\begin{equation}
	U^*(D) = \argmin_{U(D) \in \Omega_M} \mathbb{E}_{M|D}[C(U(D), M) \mid D],
	\label{eq:model_selection_expected_cost}
\end{equation}
where $C(u, m)$ denotes the cost (or loss) incurred by selecting model $u$ when Nature’s true model is $m$. Expanding~\eqref{eq:model_selection_expected_cost} gives
\begin{equation}
	\mathbb{E}_{M|D}[C(U(D), M) \mid D]
	= \sum_{m=1}^{K} C(U(D), m)\, p(m \mid D),
	\label{eq:model_selection_cost_expanded}
\end{equation}
where $p(m \mid D)$ denotes the posterior probability that model $m$ is the true model given data $D$. Typically, $K=2$, in which case $U^*(D) = u^{(1)}$ is picked iff 
\begin{equation} 
	\mathbb{E}_{M\mid D}[C(u^{(1)}, M)|D]<\mathbb{E}_{M\mid D}[C(u^{(2)}, M)|D], 
\end{equation} 
meaning 
\begin{equation} 
	\frac{p(m^{(1)}|D)}{p(m^{(2)}|D)}>\frac{C(u^{(1)},m^{(1)})-C(u^{(2)},m^{(2)})}{C(u^{(2)},m^{(1)})-C(u^{(1)},m^{(1)})}. 
\end{equation} 
The ratio $\frac{p(m^{(1)}|D)}{p(m^{(2)}|D)}$ is referred to as the posterior ratio\index{Posterior ratio}. Using Bayes theorem it can be re-written as follows 
\begin{equation} 
	\begin{split} 
		\text{posterior ratio} &= \frac{p(m^{(1)}|D)}{p(m^{(2)}|D)}\\ & = \frac{p(D_y|m^{(1)},D_x)p(m^{(1)})}{p(D_s|m^{(2)},D_x)p(m^{(2)})}, 
	\end{split}
\end{equation} 
where for the second equality it has been used that the normalization $p(D)$ cancels out between the denominator and nominator and \axref{ax:observation_relevance} has been employed. Given there is no a priori bias towards any model, 
\begin{equation} 
	p(m^{(1)}) = p(m^{(2)}) 
\end{equation} 
meaning 
\begin{equation} 
	\text{posterior ratio} = \frac{p(D_y|m^{(1)},D_x)}{p(D_y|m^{(2)},D_x)}. 
	\label{eq:bayes_factor} 
\end{equation} 
$p(D_y|m^{(1)},D_x)$ and $p(D_y|m^{(2)},D_x)$ can then be expanded via marginalization, the chain rule and Bayes theorem until they can be evaluated either analytically or numerically. \EQref{eq:bayes_factor} is referred to as Bayes factor\index{Bayes factor} and as a rule of thumb 

\begin{definition}[Bayes Factor Interpretation Rule of Thumb] 
	If the probability of either of two models being the model of Nature is more than 3 times likely than the other, the likelier model is accepted. Otherwise the result does not significantly favor either model. 
\end{definition}