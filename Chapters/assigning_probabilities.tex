\chapter{Assigning Probability Functions}
The axioms and definitions (\axref{ax:non_neg}-\axref{ax:add}, \dfref{eq:cond} and \dfref{eq:ind}) of probability theory can be used to define and relate probability measures, however, they are not sufficient to conduct inference because, ultimately, the probability measure or relevant probability functions (density or mass) needs to be specified. Thus, the rules for manipulating probability functions must be supplemented by rules for assigning probability functions. To assign any probability function, there is ultimately only one way, logical analysis, i.e., non-self-contradictory analysis of the available information. The difficulty is to incorporate only the information one actually possesses without making gratuitous assumptions about things one does not know. A number of procedures have been developed that accomplish this task: Logical analysis may be applied directly to the sum and product rules to yield probability functions~\citep{jaynes_11}. Logical analysis may be used to exploit the group invariances of a problem~\citep{jaynes_16}. Logical analysis may be used to ensure consistency when uninteresting or nuisance parameter are marginalized from probability functions~\citep{jaynes_21}. And last, logical analysis may be applied in the form of the principle of maximum entropy to yield probability functions \cite{zellner_bayesian_inference, jaynes_16,jaynes_19, shore_17,shore_18}. Of these techniques the principle of maximum entropy is probably the most powerful.

\section{The Principle of Maximum Entropy}
\label{sec:maxent}
The principle of maximum entropy\index{Maximum entropy}, first proposed by \citet{Jaynes1957}, addresses the problem of assigning a probability distribution to a random variable in a way that is maximally noncommittal with respect to missing information. Let $Z$ be a generic random variable describing an abstract experiment. Its probability distribution is denoted by $p(z|\lambda, I)$, parameterized by $\lambda = \{\lambda_0, \dots, \lambda_n\}$. Here, $I$ represents the background information related to the system (\dfref{def:background_information}).
\newpage
\begin{definition}[Background information]
	\label{def:background_information}
	Background information, denoted by $I$, consists of all prior knowledge, assumptions, and constraints that are available before observing the outcome of a random experiment. This includes, but is not limited to:
	\begin{enumerate}
		\item Known properties of the system or phenomenon being modeled, such as symmetries, invariances, or physical laws.
		\item Knowledge of which probability distributions or families of distributions are plausible for the random variables.
		\item Preferences, biases, or prior beliefs regarding particular modeling methods, distributions, or parameter choices.
		\item Any additional constraints, such as known moments, support, or relationships between variables.
	\end{enumerate}
	Background information formally determines the class of admissible probability distributions and methods considered suitable for representing uncertainty in the system.
\end{definition}

The maximum entropy principle asserts that the probability distribution $p(z|\lambda, I)$ that best represents the current state of knowledge is the one that maximizes the constrained entropy~\citep{Sivia2006}. Using the method of Lagrange multipliers, the constrained entropy is expressed via the Lagrangian
\begin{equation}
	\mathcal{L} = \int dz F ,
\end{equation}
with
\begin{equation}
	F = -p(z|\lambda, I) \ln \frac{p(z|\lambda, I)}{m(z)} 
	- \lambda_0 p(z|\lambda, I) - \sum_{j=1}^n \lambda_j C_j(z).
\end{equation}
Here, $m(z)$, referred to as the reference measure\index{Reference measure} or Lebesgue measure\index{Lebesgue measure}, ensures that the entropy
\begin{equation}
	- \int dz p(z|\lambda, I) \ln \frac{p(z|\lambda, I)}{m(z)}
\end{equation}
is invariant under changes of variables. The functions $C_j(z)$ represent additional constraints imposed by the background information $I$ (\dfref{def:background_information}), beyond the normalization constraint. The Lagrangian is maximized by solving the Euler-Lagrange equation
\begin{equation}
	\frac{\partial F}{\partial p(z|\lambda, I)} - \frac{d}{dz} \frac{\partial F}{\partial p'(z|\lambda, I)} = 0,
\end{equation}
where $p'(z|\lambda, I) = \partial p(z|\lambda, I)/\partial z$. Since $F$ does not depend on derivatives of $p$, the equation simplifies to
\begin{equation}
	\frac{\partial F}{\partial p(z|\lambda, I)} = 0.
\end{equation}
From this, the maximum entropy distribution is obtained as
\begin{equation}
	\frac{\partial F}{\partial p(z|\lambda, I)} = - \ln\frac{p(z|\lambda, I)}{m(z)} - 1 - \sum_j \lambda_j C_j(z) = 0,
\end{equation}
leading to
\begin{equation}
	p(z|\lambda, I) = m(z) e^{-1 - \sum_j \lambda_j C_j(z)} = \tilde{m}(z) e^{- \sum_j \lambda_j C_j(z)},
\end{equation}
where $\tilde{m}(z) \equiv m(z) e^{-1}$. Enforcing normalization $\int dz p(z|\lambda, I) = 1$ gives
\begin{equation}
	p(z|\lambda, I) = \frac{\tilde{m}(z) e^{- \sum_j \lambda_j C_j(z)}}{\int dz' \tilde{m}(z') e^{- \sum_j \lambda_j C_j(z')}}.
\end{equation}
The reference measure $m$ is invariant under parameter transformations, and the Lagrange multipliers $\lambda_j$ are determined by the additional constraints, e.g., on the mean or variance of $Z$.

\begin{example}
	\index{Example: Maximum entropy normal distribution}
	Consider a random variable, $Z$, with unlimited support, $z\in [-\infty,\infty]$, assumed to be symmetric around a single peak defined by the mean $\mu$, standard deviation $\sigma$. In this case $\lambda = \{\lambda_0,\lambda_1,\lambda_2\}$, where it will be shown that $\lambda_1,\lambda_2$ are related to $\mu,\sigma$. In this case $F$ can be written\label{ex:gauss}
	\begin{equation}
		\begin{split}
			F =& -p(z|\lambda,I)\ln\bigg(\frac{p(z|\lambda,I)}{m(z)}\bigg)-\lambda_0p(z|\lambda,I)\\
			&-\lambda_1p(z|\lambda,I)z-\lambda_2p(z|\lambda,I)z^2
		\end{split}
	\end{equation}
	with the derivative
	\begin{equation}
		\begin{split}
			\frac{\partial F}{\partial p(z|\lambda,I)} &= -1-\ln\bigg(\frac{p(z|\lambda,I)}{m(z)}\bigg)-\lambda_1z-\lambda_2z^2\\
			&=0,
		\end{split}
	\end{equation}
	meaning
	\begin{equation}
		p(z|\lambda,I)=m(z)e^{-1-\lambda_0-\lambda_1z-\lambda_2z^2}.
	\end{equation}
	Taking a unifoirm measure ($m= const$) and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\int dz p(z|\lambda,I) &= me^{-1-\lambda_0}\int dz e^{-\lambda_1z-\lambda_2z^2}\\
			&= me^{-1-\lambda_0}\sqrt{\frac{\pi}{\lambda_2}}e^{\frac{\lambda_1^2}{4\lambda_2}}\\
			&=1.
		\end{split}
	\end{equation}
	Defining $K^{-1} = me^{-1-\lambda_0}$ yields
	\begin{equation}
		\begin{split}
			p(z|\lambda,I) &= \frac{e^{-\lambda_1x-\lambda_2x^2}}{K}\\
			&= \sqrt{\frac{\lambda_2}{\pi}}e^{-\frac{\lambda_1^2}{4\lambda_2}-\lambda_1z-\lambda_2z^2}.\\
		\end{split}
	\end{equation}
	Now, imposing the mean constraint
	\begin{equation}
		\begin{split}
			\int dz zp(z|\lambda,I) &= \frac{\int dz ze^{-\lambda_1z-\lambda_2z^2}}{K}\\
			&= -\frac{\lambda_1}{2\lambda_2}\\
			&=\mu.
		\end{split}
	\end{equation}
	Hereby
	\begin{equation}
		\begin{split}
			p(z|\lambda,I) &= \sqrt{\frac{\lambda_2}{\pi}}e^{-\mu^2\lambda_2+2\mu \lambda_2z-\lambda_2z^2}\\
			&= \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2}\big(\frac{\mu-z}{\sigma}\big)^2},\\\\
		\end{split}
	\end{equation}
	where $\sigma\equiv \frac{1}{2\lambda_2}$ has been defined. Hence, it is clear that the normal distribution\index{Normal distribution} can be derived from general constraints via the principle of maximum entropy\index{Maximum entropy}.
\end{example}

\begin{example}
	\index{Example: Maximum entropy beta distribution}
	Consider a random variable, $Z$, with limited support, $z\in [0,1]$. In order to impose the limited support, require that $\ln(z)$ and $\ln(1-z)$ be well defined. In this case $F$ can be written\label{ex:beta}
	\begin{equation}
		\begin{split}
			F =& -p(z|\lambda,I)\ln\bigg(\frac{p(z|\lambda,I)}{m(z)}\bigg)-\lambda_0p(z|\lambda,I)\\
			&-\lambda_1p(z|\lambda,I)\ln(z)-\lambda_2p(z|\lambda,I)\ln(1-z)
		\end{split}
	\end{equation}
	with the derivative
	\begin{equation}
		\begin{split}
			\frac{\partial F}{\partial p(z|\lambda,I)} &= -1-\ln\bigg(\frac{p(z|\lambda,I)}{m(z)}\bigg)-\lambda_1\ln(z)-\lambda_2\ln(1-z)\\
			&=0,
		\end{split}
	\end{equation}
	meaning
	\begin{equation}
		p(z|\lambda,I)=m(z)e^{-1-\lambda_0-\lambda_1\ln(z)-\lambda_2\ln(1-z)}.
	\end{equation}
	Taking a unifoirm measure ($m= const$) and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\int dz p(z|\lambda,I) &= me^{-1-\lambda_0}\int dz z^{-\lambda_1}(1-z)^{-\lambda_2}\\
			&= me^{-1-\lambda_0}\frac{\Gamma(1-\lambda_1)\Gamma(1-\lambda_2)}{\Gamma(2-\lambda_1-\lambda_2)}\\
			&=1.
		\end{split}
	\end{equation}
	Now define $\alpha \equiv 1-\lambda_1\wedge \beta \equiv 1-\lambda_2$. Hereby
	\begin{equation}
		p(z|\alpha,\beta,I) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1},
	\end{equation}
	which is the beta distribution\index{Maximum entropy}. 
\end{example}


\begin{example}
	\index{Example: Maximum entropy Gamma distribution}
	Consider a continuous random variable $Z \in [0,\infty)$ with known mean $\mu$ and known logarithmic mean $\nu$. In this case
	\begin{equation}
		F = - p(z|\lambda,I) \ln \frac{p(z|\lambda,I)}{m(z)} - \lambda_0 p(z|\lambda,I) - \lambda_1 z p(z|\lambda,I) - \lambda_2 \ln(z) p(z|\lambda,I),
	\end{equation}
	and derivative
	\begin{equation}
		\begin{split}
			\frac{\partial F}{\partial p(z|\lambda,I)} &= -1 - \ln\frac{p(z|\lambda,I)}{m(z)} - \lambda_0 - \lambda_1 z - \lambda_2 \ln z\\
			& = 0
		\end{split}
	\end{equation}
	meaning
	\begin{equation}
		\begin{split}
			p(z|\lambda,I)&= m(z) e^{-1-\lambda_0 - \lambda_1 z - \lambda_2 \ln z}\\
			& = \tilde{m}(z)  z^{-\lambda_2} e^{-\lambda_1 z}
		\end{split}
		\label{eq:gam1}
	\end{equation}
	where $\tilde{m}(z) = m(z) e^{-1-\lambda_0}$. Taking a uniform measure ($m(z) = \text{const}$) and imposing normalization
	\begin{equation}
		\begin{split}
			\int_0^\infty dz p(z|\lambda,I) &= \tilde{m} \int_0^\infty dz z^{-\lambda_2} e^{-\lambda_1 z}\\
			& = 1.
		\end{split}
	\end{equation}
	The integral is recognized as the Gamma function
	\begin{equation}
		\int_0^\infty z^{\alpha-1} dz e^{-\beta z} = \frac{\Gamma(\alpha)}{\beta^\alpha}
	\end{equation}
	with $\alpha = 1 - \lambda_2$ and $\beta = \lambda_1$. Substituting $\tilde{m}$, $\alpha$, $\beta$ back into \EQref{eq:gam1}
	\begin{equation}
		p(z|\lambda,I) = \frac{\beta^\alpha}{\Gamma(\alpha)} z^{\alpha-1} e^{-\beta z},
	\end{equation}
	which is the Gamma distribution\index{Maximum entropy}.
\end{example}

\begin{example}
	\index{Example: Maximum entropy Exponential distribution}
	Consider a continuous random variable $Z \in [0,\infty)$ with known mean $\mu$. In this case
	\begin{equation}
		F = - p(z|\lambda,I) \ln \frac{p(z|\lambda,I)}{m(z)} - \lambda_0 p(z|\lambda,I) - \lambda_1 z p(z|\lambda,I),
	\end{equation}
	and derivative
	\begin{equation}
		\frac{\partial F}{\partial p(z|\lambda,I)} = -1 - \ln\frac{p(z|\lambda,I)}{m(z)} - \lambda_0 - \lambda_1 z = 0.
	\end{equation}
	Solving for $p(z|\lambda,I)$ gives
	\begin{equation}
		p(z|\lambda,I) = m(z) e^{-1-\lambda_0 - \lambda_1 z}.
		\label{eq:n13}
	\end{equation}
	Taking $m(z) = \text{const}$ and imposing normalization constraint
	\begin{equation}
		\begin{split}
			\int_0^\infty dz p(z|\lambda,I) &= m e^{-1-\lambda_0}\int_0^\infty dz e^{- \lambda_1 z}\\
			& = m e^{-1-\lambda_0} \frac{1}{\lambda_1}\\
			&= 1\\
		\end{split}
		\label{eq:na1}
	\end{equation}
	and the mean constraint
	\begin{equation}
		\begin{split}
			\int_0^\infty dz z p(z|\lambda,I) &= \int_0^\infty dz z \lambda_1 e^{-\lambda_1 z}\\
			& = \frac{1}{\lambda_1}\\
			&= \mu.
		\end{split}
		\label{eq:na2}
	\end{equation}
	Combining \EQref{eq:na1}, \EQref{eq:na2} and \EQref{eq:n13} yields
	\begin{equation}
		p(z|\lambda,I) = \frac{\beta^\alpha}{\Gamma(\alpha)} z^{\alpha-1} e^{-\beta z},
	\end{equation}
	which is the Gamma distribution\index{Maximum entropy}.
\end{example}



\begin{example}
	\index{Example: Maximum entropy bernoulli distribution}
	Consider a random variable $Z\in\{0,1\}$ with mean $\mu$. In this case
	\begin{equation}
		\mathcal{L} = \sum_{z=0}^1 F
	\end{equation}
	 $F$ can be written
	\begin{equation}
		F = - p(z|\lambda,I)\ln\frac{p(z|\lambda,I)}{m(z)} - \lambda_0 p(z|\lambda,I) - \lambda_1 zp(z|\lambda,I),
	\end{equation}
	with the derivative
	\begin{equation}
		\begin{split}
			\frac{\partial F}{\partial p(z|\lambda,I)} &= -1 - \ln\bigg(\frac{p(z|\lambda,I)}{m(z)}\bigg) - \lambda_0 - \lambda_1 z\\
			& = 0,
		\end{split}
	\end{equation}
	meaning
	\begin{equation}
		p(z|\lambda,I) = m(z) e^{-1-\lambda_0 - \lambda_1 z}.
	\end{equation}
	Taking a uniform measure ($m= const$) and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^1 p(z) &=m e^{-1-\lambda_0}\bigg(1+ e^{- \lambda_1}\bigg)\\
			&=1
		\end{split}
	\end{equation}
	and mean constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^1 zp(z) &=m e^{-1-\lambda_0}e^{- \lambda_1}\\
			& = \frac{1}{1+ e^{\lambda_1}}\\
			&=\mu
		\end{split}
	\end{equation}
	This means
	\begin{equation}
		\begin{split}
			p(0|\lambda,I) &= m e^{-1-\lambda_0}\\
			&= \frac{1}{1+ e^{- \lambda_1}}\\
			& = 1-\mu
		\end{split}
	\end{equation}	
	and
	\begin{equation}
		\begin{split}
			p(1|\lambda,I) &= m e^{-1-\lambda_0-\lambda_1}\\
			&=\mu,
		\end{split}
	\end{equation}
	or 
	\begin{equation}
		p(z|\lambda,I) = \mu^z (1-\mu)^{1-z}.
	\end{equation}
	which is the Bernoulli distribution\index{Maximum entropy}.
\end{example}

\begin{example}
	\index{Example: Maximum entropy Binomial distribution}
	Consider $Z \in \{0,1,\dots,n\}$ representing the total number of successes in $n$ independent Bernoulli trials with mean $\mu$. Using the principle of maximum entropy, define
	\begin{equation}
		F = - p(z|\lambda,I) \ln \frac{p(z|\lambda,I)}{m(z)} - \lambda_0 p(z|\lambda,I) - \lambda_1 z\, p(z|\lambda,I),
	\end{equation}
	with the derivative
	\begin{equation}
		\begin{split}
			\frac{\partial F}{\partial p(z|\lambda,I)} &= -1 - \ln \frac{p(z|\lambda,I)}{m(z)} - \lambda_0 - \lambda_1 z\\
			& = 0,
		\end{split}
	\end{equation}
	which implies
	\begin{equation}
		p(z|\lambda,I) = m(z) e^{-\lambda_0 - \lambda_1 z}.
	\end{equation}
	Taking a uniform measure for the underlying sequences of Bernoulli trials, equivalent to $m(z) = \binom{n}{z}$, and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^n p(z|\lambda,I) &= \sum_{z=0}^n \binom{n}{z} e^{-\lambda_0 - \lambda_1 z}\\
			& = 1,
		\end{split}
	\end{equation}
	yields
	\begin{equation}
		e^{-\lambda_0} = (1 + e^{-\lambda_1})^{-n}.
	\end{equation}
	The mean constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^n z\, p(z|\lambda,I) &= n \frac{e^{-\lambda_1}}{1 + e^{-\lambda_1}}\\
			& = n\mu
		\end{split}
	\end{equation}
	gives
	\begin{equation}
		e^{-\lambda_1} = \frac{\mu}{1-\mu}.
	\end{equation}
	Finally, substituting $e^{-\lambda_0}$ and $e^{-\lambda_1}$ into $p(z|\lambda,I)$ gives the maximum entropy distribution
	\begin{equation}
		p(z|\lambda,I) = \binom{n}{z} \mu^z (1-\mu)^{n-z},
	\end{equation}
	which is the Binomial distribution\index{Maximum entropy}.
\end{example}


\begin{example}
	\index{Example: Maximum entropy Poisson distribution}
	Consider a random variable $Z \in \{0,1,2,\dots\}$ with a known mean $\mu$. In this case
	\begin{equation}
		\mathcal{L} = \sum_{z=0}^\infty F
	\end{equation}
	and $F$ can be written
	\begin{equation}
		F = -p(z|\lambda,I)\ln\frac{p(z|\lambda,I)}{m(z)} - \lambda_0 p(z|\lambda,I) - \lambda_1 z p(z|\lambda,I),
	\end{equation}
	with the derivative
	\begin{equation}
		\begin{split}
			\frac{\partial F}{\partial p(z|\lambda,I)} &= -1 - \ln\bigg(\frac{p(z|\lambda,I)}{m(z)}\bigg) - \lambda_0 - \lambda_1 z \\
			&= 0,
		\end{split}
	\end{equation}
	meaning
	\begin{equation}
		p(z|\lambda,I) = m(z) e^{-1-\lambda_0 - \lambda_1 z}.
		\label{eq:qsa}
	\end{equation}
	For the Poisson distribution, the measure encodes the counting of configurations for each $z$ and is given by $m(z) = 1/z!$. Imposing this measure and the normalization constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^{\infty} p(z|\lambda,I) &= \sum_{z=0}^{\infty} \frac{e^{-1-\lambda_0 - \lambda_1 z}}{z!}\\
			& = e^{-1-\lambda_0} \sum_{z=0}^{\infty} \frac{e^{-\lambda_1 z}}{z!}\\
			& = 1,
		\end{split}
		\label{eq:asd}
	\end{equation}
	Identifying the sum with the Taylor expansion
	\begin{equation}
		\sum_{z=0}^{\infty} \frac{e^{-\lambda_1 z}}{z!} = e^{e^{-\lambda}}
	\end{equation}
	yields
	\begin{equation}
		e^{-1-\lambda_0} = e^{-e^{-\lambda_1}} \quad \Rightarrow \quad 1+\lambda_0 = e^{-\lambda_1}.
		\label{eq:qwe}
	\end{equation}
	Imposing the mean constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^{\infty} z p(z|\lambda,I)& = e^{-1-\lambda_0} \sum_{z=1}^{\infty} \frac{z e^{-\lambda_1 z}}{z!}\\
			& = e^{-1-\lambda_0} \sum_{z=1}^{\infty} \frac{e^{-\lambda_1 z}}{(z-1)!}\\
			& = e^{-\lambda_1} e^{-1-\lambda_0}\sum_{y=0}^{\infty} \frac{e^{-\lambda_1 y}}{y!}\\
			& = e^{-\lambda_1}\\
			& = \mu
		\end{split}
		\label{eq:qty}
	\end{equation}
	where \EQref{eq:asd} and $y = z-1$ has been used. Combining \EQref{eq:qsa}, \EQref{eq:qwe} and \EQref{eq:qty} yield
	\begin{equation}
		p(z|\lambda,I) = \frac{\mu^ze^{-\mu }}{z!} .
	\end{equation}
	which is the Poisson distribution\index{Maximum entropy}.
\end{example}


