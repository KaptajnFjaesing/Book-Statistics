\chapter{Assigning Probability Functions}
While \chref{chp:probaiblity_theory} provides the formal definition of probability measures and their manipulation, it is not sufficient on its own to conduct inference. In practice, one must also specify the probability measure or, equivalently, the probability density or mass functions. Assigning these functions requires a principled method to convert available information into a probability distribution.

The central challenge is to incorporate only the information that is actually known, without introducing unwarranted assumptions about unknown quantities. Logical analysis provides the framework for this task: it ensures that probability assignments are internally consistent and make full use of the information at hand. Several approaches implement this principle. Logical analysis can be applied directly to the sum and product rules to construct probability functions~\citep{jaynes_11}; it can exploit group invariances inherent in the problem~\citep{jaynes_16}; and it can ensure consistent marginalization of nuisance parameters~\citep{jaynes_21}.

Among these methods, the principle of maximum entropy\citep{Jaynes1957} stands out for its generality and power. By selecting the probability distribution that maximizes entropy subject to the known constraints, it provides a systematic, non-arbitrary means of assigning probabilities while remaining maximally noncommittal about unknown information~\cite{zellner_bayesian_inference, jaynes_16, jaynes_19, shore_17, shore_18}. The remainder of this chapter develops the maximum entropy principle and illustrates how it can be applied to derive probability distributions from partial knowledge.

\section{The Principle of Maximum Entropy}
\label{sec:maxent}
The principle of maximum entropy\index{Maximum entropy}, first proposed by \citet{Jaynes1957}, addresses the problem of assigning a probability distribution to a random variable in a way that is maximally noncommittal with respect to missing information. Let
\begin{equation}
	Z: \Omega \to \Omega_Z
\end{equation}
be a generic random variable\index{Random variable} from the probability space\index{Probability space} $(\Omega, \mathcal{F}, \mathbb{P})$ to the probability space $(\Omega_Z,\mathcal{F}_Z, \mathbb{P}_Z)$, where $\mathbb{P}_Z$ is the image measure\index{Image measure} of $\mathbb{P}$. The goal of the maximum entropy principle is to determine the probability measure $\mathbb{P}_Z$ that best represents the current state of knowledge, given background information $I$ and a set of moment constraints. 

\begin{definition}[Background information]
	\label{def:background_information}
	Background information\index{Background information}, denoted by $I$, consists of all prior knowledge, assumptions, and constraints that are available before observing the outcome of a random experiment. This includes, but is not limited to:
	\begin{enumerate}
		\item Known properties of the system or phenomenon being modeled, such as symmetries, invariances, or physical laws.
		\item Knowledge of which probability distributions or families of distributions are plausible for the random variables.
		\item Preferences, biases, or prior beliefs regarding particular modeling methods, distributions, or parameter choices.
		\item Any additional constraints, such as known moments, support, or relationships between variables.
	\end{enumerate}
	Background information formally determines the class of admissible probability distributions and methods considered suitable for representing uncertainty in the system.
\end{definition}

From definition \dfref{def:prob_density_general}, the image measure\index{Image measure} $\mathbb{P}_Z$ admits a density $f_Z$ with respect to the measure $\mu$, so that for any event $B\in \mathcal{F}_Z$,
\begin{equation}
	\mathbb{P}_Z(B \mid\gamma,I) = \int_B f_Z(z\mid \gamma,I) d\mu(z),
	\label{eq:max_ent_probability_measure}
\end{equation}
where $f_Z$ is the PMF\index{Probaility mass function} for a discrete sample space and the PDF\index{Probability density function} for a continuous sample space. The maximum entropy principle asserts that the density $f_Z(z | \gamma, I)$ that best represents the current state of knowledge is the one that maximizes the constrained Shannon entropy~\citep{Sivia2006}, where $\gamma$ denotes the parameters of the distribution and $I$ the background information. The Shannon entropy\index{Shannon entropy} of a probability density $f_Z$ can be written
\begin{equation}
	H[f_Z] = - \int_{\Omega_Z} f_Z(z \mid \gamma, I) 
	\ln \frac{f_Z(z \mid \gamma, I)}{m(z)}  d\mu(z),
\end{equation}
where $m(z)$ is a reference measure\index{Reference measure} that ensures invariance of the entropy under reparameterizations of $z$. To incorporate known constraints, such as moment conditions or normalization, one introduces Lagrange multipliers $\gamma_0, \gamma_1, \dots, \gamma_n$ and defines the Lagrangian functional, which represents the constrained entropy\index{Constrained entropy}:
\begin{equation}
	\mathcal{L}[f_Z] = - \int_{\Omega_Z} 
	f_Z(z \mid \gamma, I) \Bigg(
	\ln \frac{f_Z(z \mid \gamma, I)}{m(z)}
	+ \gamma_0 + \sum_{j=1}^n \gamma_j C_j(z)
	\Bigg) d\mu(z),
\end{equation}
where each $C_j(z)$ denotes a constraint function. Maximizing $\mathcal{L}[f_Z]$ with respect to $f_Z$ yields the probability distribution of maximum entropy consistent with the given constraints. The maximum of $\mathcal{L}$ with respect to $f_Z$ is defined by the Euler-Lagrange condition
\begin{equation}
	\frac{\partial}{\partial f_Z} f_Z\Big(\ln \tfrac{f_Z}{m} + \gamma_0 + \sum_{j=1}^n \gamma_j C_j\Big) = 0,
\end{equation}
which simplifies to
\begin{equation}
	\ln \frac{f_Z(z \mid \gamma, I)}{m(z)} + 1 + \gamma_0 + \sum_{j=1}^n \gamma_j C_j(z) = 0.
\end{equation}
Hence the maximum-entropy distribution takes the exponential family form
\begin{equation}
	\begin{split}
		f_Z(z \mid \gamma, I)
		&= m(z) e^{-1 - \gamma_0 - \sum_{j=1}^n \gamma_j C_j(z)} \\
		&= \frac{m(z) e^{-\sum_{j=1}^n \gamma_j C_j(z)}}{
			\int_{\Omega_Z} m(z') e^{-\sum_{j=1}^n \gamma_j C_j(z')} d\mu(z')}.\\
	\end{split}
\end{equation}
The constants $\gamma_j$ are determined by the imposed constraints. The resulting probability measure (\EQref{eq:max_ent_probability_measure}) defines the unique maximum-entropy probability measure consistent with the given information.

\newpage
\begin{example}
	\index{Example: Maximum entropy normal distribution}
	Consider a continuous random variable, $Z$, with sample space $\Omega_Z=\mathbb{R}$, assumed to be symmetric around a mean $\mu$ and with variance $\sigma^2$. In this case
	\begin{equation}
			f_Z(z \mid \gamma, I)= m(z) e^{-1 - \gamma_0 - \gamma_1z-\gamma_2z^2}.
	\end{equation}
	Taking a uniform measure ($m= const$) and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\int_{\Omega_Z} f_Z(z|\gamma,I) dz &= me^{-1-\gamma_0}\int_{\Omega_Z} e^{-\gamma_1z-\gamma_2z^2} dz\\
			&= me^{-1-\gamma_0}\sqrt{\frac{\pi}{\gamma_2}}e^{\frac{\gamma_1^2}{4\gamma_2}}\\
			&=1.
		\end{split}
	\end{equation}
	Defining $K^{-1} = me^{-1-\gamma_0}$ yields
	\begin{equation}
		\begin{split}
			f_Z(z|\gamma,I) &= \frac{e^{-\gamma_1z-\gamma_2z^2}}{K}\\
			&= \sqrt{\frac{\gamma_2}{\pi}}e^{-\frac{\gamma_1^2}{4\gamma_2}-\gamma_1z-\gamma_2z^2}.\\
		\end{split}
	\end{equation}
	Now, imposing the mean constraint
	\begin{equation}
		\begin{split}
			\int_{\Omega_Z} zf_Z(z|\gamma,I) dz &= \frac{\int_{\Omega_Z} ze^{-\gamma_1z-\gamma_2z^2}dz}{K}\\
			&= -\frac{\gamma_1}{2\gamma_2}\\
			&=\mu.
		\end{split}
	\end{equation}
	Hereby
	\begin{equation}
		\begin{split}
			f_Z(z|\gamma,I) &= \sqrt{\frac{\gamma_2}{\pi}}e^{-\mu^2\gamma_2+2\mu \gamma_2z-\gamma_2z^2}\\
			&= \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2}\big(\frac{\mu-z}{\sigma}\big)^2},\\\\
		\end{split}
		\label{eq:norm1}
	\end{equation}
	where $\sigma\equiv \frac{1}{2\gamma_2}$ has been defined. \EQref{eq:norm1} can be identified as the normal distribution\index{Normal distribution}\index{Maximum entropy}.
\end{example}

\begin{example}
	\index{Example: Maximum entropy beta distribution}
	Consider a continuous random variable, $Z$, with sample space $\Omega_S =[0,1]$. In order to impose the limited support, require that $\ln(z)$ and $\ln(1-z)$ be well defined. In this case
	\begin{equation}
		f_Z(z|\gamma,I)=m(z)e^{-1-\gamma_0-\gamma_1\ln z-\gamma_2\ln(1-z)}.
	\end{equation}
	Taking a uniform measure ($m= const$) and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\int_{\Omega_Z} f_Z(z|\gamma,I) &= me^{-1-\gamma_0}\int_{\Omega_Z} z^{-\gamma_1}(1-z)^{-\gamma_2}dz\\
			&= me^{-1-\gamma_0}\frac{\Gamma(1-\gamma_1)\Gamma(1-\gamma_2)}{\Gamma(2-\gamma_1-\gamma_2)}\\
			&=1.
		\end{split}
	\end{equation}
	Now define $\alpha \equiv 1-\gamma_1\wedge \beta \equiv 1-\gamma_2$. Hereby
	\begin{equation}
		f_Z(z|\alpha,\beta,I) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1},
	\end{equation}
	which is the beta distribution\index{Maximum entropy}\index{Beta distribuion}. 
\end{example}


\begin{example}
	\index{Example: Maximum entropy Gamma distribution}
	Consider a continuous random variable, $Z$, with sample space $\Omega_Z= [0,\infty)$, a known mean $\mu$ and a known logarithmic mean $\nu$. In this case
	\begin{equation}
		\begin{split}
			f_Z(z|\gamma,I)&= m(z) e^{-1-\gamma_0 - \gamma_1 z - \gamma_2 \ln z}\\
			& = \tilde{m}(z)  z^{-\gamma_2} e^{-\gamma_1 z}
		\end{split}
		\label{eq:gam1}
	\end{equation}
	where $\tilde{m}(z) = m(z) e^{-1-\gamma_0}$. Taking a uniform measure ($m(z) = \text{const}$) and imposing normalization
	\begin{equation}
		\begin{split}
			\int_{\Omega_Z} f_Z(z|\gamma,I) dz &= \tilde{m} \int{\Omega_Z} z^{-\gamma_2} e^{-\gamma_1 z} dz\\
			& = 1.
		\end{split}
	\end{equation}
	The integral is recognized as the Gamma function
	\begin{equation}
		\int_{\Omega_Z} z^{\alpha-1}  e^{-\beta z} dz = \frac{\Gamma(\alpha)}{\beta^\alpha}
	\end{equation}
	with $\alpha = 1 - \gamma_2$ and $\beta = \gamma_1$. Substituting $\tilde{m}$, $\alpha$, $\beta$ back into \EQref{eq:gam1}
	\begin{equation}
		f_Z(z|\gamma,I) = \frac{\beta^\alpha}{\Gamma(\alpha)} z^{\alpha-1} e^{-\beta z},
	\end{equation}
	which is the Gamma distribution\index{Maximum entropy}.
\end{example}

\begin{example}
	\index{Example: Maximum entropy Exponential distribution}
	Consider a continuous random variable, $Z$, with sample space $\Omega_Z = [0,\infty)$ and known mean $\mu$. In this case
	\begin{equation}
		f_Z(z|\gamma,I) = m(z) e^{-1-\gamma_0 - \gamma_1 z}.
		\label{eq:n13}
	\end{equation}
	Taking $m(z) = \text{const}$ and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\int_{\Omega_Z} f_Z(z|\gamma,I) dz &= m e^{-1-\gamma_0}\int_{\Omega_Z} e^{- \gamma_1 z} dz\\
			& = m e^{-1-\gamma_0} \frac{1}{\gamma_1}\\
			&= 1\\
		\end{split}
		\label{eq:na1}
	\end{equation}
	and the mean constraint
	\begin{equation}
		\begin{split}
			\int_{\Omega_Z} z p(z|\gamma,I) dz&= \int_{\Omega_Z} z \gamma_1 e^{-\gamma_1 z} dz\\
			& = \frac{1}{\gamma_1}\\
			&= \mu.
		\end{split}
		\label{eq:na2}
	\end{equation}
	Combining \EQref{eq:na1}, \EQref{eq:na2} and \EQref{eq:n13} yields
	\begin{equation}
		f_Z(z|\gamma,I) = \frac{\beta^\alpha}{\Gamma(\alpha)} z^{\alpha-1} e^{-\beta z},
	\end{equation}
	which is the Gamma distribution\index{Maximum entropy}.
\end{example}



\begin{example}
	\index{Example: Maximum entropy bernoulli distribution}
	Consider a discrete random variable, $Z$, with sample space $\Omega_Z= \{0,1\}$ and mean $\mu$. In this case
	\begin{equation}
		f_Z(z|\gamma,I) = m(z) e^{-1-\gamma_0 - \gamma_1 z}.
	\end{equation}
	Taking a uniform measure ($m= const$) and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^1 f_Z(z) &=m e^{-1-\gamma_0}\bigg(1+ e^{- \gamma_1}\bigg)\\
			&=1
		\end{split}
	\end{equation}
	and mean constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^1 zf_Z(z) &=m e^{-1-\gamma_0}e^{- \gamma_1}\\
			& = \frac{1}{1+ e^{\gamma_1}}\\
			&=\mu
		\end{split}
	\end{equation}
	This means
	\begin{equation}
		\begin{split}
			f_Z(0|\gamma,I) &= m e^{-1-\gamma_0}\\
			&= \frac{1}{1+ e^{- \gamma_1}}\\
			& = 1-\mu
		\end{split}
	\end{equation}	
	and
	\begin{equation}
		\begin{split}
			f_Z(1|\gamma,I) &= m e^{-1-\gamma_0-\gamma_1}\\
			&=\mu,
		\end{split}
	\end{equation}
	or 
	\begin{equation}
		f_Z(z|\gamma,I) = \mu^z (1-\mu)^{1-z}.
	\end{equation}
	which is the Bernoulli distribution\index{Maximum entropy}.
\end{example}

\begin{example}
	\index{Example: Maximum entropy Binomial distribution}
	Consider a discrete random variable, $Z$, with sample space $\Omega_Z = \{0,1,\dots,n\}$ representing the total number of successes in $n$ independent Bernoulli trials with mean $\mu$. In this case
	\begin{equation}
		f_Z(z|\gamma,I) = m(z) e^{-\gamma_0 - \gamma_1 z}.
	\end{equation}
	Taking a uniform measure for the underlying sequences of Bernoulli trials, equivalent to the counting measure\index{Counting measure} $m(z) = \binom{n}{z}$, and imposing the normalization constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^n f_Z(z|\gamma,I) &= \sum_{z=0}^n \binom{n}{z} e^{-\gamma_0 - \gamma_1 z}\\
			& = 1,
		\end{split}
	\end{equation}
	yields
	\begin{equation}
		e^{-\gamma_0} = (1 + e^{-\gamma_1})^{-n}.
	\end{equation}
	The mean constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^n z f_Z(z|\gamma,I) &= n \frac{e^{-\gamma_1}}{1 + e^{-\gamma_1}}\\
			& = n\mu
		\end{split}
	\end{equation}
	gives
	\begin{equation}
		e^{-\gamma_1} = \frac{\mu}{1-\mu}.
	\end{equation}
	Finally, substituting $e^{-\gamma_0}$ and $e^{-\gamma_1}$ into $f_Z(z|\gamma,I)$ gives the maximum entropy distribution
	\begin{equation}
		f_Z(z|\gamma,I) = \binom{n}{z} \mu^z (1-\mu)^{n-z},
	\end{equation}
	which is the Binomial distribution\index{Maximum entropy}.
\end{example}


\begin{example}
	\index{Example: Maximum entropy Poisson distribution}
	Consider a discrete random variable $Z$ with sample space $\Omega_Z = \mathbb{N}_0$ with a known mean $\mu$. In this case
	\begin{equation}
		f_Z(z|\gamma,I) = m(z) e^{-1-\gamma_0 - \gamma_1 z}.
		\label{eq:qsa}
	\end{equation}
	Take the counting measure\index{Counting measure} $m(z) = 1/z!$ and impose the normalization constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^{\infty} f_Z(z|\gamma,I) &= \sum_{z=0}^{\infty} \frac{e^{-1-\gamma_0 - \gamma_1 z}}{z!}\\
			& = e^{-1-\gamma_0} \sum_{z=0}^{\infty} \frac{e^{-\gamma_1 z}}{z!}\\
			& = 1,
		\end{split}
		\label{eq:asd}
	\end{equation}
	Identifying the sum with the Taylor expansion
	\begin{equation}
		\sum_{z=0}^{\infty} \frac{e^{-\gamma_1 z}}{z!} = e^{e^{-\gamma}}
	\end{equation}
	yields
	\begin{equation}
		e^{-1-\gamma_0} = e^{-e^{-\gamma_1}} \quad \Rightarrow \quad 1+\gamma_0 = e^{-\gamma_1}.
		\label{eq:qwe}
	\end{equation}
	Imposing the mean constraint
	\begin{equation}
		\begin{split}
			\sum_{z=0}^{\infty} z f_Z(z|\gamma,I)& = e^{-1-\gamma_0} \sum_{z=1}^{\infty} \frac{z e^{-\gamma_1 z}}{z!}\\
			& = e^{-1-\gamma_0} \sum_{z=1}^{\infty} \frac{e^{-\gamma_1 z}}{(z-1)!}\\
			& = e^{-\gamma_1} e^{-1-\gamma_0}\sum_{y=0}^{\infty} \frac{e^{-\gamma_1 y}}{y!}\\
			& = e^{-\gamma_1}\\
			& = \mu
		\end{split}
		\label{eq:qty}
	\end{equation}
	where \EQref{eq:asd} and $y = z-1$ has been used. Combining \EQref{eq:qsa}, \EQref{eq:qwe} and \EQref{eq:qty} yield
	\begin{equation}
		f_Z(z|\gamma,I) = \frac{\mu^ze^{-\mu }}{z!} .
	\end{equation}
	which is the Poisson distribution\index{Maximum entropy}.
\end{example}



