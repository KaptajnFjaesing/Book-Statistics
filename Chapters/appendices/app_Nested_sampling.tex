\chapter{Nested Sampling}
\label{app:NS}
A major challenge in estimating the evidence via conventional Monte Carlo Methods is that generally the prior is a very broad and regular distribution whereas the likelihood is a very narrow and irregular distribution. This poses a challenge when the evidence is estimated conventionally, i.e. as the mean of the likelihood evaluated at points in parameter space corresponding to samples from the prior distribution. For a reasonable number of samples, the conventional procedure has a relatively high likelihood of relatively poor sampling in regions near the peaks in the likelihood distribution. This means a conventional estimate of the evidence via Monte Carlo Methods has a high variance. Nested Sampling~\citep{skilling2004} (NS) address this challenge by accounting for the likelihood distribution when sampling the prior distribution. Consider the integral
\begin{equation}
	Z  = \int L(\theta)\pi(\theta)d\theta,
\end{equation}
with $L$ being the likelihood distribution and $\pi$ the prior distribution. Conventional Monte Carlo methods approximate this integral via importance sampling, meaning
\begin{equation}
	\begin{split}
		Z &= \mathbb{E}_\pi[L]\\
		&\approx \frac{1}{N}\sum_{i\in \pi}L(\theta_i)
	\end{split},
	\label{eq:importance}
\end{equation}
where the second equality become exact for $N\rightarrow \infty$. NS project the integral down into one dimension viz\footnote{Attempting a higher accuracy via better numerical approximations of the integral is mute since the uncertainty in $\xi$ dominate the approximation~\citep{skilling2004}.}
\begin{equation}
	\begin{split}
		Z &= \int_0^1 L(\xi) d\xi\\
		&\approx \sum_{i}L(\xi_i)\Delta \xi_i
	\end{split},
	\label{e12}
\end{equation}
where
\begin{equation}
	\xi(\lambda) = \int_{L>\lambda} \pi(\theta)d\theta,
\end{equation}
is the proportion of the prior with likelihood greater than $\lambda$ and $\Delta \xi_i\equiv \xi_{i-1}-\xi_i$. Due to the constraint $L>\lambda$ on the integral bound of $\xi$, $L(\xi)$ is a decreasing function of $\xi$, meaning $L(\xi_1)>L(\xi_2)$ if $\xi_1<\xi_2$. The sum in equation \eqref{e12} can then be evaluated by generating a sequence
\begin{equation}
	\{\{L(\xi_m),\xi_m\},\{L(\xi_{m-1}),\xi_{m-1}\},\dots\{L(\xi_1),\xi_1\}\},
	\label{seq}
\end{equation}
with $\xi_1<\xi_2<\dots <\xi_m$. The sorting operation eliminate coordinate dependent complications of geometry, topology and dimensionality~\citep{skilling2006}. A sequence upholding equation \eqref{seq} can be generated as follows; consider $n$ random draws from $g$ with corresponding values of $L$ and $\xi$. Let $L(\xi^*)$ denote the minimum value of $L$ in the sample with $\xi^*$ the corresponding value of $\xi$ in the sample. $\{L(\xi^*), \xi^*\}$ is replaced by another set which is sampled from $g$ with the constraint that $\xi_{new}<\xi^*$ and stored in a list of discarded states. Continuing this sequence again and again will fill the list of discarded states that uphold equation \eqref{seq}. In practice $L(\xi)$ is not readily available, so instead $L$ can be generated from values of $\theta$. The value of $\xi_k$ can be determined by using that~\citep{skilling2004}
\begin{equation}
	\xi_k=\xi_0\prod_{i=1}^{k}t_i,
\end{equation}
with $t_i=\frac{\xi_k}{\xi_{k-1}}$, called the shrinkage ratio. The shrinkage ratio follow a beta distribution
\begin{equation}
	p(t)=nt^{n-1},
\end{equation}
with $n$ being the number of initialy samples from $g$ (the number of live points), such that 
\begin{equation}
	\begin{split}
		\langle\ln(t)\rangle&=\mathbb{E}[\ln(t)]\pm \sqrt{V[\ln(t)]}\\
		&=\int_0^1 nt^{n-1}\ln(t)dt\pm I_2\\
		&=\frac{1}{n}(-1\pm 1)
	\end{split},
\end{equation}
with 
\begin{equation}
	I_2 = \sqrt{\int_0^1nt^{n-1}\ln(t)^2dt-\bigg(\int_0^1nt^{n-1}\ln(t)dt\bigg)^2}.
\end{equation}
Using $\ln(\xi_k)=\sum_{i=1}^k\ln(t_i)$ and taking $t_i$ to be i.i.d. yield
\begin{equation}
	\begin{split}
		\langle\ln(\xi_k)\rangle&=k\mathbb{E}[\ln(t)]\pm \sqrt{kV[\ln(t)]}\\
		&=\frac{1}{n}(-k\pm \sqrt{k})
	\end{split}.
	\label{eqln}
\end{equation}
Ignoring uncertainty $\xi_k$ can be approximated by the mean viz
\begin{equation}
	\xi_k\approx e^{-\frac{k}{n}},
\end{equation}
meaning
\begin{equation}
	\Delta \xi_i\approx e^{-\frac{i}{n}}\big(e^{\frac{1}{n}}-1\big).
\end{equation}
A heuristic measure for terminating the collection of samples is to require that the maximum likelihood collected make up only a small fraction, $B$, of the evidence, meaning
\begin{equation}
	\max(\{L\})\xi_j < BZ,
\end{equation}
for iteration $j$. Another approach to terminating the collection of samples is to use that most of the area in the $L\xi$-plane is usually found in the region~\citep{skilling2004,skilling2006} $\xi \sim e^{-\mathcal{H}}\sim e^{-\frac{i}{n}}$, meaning the collection of samples can be terminated when
\begin{equation}
	i\gg n\mathcal{H},
	\label{eq:stop2}
\end{equation}
with $\mathcal{H}$ being the information~\citep{skilling2004}
\begin{equation}
	\begin{split}
		\mathcal{H} &= \int \frac{L(\xi)}{Z}\ln\bigg(\frac{L(\xi)}{Z}\bigg)d\xi\\
		& \approx \sum_i\frac{L(\xi_i)}{Z}\ln\bigg(\frac{L(\xi_i)}{Z}\bigg)\Delta \xi_i.
	\end{split}
\end{equation}
Temrinating at $i\sim n\mathcal{H}$ yield (equation \eqref{eqln}) an uncertainty $\delta (\langle\ln(\xi_i)\rangle)=\sqrt{\frac{\mathcal{H}}{n}}$ meaning
\begin{equation}
	\ln(Z)\approx \ln\bigg(\sum_{i}L(\xi_i)\Delta \xi_i\bigg)\pm \sqrt{\frac{\mathcal{H}}{n}}.
\end{equation}
The NS algorithm with equation \eqref{eq:stop2} as termination criterion is shown in algorithm \ref{alg:NS}. $A$ and $B$ are parameters of the algorithm. The "Remainder" in the second to last line in algorithm \ref{alg:NS} fills in the missing band $0<\xi<e^{-\frac{k+1}{n}}$ with the average value of the remaining values of $L$. Due to the chosen stopping criterion, the "Remainder" will be construction be small.

\begin{algorithm}
	\caption{Nested Sampling Algorithm in pseudo code}
	\begin{algorithmic}[1]
		\State \textbf{Import:} $S = n$ samples $\theta_1, \theta_2, \dots, \theta_n$ from the prior distribution with $L$ being the corresponding likelihoods
		\State \textbf{Initialize:} $k \gets 0$, $a \gets 0$, $B \gets 1$, $Z \gets \text{Empty list}$
		\While{$f > B$}
		\State Let $L^* \equiv \min(L)$ and $S^* \widehat{=} L^*$
		\State $S2 \gets S \setminus S^*$ and $L2 \gets L \setminus L^*$
		\State Define $\Delta \xi_k = e^{\frac{k+1}{n}} (e^\frac{1}{n} - 1)$
		\State Store $L^* \Delta \xi_k$ in $Z$
		\State $S_{new}, L_{new} \gets \text{proposer}(\text{random}(S2), L^*)$
		\State $S \gets S2 \cup S_{new}$ and $L \gets L2 \cup L_{new}$
		\State $f \gets \frac{\max(L) e^{-\frac{k+1}{n}}}{\sum_{s=0}^k Z_s}$
		\If{$a == A$}
		\State Display status, e.g. $f$, $n\mathcal{H} - k$, $k$, $\sum_{s=0}^k Z_s$, ...
		\State $a \gets 0$
		\EndIf
		\State $k \gets k + 1$
		\State $a \gets a + 1$
		\EndWhile
		\State Remainder $\gets \frac{1}{n} \sum_i L_i e^{-\frac{k+1}{n}}$
		\State $Z \approx \sum_{s=0}^k Z_s + \text{Remainder}$
	\end{algorithmic}
	\label{alg:NS}
\end{algorithm}

