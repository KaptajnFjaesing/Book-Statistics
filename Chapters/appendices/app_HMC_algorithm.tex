\chapter{Hamiltonian Monte Carlo}
\label{app:HMC}
This appendix is taken from \citet{petersen2020}. The Hamiltonian Monte Carlo Algorithm (HMC algorithm) is a Markov Chain Monte Carlo (MCMC) algorithm used to evaluate integrals on the form
\begin{equation}
	\begin{split}
		\mathbb{E}[f] &= \int f(\theta)g(\theta)d\theta\\
		& \approx \frac{1}{N}\sum_{j\in g}f(\theta^{(i)}),
	\end{split}
\end{equation}
with $f$ being a generic function and $N$ denoting the number of samples from the posterior distribution, $g$. The sample $\{j\}$ from $g$ can be generated via a MCMC algorithm that has $g$ as a stationary distribution. The Markov chain is defined by an initial distribution for the initial state of the chain, $\theta$, and a set of transition probabilities, $p(\theta'|\theta)$, determining the sequential evolution of the chain. A distribution of points in the Markov Chain are said to comprise a stationary distribution if they are drawn from the same distribution and that this distribution persist once established. Hence, if $g$ is the a stationary distribution of the Markov Chain defined by the initial point $\theta$ and the transition probability $p(\theta'|\theta)$, then~\citep{Neal:1996}
\begin{equation}
	g(\theta')=\int p(\theta'|\theta)g(\theta)d\theta.
	\label{ee1}
\end{equation}
\EQref{ee1} is implied by the stronger condition of detailed balance, defined viz
\begin{equation}
	p(\theta'|\theta)g(\theta)=p(\theta|\theta')g(\theta').
\end{equation}
A Markov chain is ergodic if it has a unique stationary distribution, called the equilibrium distribution, to which it converge from any initial state. $\{i\}$ can be taken as a sequential subset (discarding the part of the chain before the equilibrium distribution) of a Markov chain that has $g(\theta)$ as its equilibrium distribution. \newline
The simplest MCMC algorithm is perhaps the Metropolis-Hastings (MH) algorithm ~\citep{Metropolis1953,hastings70}. The MH algorithm works by randomly initiating all coefficients for the distribution wanting to be sampled. Then, a loop runs a subjective number of times in which one coefficient at a time is perturbed by a symmetric proposal distribution. A common choice of proposal distribution is the normal distribution with the coefficient value as the mean and a subjectively chosen variance. If $g(\theta')\geq g(\theta)$ the perturbation of the coefficient is accepted, otherwise the perturbation is accepted with probability $\frac{g(\theta')}{g(\theta)}$.\newline
The greatest weaknesses of the MH algorithm is i) a slow approach to the equilibrium distribution, ii) relatively high correlation between samples from the equilibrium distribution and iii) a relatively high rejection rate of states. ii) can be rectified by only accepting every $n$'th accepted state, with $n$ being some subjective number. For $n\rightarrow \infty$ the correlation naturally disappears, so there is a trade off between efficiency and correlation. Hence, in the end the weaknesses of the MH algorithm can be boiled down to inefficiency. This weakness is remedied by the HCM algorithm~\citep{Duane:1987de} in which Hamiltonian dynamics are used to generate proposed states in the Markov chain and thus guide the journey in parameter space. Hamiltonian dynamics are useful for proposing states because~\citep{Neal2012} 1) the dynamics are reversible, implying that detailed balance is fulfilled and so there exist a stationary distribution, 2) the Hamiltonian ($H$) is conserved during the dynamics if there is no explicit time dependence in the Hamiltonian ($\frac{dH}{dt}=\frac{\partial H}{\partial t}$), resulting in all proposed states being accepted in the case the dynamics are exact and 3) Hamiltonian dynamics preserve the volume in phase space ($q_i,p_i$-space), which means that the Jacobian is unity (relevant for Metropolis updates that succeeds the Hamiltonian dynamics in the algorithm). By making sure the algorithm travel (in parameter space) a longer distance between proposed states, the proposed states can be ensured to have very low correlation, hence alleviating issues 1) and 2) of the MH algorithm. The price to pay for using the HMC algorithm relative to the MH algorithm is a) the HMC algorithm is gradient based meaning it requires the Hamiltonian to be continuous and b) the computation time can be long depending on the distribution being sampled (e.g. some recurrent ANNs are computationally heavy due to extensive gradient calculations).\newline
As previously stated, the HMC algorithm works by drawing a physical analogy and using Hamiltonian dynamics to generate proposed states and thus guide the journey in parameter space. The analogy consists in viewing $g$ as the canonical probability distribution describing the probability of a given configuration of parameters. In doing so, $g$ is related to the Hamiltonian, $H$, viz
\begin{equation}
	g=e^{\frac{F-H}{k_BT}}\Rightarrow H=F-k_BT\ln[g],
\end{equation}
where $F=-k_BTln[Z]$ denotes Helmholtz free energy of the (fictitious in this case) physical system and $Z$ is the partition function. $\ln[g(\theta)]$ contain the position (by analogy) variables of the Hamiltonian and so $Z$ must contain the momentum variables. Almost exclusively~\citep{Betancourt2013} $Z\sim \mathcal{N}(0,\sqrt{m_i})$ is taken yielding the Hamiltonian 
\begin{equation}
	H=-k_BT\bigg[\ln[g]-\sum_{i}\frac{p_i^2}{2m_i}\bigg]+const,
\end{equation}
where $i$ run over the number of variables and "const" is an additive constant (up to which the Hamiltonian is always defined). $T=k_b^{-1}$ is most often taken~\citep{Neal2012}, however, the temperature can be used to manipulate the range of states which can be accepted e.g. via simulated annealing~\citep{MacKay2002}. Here $T=k_b^{-1}$ will be adopted in accordance with \citep{Neal:1996,Neal2012} and as such
\begin{equation}
	H=\sum_{i}\frac{p_i^2}{2m_i}-\ln[g].
\end{equation}
The dynamics in parameter space are determined by Hamiltons equations
\begin{equation}
	\dot{\theta}_i=\frac{\partial H}{\partial p_i},\qquad \dot{p}_i=-\frac{\partial H}{\partial \theta_i},
\end{equation}
with $\theta_i$ denoting the different variables (coefficients). In order to implement Hamiltons equations, they are discretized via the leap frog method~\citep{Neal:1996,Neal2012} viz
\begin{equation}
	\begin{split}
		&p_i\left( t+\frac{\epsilon}{2}\right)=p_i(t)-\frac{\epsilon}{2}\frac{\partial H(\theta_i(t),p_i(t))}{\partial \theta_i},\\
		&\theta_i(t+\epsilon)=\theta_i(t)+\frac{\epsilon}{m_i}p_i\left(t+\frac{\epsilon}{2}\right),\\
		&p_i(t+\epsilon)=p_i\left(t+\frac{\epsilon}{2}\right)-\frac{\epsilon}{2}\frac{\partial H(\theta_i(t+\frac{\epsilon}{2}),p_i(t+\frac{\epsilon}{2}))}{\partial \theta_i},\\
	\end{split}
\end{equation}
with $\epsilon$ being an infinitesimal parameter. In the algorithm the initial state is defined by a random initialization of coordinates and momenta, yielding $H_{initial}$. Subsequently Hamiltonian dynamics are simulated a subjective ($L$ loops) amount of time resulting in a final state, $H_{final}$, the coordinates of which take the role of proposal state. The loop that performs $L$ steps of $\epsilon$ in time is here referred to as the dive. During the dive, the Hamiltonian remains constant, so ideally $H_{initial}=H_{final}$, however, imperfections in the discretization procedure of the dynamics can result in deviations from this equality (for larger values of $\epsilon$, as will be discussed further later on). For this reason, the proposed state is accepted as the next state in the Markov chain with probability
\begin{equation}
	\mathbb{P}(\text{transition})=\min\big[1,e^{H_{initial}-H_{final}}\big].
	\label{pro}
\end{equation}
Whether or not the proposed state is accepted, a new proposed state is next generated via Hamiltonian dynamics and so the loop goes on for a subjective amount of time. \newline
Most often, the HMC algorithm will be ergodic, meaning it will converge to its unique stationary distribution from any given initialization (i.e. the algorithm will not be trapped in some subspace of parameter space), however, this may not be so for a periodic Hamiltonian if $L\epsilon$ equal the periodicity. This potential problem can however be avoided by randomly choosing $L$ and $\epsilon$ from small intervals for each iteration. The intervals are in the end subjective, however, with some constraints and rules of thumb; the leap frog method has an error of $\mathcal{O}(\epsilon^2)$~\citep{Neal:1996} and so the error can be controlled by ensuring that $\epsilon\ll1$. A too small value of $\epsilon$ will waste computation time as a correspondingly larger number of iterations in the dive ($L$) must be used to obtain a large enough trajectory length $L\epsilon$. If the trajectory length is too short the parameter space will be slowly explored by a random walk instead of the otherwise approximately independent sampling (the advantage of non-random walks in HMC is a more uncorrelated Markov chain and better sampling of the parameter space). A rule of thumb for the choice of $\epsilon$ can be derived from a one dimensional Gaussian Hamiltonian
\begin{equation}
	H=\frac{q^2}{2\sigma^2}+\frac{p^2}{2}.
	\label{ghf}
\end{equation}
The leap frog step for this system is a linear map from $t\rightarrow t+\epsilon$. The mapping can be written
\begin{equation}
	\begin{split}
		\begin{bmatrix}
			q(t+\epsilon)\\
			p(t+\epsilon)
		\end{bmatrix}&=\begin{bmatrix}
			1-\frac{\epsilon^2}{2\sigma^2}& \epsilon\\
			\epsilon(\frac{1}{4}\epsilon^2\sigma^{-4}-\sigma^{-2}) & 1-\frac{1}{2}\epsilon^2\sigma^{-2}\\
		\end{bmatrix}\begin{bmatrix}
			q(t)\\
			p(t)
		\end{bmatrix}\\
	\end{split}
\end{equation}
The eigenvalues of the coefficient matrix represent the powers of the exponentials that are the solutions to the differential equation. They are given by
\begin{equation}
	\text{Eigenvalues}=1-\frac{1}{2}\epsilon^2\sigma^{-2}\pm \epsilon\sigma^{-1}\sqrt{\frac{1}{4}\epsilon^2\sigma^{-2}-1}.
\end{equation}
In order for the solutions to be bounded, the eigenvalues must be imaginary, meaning that
\begin{equation}
	\epsilon<2 \sigma.
	\label{gh}
\end{equation}
In higher dimensions a rule of thumb is to take $\epsilon\lesssim 2\sigma_x$, where $\sigma_x$ is the standard deviation in the most constrained direction, i.e. the square root of the smallest eigenvalue of the covariance matrix. In general~\citep{Betancourt2013} a stable solution with $\frac{1}{2}p^T\Sigma^{-1}p$ as the kinetic term in the Hamiltonian require 
\begin{equation}
	\epsilon_i<2 \lambda_i^{-\frac{1}{2}},
	\label{laa}
\end{equation}
for each eigenvalue $\lambda_i$ of the matrix
\begin{equation}
	M_{ij}=(\Sigma^{-1})_{ij}\frac{\partial^2 H}{\partial q_i\partial q_j},
\end{equation}
which means that in the case of $\Sigma^{-1}=diag(m_i^{-1})$;
\begin{equation}
	\epsilon_i<2\sqrt{\frac{m_i}{\frac{\partial^2H}{\partial q^2_i}}}.
	\label{heu}
\end{equation}
Setting $\epsilon$ according to \EQref{laa} can however introduce issues for hierarchical models (models including hyper parameters) since the reversibility property of Hamiltonian dynamics is broken if $\epsilon$ depend on any parameters. This issue can be alleviated by using the MH algorithm on a subgroup of parameters~\citep{Neal:1996,Neal2012} (which are then allowed in the expression for $\epsilon$) that is to be included in $\epsilon$. However, unless the MH algorithm is used for all parameters, some degree of approximation is required. 
\vspace{5mm} %5mm vertical space

\begin{algorithm}
	\caption{Hamiltonian Monte Carlo Algorithm in pseudo code}
	\begin{algorithmic}[1]
		\State \textbf{Save:} $q$ and $V(q)$, with $q$ randomly initialized
		\For{$i \gets 1$ to $N$}
		\State $p \gets$ Sample from standard normal distribution
		\State $H_{\text{old}} \gets H(q, p)$
		\State $p \gets p - \frac{\epsilon}{2} \frac{\partial H(q,p)}{\partial q}$
		\State $L \gets$ Random integer between $L_{\text{lower}}$ and $L_{\text{upper}}$
		\For{$j \gets 1$ to $L$}
		\State $q \gets q + \epsilon \frac{p}{\text{mass}}$
		\If{$j \neq L$}
		\State $p \gets p - \epsilon \frac{\partial H(q,p)}{\partial q}$
		\EndIf
		\EndFor
		\State $p \gets p - \frac{\epsilon}{2} \frac{\partial H(q,p)}{\partial q}$
		\State $H_{\text{new}} \gets H(q, p)$
		\State $u \gets$ Sample from uniform distribution
		\If{$u < \min(1, e^{-(H_{\text{new}} - H_{\text{old}})})$}
		\State $H_{\text{old}} \gets H_{\text{new}}$
		\State \textbf{Save:} $q$ and $V(q)$
		\EndIf
		\EndFor
	\end{algorithmic}
	\label{alg:HMC}
\end{algorithm}
