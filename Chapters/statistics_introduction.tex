\chapter{Introduction to Statistics}
\label{chp:statistics_introduction}
Let the observed outcome of a statistical experiment be described by the probability space\index{Probability space} $(\Omega, \mathcal{F}, \mathbb{P})$. Unlike in probability theory, where $\mathbb{P}$ is assumed known, in statistics the data-generating measure is typically unknown and must be inferred from observations. Let~\cite{orbanz2009functional,tausk2023basic,drewitz2019introduction,chan2021introduction}
\begin{equation}
	X_i: \Omega \to \Omega_{X_i}
\end{equation}
be a generic random variable\index{Random variable} from the probability space\index{Probability space} $(\Omega,\mathcal{F},\mathbb{P})$ to the measurable space $(\Omega_{X_i},\mathcal{F}_{X_i})$, with the image measure\index{Image measure}
\begin{equation}
	\mathbb{P}_{X_i} = \mathbb{P} \circ X_i^{-1}.
\end{equation}
The joint image measure of $n$ such random variables,
\begin{equation}
	\mathbb{P}_{X_1,\dots,X_n} = \mathbb{P} \circ (X_1,\dots,X_n)^{-1},
\end{equation}
is defined on the product measurable space\index{Measurable space}
\begin{equation}
	\bigl(\Omega_{X_1}\times \cdots \times \Omega_{X_n}, \mathcal{F}_{X_1}\otimes \cdots \otimes \mathcal{F}_{X_n}\bigr),
\end{equation}
which for brevity will be written as $\mathbb{P}_{X_{1:n}}$ and $(\Omega_{X_{1:n}},\mathcal{F}_{X_{1:n}})$, respectively.

\begin{definition}[Set of All Probability Measures]
	Let $\mathcal{P}$ be the set of all probability measures\index{Probability measure} on $(\Omega_{X_{1:n}},\mathcal{F}_{X_{1:n}})$.
\end{definition}

\begin{definition}[Parametric Family of Probability Measures]
	\label{def:parametric_family}
	A parametric family (or parametric subset) of probability measures is a set of the form
	\begin{equation}
		\mathcal{P}' = \{ \mathbb{P}^w_{X_{1:n}} \mid w \in \Omega_W \} \subseteq \mathcal{P},
	\end{equation}
	where $\Omega_W$ is the parameter space. For each fixed $w$, $\mathbb{P}^w_{X_{1:n}}$ is a probability measure\index{Probability measure}\index{Image measure} on $(\Omega_{X_{1:n}},\mathcal{F}_{X_{1:n}})$.
\end{definition}

\begin{definition}[Parameter Space]
	\label{def:parameter_space}
	The parameter space\index{Parameter space} $\Omega_W$ is the set of all values $w$ that index the probability measures $\mathbb{P}^w_{X_{1:n}} \in \mathcal{P}'$.
\end{definition}

\begin{definition}[Identifiable Statistical Model]
	A parametric family $\mathcal{P}' = \{ \mathbb{P}^w_{X_{1:n}} \mid w \in \Omega_W \}$ is identifiable if the mapping
	\begin{equation}
		w \in \Omega_W \mapsto \mathbb{P}^w_{X_{1:n}} \in \mathcal{P}'
	\end{equation}
	is injective; i.e., distinct parameter values induce distinct probability measures.
\end{definition}

The parameters $w \in \Omega_W$ may be interpreted in two different ways in practice: either as fixed but unknown constants or as realizations of a random variable.

\begin{axiom}[Parameter Fixedness]
	\label{ax:parameter_fixed}
	The parameter $w \in \Omega_W$ is treated as a fixed but unknown constant. In this setting the image measure\index{Image measure} $\mathbb{P}_{X_{1:n}}$ is assumed to equal $\mathbb{P}^{w^*}_{X_{1:n}}$ for some (unknown) $w^* \in \Omega_W$.
\end{axiom}

\begin{axiom}[Parameter as a Random Variable]
	\label{ax:parameter_variable}
	The parameter $w \in \Omega_W$ is treated as the realization of a random variable\index{Random variable}
	\begin{equation}
		W:\Omega \to \Omega_W
	\end{equation}
	from the probability space\index{Probability space} $(\Omega, \mathcal{F}, \mathbb{P})$ to a measurable space\index{Measurable space} $(\Omega_W, \mathcal{F}_W)$, with the image (prior) measure\index{Image measure}\index{Prior measure}
	\begin{equation}
		\mathbb{P}_W = \mathbb{P} \circ W^{-1}.
	\end{equation}
 	The joint image measure $\mathbb{P}_{W,X_{1:n}}$ is defined on 
	\begin{equation}
		(\Omega_W \times \Omega_{X_{1:n}}, \mathcal{F}_W \otimes \mathcal{F}_{X_{1:n}}).
	\end{equation}
\end{axiom}

\begin{remark}[Parameter interpretation]
	For both \axref{ax:parameter_fixed} and \axref{ax:parameter_variable}, the value of a parameter is considered fixed. \axref{ax:parameter_variable} introduces a random variable\index{Random variable} $W$ not to add randomness to the parameter $w$ but to model uncertainty or variability about the fixed but unknown parameter value.
\end{remark}

\newpage
\section{Interpretation of Probability Measures} 
Although probability measures\index{Probability measure interpretation} are defined according to \dfref{def:probability}, their interpretation is not defined beyond this definition. For this reason there are two broadly accepted interpretations of probability; objective and subjective. 

\begin{definition}[Objective Probability Measure]
	\label{def:objective_probability}
	Let $\mathbb{P}$ denote a generic probability measure\index{Probability measure} defined on the generic probability space\index{Probability space} $(\Omega,\mathcal{F},\mathbb{P})$. The "objective probability measure"\index{Objective probability measure}-interpretation define $\mathbb{P}$ as the long-run or limiting frequency of an event, $E$. That is, let $m$ be the number of occurrences of $E$, and let $n$ be the number of experiments, then~\cite{Leamer1978}
	\begin{equation}
		\mathbb{P}(E) \equiv \lim_{{n \to \infty}} \bigg(\frac{m}{n}\bigg)
	\end{equation}
	define the probability measure as the limit of a relative frequency.
\end{definition}

\begin{definition}[Sugeno Measure]
	\label{def:sugeno_measure}
	Let $(\Omega, \mathcal{F})$ be a measurable space\index{Sugeno measure}\index{Measurable space}. A set function\index{Belief function}
	\begin{equation}
		\operatorname{Bel}: \mathcal{F} \to [0,1]
	\end{equation}
	is called a Sugeno measure~\cite{shafer1987} if it satisfies the following properties:
	\begin{enumerate}
		\item \axref{ax:non_neg} (non-negativity),
		\item $\operatorname{Bel}(\Omega) = 1$ (normalization),
		\item $\operatorname{Bel}(A) \le \operatorname{Bel}(B)$ for all $A, B \in \mathcal{F}$ with $A \subseteq B$ (monotonicity).
	\end{enumerate}
\end{definition}

\begin{definition}[Subjective Probability Measure]
	\label{def:subjective_probability}
	A subjective probability measure\index{Subjective probability measure} is a numerical representation of rational beliefs\index{Rational beliefs}. Formally, it is a probability measure\index{Probability measure} $\mathbb{P}$, according to \dfref{def:probability}, on a measurable space\index{Measurable space} $(\Omega, \mathcal{F})$ that fulfills \dfref{def:sugeno_measure} ~\cite{shafer1987,hoff2009first}.
\end{definition}

\begin{theorem}[Probability vs.\ Sugeno Measures]
	Any probability measure $\mathbb{P}$ on $(\Omega, \mathcal{F})$ is a Sugeno measure.
\end{theorem}
\begin{proof}
	Let $\mathbb{P}$ be a probability measure on $(\Omega, \mathcal{F})$. By definition, $\mathbb{P}$ satisfies:
	\begin{enumerate}
		\item $\mathbb{P}(\emptyset) = 0$ and $\mathbb{P}(\Omega) = 1$ (Boundary Conditions).
		\item If $A, B \in \mathcal{F}$ and $A \subseteq B$, then $\mathbb{P}(A) \leq \mathbb{P}(B)$ (Monotonicity).
	\end{enumerate}
	Thus, $\mathbb{P}$ is a Sugeno measure.
\end{proof}

\begin{remark}
	Since a probability measure $\mathbb{P}$ satisfies the axioms of a Sugeno measure\index{Sugeno measure}, it can be interpreted as a belief function\index{Belief function}.
\end{remark}


\begin{definition}[Frequentist Statistics]
	\label{def:frequentist_statistics}
	Frequentist statistics\index{Frequentist statistics definition} is a	paradigm that adopts \axref{ax:parameter_fixed} and \dfref{def:objective_probability} of probability. 
\end{definition}

\begin{definition}[Bayesian Statistics]
	\label{def:bayesian_statistics}
	Bayesian statistics\index{Bayesian statistics definition} is a paradigm that adopts \axref{ax:parameter_variable} and definition \dfref{def:subjective_probability} of probability. 
\end{definition}


\begin{remark}[Frequentist vs.\ Bayesian Interpretation]
	\label{rem:frequentist_statistics}
	In the Frequentist framework, parameters are fixed but unknown, and probability statements concern the variability of estimators across hypothetical repeated samples. For instance, a $95\%$ confidence interval means that if the experiment were repeated many times, approximately $95\%$ of the constructed intervals would contain the true parameter value. \newline
	
	In the Bayesian framework, parameters are treated as random variables with a posterior distribution given the observed data. A $95\%$ credible interval therefore means that, conditional on the data and prior information, there is a $95\%$ probability that the true parameter lies within the interval.\newline
	
	Thus, in the Frequentist view, the interval varies across repeated experiments while the parameter remains fixed, whereas in the Bayesian view, the interval is fixed (given the data) and the parameter is uncertain.
\end{remark}

\begin{example}
	\label{ex:bayesian_model}
	Consider a Bayesian statistical model in which the observed data are generated from a combination of a Normal distribution with parameters $\mu,\sigma$ and a Beta distribution with parameters $a,b$. Define the parameter random vector
	\begin{equation}
		W = 
		\begin{pmatrix}
			W_\mu \\ W_\sigma \\ W_a \\ W_b
		\end{pmatrix} : \Omega \to \Omega_W,
	\end{equation}
	where each component $W_\mu, W_\sigma, W_a, W_b$ is a random variable from the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ to a measurable space $(\Omega_i, \mathcal{F}_i)$ with an image (prior) measure $\mathbb{P}_i$ $\forall i\in\{\mu,\sigma,a,b\}$.
\end{example}

