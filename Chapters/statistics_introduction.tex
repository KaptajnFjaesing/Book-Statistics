\chapter{Framing of Statistics}
In this book, statistics is framed as a game against Nature, following conventions from decision theory~\cite{lavalle2006planning}. 
In this game, two players interact under uncertainty: the Robot, who seeks to make optimal decisions, and Nature, who determines the state of the world.

\begin{definition}[Robot]
	\label{def:robot}
	The Robot is the decision maker in the statistical game.\index{Robot}
	It selects actions or decisions based on available information with the aim of achieving an optimal outcome under uncertainty.
\end{definition}

\begin{definition}[Nature]
	\label{def:nature}
	Nature represents the inherent uncertainty in the environment and the data-generating process\index{Nature}.
	It determines the true state of the world, which influences the outcomes of the statistical experiment, but it does not act strategically or make decisions in the usual sense.
\end{definition}


\begin{definition}[Statistical Game]
	\label{def:statistical_game}
	The interaction between the Robot and Nature is formalized as a statistical game under uncertainty. Let
	\begin{equation}
		(\Omega, \mathcal{F}, \mathbb{P})
	\end{equation}
	be a probability space. Consider $n+1$ pairs of random variables\index{Random variable}
	\begin{equation}
		(X_1, Y_1), \dots, (X_n, Y_n), (X_{n+1}, Y_{n+1}) \colon \Omega \to \Omega_X \times \Omega_Y,
	\end{equation}
	where $(X_i,Y_i)$ for $i=1,\dots,n$ correspond to past observations collected as data, and $(X_{n+1},Y_{n+1})$ corresponds to a new observation $X_{n+1}$ with unknown outcome $Y_{n+1}$ to be predicted. Define the dataset\index{Dataset} as the collection of past realizations
	\begin{equation}
		D = \{(x_i, y_i)\}_{i=1}^n \in (\Omega_X \times \Omega_Y)^n,
	\end{equation}
	where $x_i \in \Omega_X$ and $y_i \in \Omega_Y$ are realizations of $X_i$ and $Y_i$, respectively. Each pair $(x_i, y_i)$ corresponds to one past round of the game. The image measures of the random variables are
	\begin{equation}
		\begin{split}
			\mathbb{P}_{X_{1:n+1}} &= \mathbb{P} \circ (X_1,\dots,X_{n+1})^{-1},\\
			\mathbb{P}_{Y_{1:n+1}} &= \mathbb{P} \circ (Y_1,\dots,Y_{n+1})^{-1},\\
			\mathbb{P}_{(X,Y)_{1:n+1}} &= \mathbb{P} \circ ((X_1,Y_1),\dots,(X_{n+1},Y_{n+1}))^{-1},
		\end{split}
	\end{equation}
	defined on the measurable spaces
	\begin{equation}
		(\Omega_X^{n+1},\mathcal{F}_X^{n+1}), \quad(\Omega_Y^{n+1},\mathcal{F}_Y^{n+1}), \quad ((\Omega_X \times \Omega_Y)^{n+1}, (\mathcal{F}_X \otimes \mathcal{F}_Y)^{n+1}).
	\end{equation}
	Suppose that the joint image measure $\mathbb{P}_{(X,Y)_{1:n+1}}$ depends on the unknown parameter $w \in \Omega_W$, where $\Omega_W$ is the parameter space\index{Parameter space}. Let
	\begin{equation}
		\mathcal{P}' = \{ \mathbb{P}^w_{(X,Y)_{1:n+1}} \colon w \in \Omega_W \},
	\end{equation}
	denote a parametric family of joint image measures\index{Parametric family of image measures} on $(\Omega_X \times \Omega_Y)^{n+1}$. The statistical model\index{Statistical model} of the game is then defined as the triple
	\begin{equation}
		((\Omega_X \times \Omega_Y)^{n+1}, (\mathcal{F}_X \otimes \mathcal{F}_Y)^{n+1}, \mathcal{P}').
	\end{equation}
	In the game, the Robot selects an action $u_{n+1} \in \Omega_U$ according to a decision rule\index{Decision rule}
	\begin{equation}
		U \colon \Omega_X \times (\Omega_X \times \Omega_Y)^n \to \Omega_U,
	\end{equation}
	mapping the new observation $x_{n+1}$ and past dataset $D$ to an action. The Robot incurs a penalty determined by a cost function\index{Cost function}
	\begin{equation}
		C \colon \Omega_U \times \Omega_N \to \mathbb{R},
	\end{equation}
	which assigns a numerical cost to each pair $(u, n) \in \Omega_U \times \Omega_N$, where $\Omega_N$ denotes Natureâ€™s action space. The Robot's goal is to minimize the expected cost~\cite{murphy2023probabilistic}, which for prediction is given by
	\begin{equation}
		\mathbb{E}_{(X,Y)_{1\colon n+1}}^{\mathcal{P}'}[C(U(x_{n+1},D), Y_{n+1})].
		\label{eq:expcost}
	\end{equation}
	
\end{definition}

\begin{remark}[Superscript Notation]
	\label{rem:superscript_w}
	The superscript $w$ in $\mathbb{P}_Z^w$ serves as an index identifying a particular member of a family of probability measures or densities. It reflects dependence on an underlying parameter $w \in \Omega_W$ without specifying whether $w$ is fixed, unknown, or random.

	The superscript $\mathcal{P}'$ indicates that the object is taken with respect to a member or mixture of the parametric family. The superscript notation provides a unified notation across different statistical paradigms (see \rmref{rem:notation} and \rmref{rem:frequentist_bayesian_expected_cost}).
\end{remark}


\newpage
\begin{remark}[Notation of Statistics]
	\label{rem:notation}
	The expected cost of \dfref{def:statistical_game} (\EQref{eq:expcost}) can be written as follows
	\begin{equation}
		\begin{split}
			&\mathbb{E}_{(X,Y)_{1\colon n+1}}^{\mathcal{P}'}\big[C(U(x_{n+1},D),Y_{n+1})\big] \\
			&\quad= \int_{(\Omega_X\times\Omega_Y)^{n+1}}
			C\big(U(x_{n+1},D),y_{n+1}\big)\;
			\mathrm{d}\mathbb{P}_{(X,Y)_{1\colon n+1}}^{\mathcal{P}'}(D,x_{n+1},y_{n+1}).
		\end{split}
		\label{eq:expcost2}
	\end{equation}
	\EQref{eq:expcost2} can be further decomposed by specifying the cost function or by factorizing the joint image measure using \thref{theorem:chain_rule}, \thref{theorem:bayes_theorem}, or \thref{theorem:law_of_total_probability}. In such cases, the notation of probability theory can become cumbersome. Therefore, when the context is sufficiently clear, statistical notation is often relaxed by omitting distributional subscripts, superscripts, and explicit integral domains. In this notation, \EQref{eq:expcost2} becomes
	\begin{equation}
			\mathbb{E}\big[\,C(U(\tilde{D}) ,Y_{n+1})\big] = \int C\big(U(\tilde{D}),y_{n+1}\big) \mathrm{d}\mathbb{P}(\tilde{D},y_{n+1}),
		\label{eq:expcost3}
	\end{equation}	
	where $\tilde{D}= \{x_{n+1},D\}$ is used to further compress the notation.
\end{remark}

\begin{theorem}[Optimal Decision Rule]
	\label{theorem:opt_decision_rule}
	Following \dfref{def:statistical_game}, the optimal decision rule for the Robot is
	\begin{equation}
		\begin{split}
			U^*(\tilde{D}) &= \argmin_{U(\tilde{D})} \mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}]\\
			& = \argmin_{U(\tilde{D})}\int  C(U(\tilde{D}),y_{n+1}) p(y_{n+1}|\tilde{D}) \mathrm{d}y_{n+1}.
		\end{split}
		\label{eq:decision_rule3}
	\end{equation}
	
\end{theorem}
\begin{proof}
	From \dfref{def:statistical_game}
	\begin{equation}
		U^*(\tilde{D}) = \argmin_{U(\tilde{D})} \mathbb{E}[C(U(\tilde{D}), Y_{n+1})],
		\label{eq:decision_rule_x}
	\end{equation}	
	where the expected cost (\EQref{eq:expcost}) can be written
	\begin{equation}
			\mathbb{E}\big[\,C(U(\tilde{D}) ,Y_{n+1})\big]= \int C\big(U(\tilde{D}),y_{n+1}\big) \mathrm{d}\mathbb{P}(\tilde{D},y_{n+1}).
		\label{eq:conditional_expected_cost}
	\end{equation}
	From \thref{theorem:total_expectation}
	\begin{equation}
		\mathbb{E}[C(U(\tilde{D}) , Y_{n+1})] = \mathbb{E}[\mathbb{E}[C(U(\tilde{D}) , Y_{n+1})|\tilde{D}]].
		\label{eq:total2}
	\end{equation}
	Using \EQref{eq:total2} in \EQref{eq:decision_rule_x}
	\begin{equation}
		\begin{split}
			U^*(\tilde{D}) &= \argmin_{U(\tilde{D}) } \mathbb{E}[\mathbb{E}[C(U(\tilde{D}) , Y_{n+1})|\tilde{D}]]\\
			&= \argmin_{U(\tilde{D})} \int p(\tilde{D}) \mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}] \mathrm{d}\tilde{D}.
		\end{split}
		\label{eq:decision_rule2}
	\end{equation}
	Since $p(\tilde{D})$ is a non-negative function, the minimizer of the integral is the same as the minimizer of the conditional expectation, meaning
	\begin{equation}
		\begin{split}
			U^*(\tilde{D}) &= \argmin_{U(\tilde{D})} \mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}]\\
			& = \argmin_{U(\tilde{D})}\int  C(U(\tilde{D}),y_{n+1}) p(y_{n+1}|\tilde{D}) \mathrm{d}y_{n+1}.
		\end{split}
	\end{equation}
\end{proof}

\begin{example}
	The random variables $X_i$ represent the observations the Robot has available that are related to the decision Nature is going to make. However, this information may not be given, in which case $\{x_{n+1},D_x\}=\emptyset$ and consequently
	\begin{equation}
		\begin{split}
			\tilde{D} &=\{y_i\}_{i=1}^n\\
			&\equiv D_y.
		\end{split}
	\end{equation}
	In this case, the Robot is forced to model the decisions of Nature without observations. From \thref{theorem:opt_decision_rule} the optimal action for the Robot can be written
	\begin{equation}
		U^*(D_y) = \argmin_{U(D_y)} \mathbb{E}[C(U(D_y), Y_{n+1})|D_y]
		\label{eq:best_decision1}
	\end{equation}
\end{example}

\section{Assigning a Cost Function}
\label{sec:assing_cost}
The cost function from \dfref{def:statistical_game} associates a numerical penalty to the Robot's action and thus the details of it determine the decisions made by the Robot. Under certain conditions, a cost function can be shown to exist~\cite{lavalle2006planning}, however, there is no systematic way of producing or deriving the cost function beyond applied logic. In general, the topic can be split into considering a continuous and discrete action space, $\Omega_U$. 	

\subsection{Continuous Action Space}
In case of a continuous action space, the cost function is typically picked from a set of standard choices.	
\begin{definition}[Linear Cost Function]
	\label{def:linear_cost_function}
	The linear cost function is defined as follows
	\begin{equation}
		C(U(\tilde{D}),y_{n+1}) \equiv |U(\tilde{D})-y_{n+1}|.
	\end{equation}	
\end{definition}

\begin{theorem}[Median Decision Rule]
	\label{theorem:median_decision_rule}
	The linear cost function (\dfref{def:linear_cost_function}) leads to the median decision rule
	\begin{equation}
		\int_{-\infty}^{U^*(\tilde{D})} p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1} = \frac{1}{2}.
	\end{equation}
\end{theorem}

\begin{proof}
	From \thref{theorem:opt_decision_rule}
	\begin{equation}
		\begin{split}
			\mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}] &= \int_{-\infty}^{\infty} |U(\tilde{D})-y_{n+1}| p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}\\
			&= \int_{-\infty}^{U(\tilde{D})} (y_{n+1}-U(\tilde{D}))p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}\\
			&\quad+\int_{U(\tilde{D})}^\infty  (U(\tilde{D})-y_{n+1})p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}.\\
		\end{split}
	\end{equation}
	\begin{equation}
		\begin{split}
			0 &=\frac{\partial \mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}]}{\partial U(\tilde{D})}\bigg|_{U(\tilde{D})=U^*(\tilde{D})}\\
			&= (U^*(\tilde{D})-U^*(\tilde{D}))p(U^*(\tilde{D})|\tilde{D})+\int_{-\infty}^{U^*(\tilde{D})} p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}\\
			&\quad+(U^*(\tilde{D})-U^*(\tilde{D}))p(U^*(\tilde{D})|\tilde{D})-\int_{U^*(\tilde{D})}^\infty  p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}.
		\end{split}
	\end{equation}
	This leads to
	\begin{equation}
		\begin{split}
			\int_{-\infty}^{U^*(\tilde{D})} p(y_{n+1}|\tilde{D}) \mathrm{d}y_{n+1} &= \int_{U^*(\tilde{D})}^\infty p(y_{n+1}|\tilde{D}) \mathrm{d}y_{n+1}\\
			&= 1- \int_{-\infty}^{U^*(\tilde{D})} p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}\\
			&\Downarrow\\
			\int_{-\infty}^{U^*(\tilde{D})} p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}& = \frac{1}{2}.
		\end{split}
	\end{equation}	
\end{proof}

\begin{definition}[Quadratic Cost Function]
	\label{def:quadratic_cost}
	The quadratic cost function is defined as follows
	\begin{equation}
		C(U(\tilde{D}),y_{n+1}) \equiv (U(\tilde{D})-y_{n+1})^2.
	\end{equation}
\end{definition}

\begin{theorem}[Expectation Decision Rule]
	\label{theorem:expectation_decision_rule}
	The quadratic cost function (\dfref{def:quadratic_cost}) leads to the expectation decision rule
	\begin{equation}
		U^*(\tilde{D}) = \mathbb{E}[Y_{n+1}|\tilde{D}].
	\end{equation}
\end{theorem}

\begin{proof}
	From \thref{theorem:opt_decision_rule}
	\begin{equation}
		\begin{split}
			\mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}] &= \int (U(\tilde{D})-y_{n+1})^2 p(y_{n+1}|\tilde{D}) \mathrm{d}y_{n+1}\\
			&\Downarrow\\
			\frac{\partial \mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}]}{\partial U(\tilde{D})}\bigg|_{U(\tilde{D})=U^*(x)} &= 2U^*(\tilde{D})-2\int y_{n+1}p(y_{n+1}|\tilde{D}) \mathrm{d}y_{n+1}\\
			&=0\\
			&\Downarrow\\
			U^*(\tilde{D})& = \int y_{n+1}p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}\\
			&= \mathbb{E}[Y_{n+1}|\tilde{D}].
		\end{split}
	\end{equation}
\end{proof}

\begin{definition}[0-1 Cost Function]
	\label{def:0_1_cost_function}
	The 0-1 cost function is defined as follows
	\begin{equation}
		C(U(\tilde{D}),y_{n+1}) \equiv 1-\delta(U(\tilde{D})-y_{n+1}).
	\end{equation}
\end{definition}

\begin{theorem}[MAP Decision Rule]
	\label{theorem:MAP}
	The 0-1 cost function (\dfref{def:0_1_cost_function}) leads to the maximum a posteriori (MAP) decision rule
	\begin{equation}
		\frac{\partial p_{Y_{n+1}|X_{n+1},(X,Y)_{1\colon n}}(U(\tilde{D})|\tilde{D})}{\partial U(\tilde{D})}\bigg|_{U(\tilde{D})=U^*(\tilde{D})}=0,
	\end{equation}
	where the distribution subscript has been included since it is not apparent from the context.
\end{theorem}

\begin{proof}
	From \thref{theorem:opt_decision_rule}
	\begin{equation}
		\mathbb{E}[C((\tilde{D}), Y_{n+1})|\tilde{D}] = 1-\int \delta(U(\tilde{D})-y_{n+1}) p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}
	\end{equation}
	\begin{equation}
		\begin{split}
			\frac{\partial \mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}]}{\partial U(\tilde{D})}\bigg|_{U(\tilde{D})=U^*(\tilde{D})} &= -\frac{\partial p(U(\tilde{D})|\tilde{D})}{\partial U(\tilde{D})}\bigg|_{U(\tilde{D})=U^*(\tilde{D})}\\
			&=0.\\
		\end{split}
	\end{equation}
\end{proof}


\begin{example}
	The median decision rule is symmetric with respect to
	\begin{equation}
		z(\tilde{D},y_{n+1}) \equiv U(\tilde{D})-y_{n+1},
	\end{equation}
	meaning underestimation ($z<0$) and overestimation ($z>0$) is penalized equally. This decision rule can be generalized by adopting the cost function
	\begin{equation}
		C(U(\tilde{D}), y_{n+1}) = \alpha\cdot \operatorname{swish}(z(\tilde{D},y_{n+1}),\beta)
		+(1-\alpha)\cdot \operatorname{swish}(-z(\tilde{D},y_{n+1}),\beta),
	\end{equation}
	where
	\begin{equation}
		\operatorname{swish}(z,\beta) = \frac{z}{1+e^{-\beta z}}.
	\end{equation}
	Taking $\alpha \ll 1$ means $z<0$ will be penalized relatively more than $z>0$. The expected cost is
	\begin{equation}
		\mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}] = \int C(U(\tilde{D}),y_{n+1}) p(y_{n+1}|\tilde{D})\mathrm{d}y_{n+1}.
	\end{equation}
	The derivative of the cost function with respect to the decision rule can be approximated as follows
	\begin{equation}
		\begin{split}
			\frac{dC}{dU} & = \frac{dC}{dz}\frac{dz}{dU}\\
			& = \bigg(\frac{\alpha}{1+e^{-\beta z}}-\frac{1-\alpha}{1+e^{\beta z}}\\
			&\qquad+\frac{\alpha\beta e^{-\beta z}z}{(1+e^{-\beta z})^2}+\frac{(1-\alpha)\beta e^{\beta z}z}{(1+e^{\beta z})^2}\bigg)\frac{dz}{dU}\\
			&= \frac{\beta z e^{\beta z}-e^{\beta z}-1}{(1+e^{\beta z})^2}+\alpha+\mathcal{O}(\alpha^2)\\
			&\approx  \alpha -\frac{1}{(1+e^{\beta z})^2}
		\end{split}
	\end{equation}
	leading to the derivative of the expected cost
	\begin{equation}
		\begin{split}
			\frac{d\mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}]}{dU(\tilde{D})} &\approx \int \bigg(\alpha -\frac{1}{(1+e^{\beta z(\tilde{D},y_{n+1})})^2}\bigg)p(y_{n+1}|\tilde{D}) \mathrm{d}y_{n+1}\\
			& = \alpha -\int \frac{1}{(1+e^{\beta z(\tilde{D},y_{n+1})})^2}p(y_{n+1}|\tilde{D}) \mathrm{d} y_{n+1}.\\
		\end{split}
	\end{equation}
	For large $\beta$, $\frac{1}{(1+e^{\beta z(\tilde{D},y_{n+1})})^2}$ approaches the indicator $\mathbb{1}\{y_{n+1}>U(\tilde{D})\}$. Hence,
	\begin{equation}
		\int_{-\infty}^{\infty} p(y_{n+1}|\tilde{D})\frac{1}{(1+e^{\beta z(\tilde{D},y_{n+1})})^2} \mathrm{d}y_{n+1} \approx \int_{U(\tilde{D})}^{\infty} p(y_{n+1}|\tilde{D}) \mathrm{d}y_{n+1}
	\end{equation}
	This means the optimal decision rule can be written as follows
	\begin{equation}
		\alpha \approx \int_{U^*(\tilde{D})}^{\infty} p(y_{n+1}|\tilde{D}) \mathrm{d}y_{n+1}.
		\label{eq:quantile_decision_rule}
	\end{equation}
	The optimal decision $U^*(\tilde{D})$ is the $\alpha$-quantile of the conditional distribution $p(y_{n+1}|\tilde{D})$. This rule is known as the quantile decision rule.
\end{example}

\subsection{Discrete Action Space}
In case of a continuous action space, the conditional expected loss from \thref{theorem:opt_decision_rule} can be written
\begin{equation}
	\mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}] = \sum_{y_{n+1}\in \Omega_Y}C(U(\tilde{D}),y_{n+1})p(y_{n+1}|\tilde{D}),
	\label{eq:conditional_cost_discrete}
\end{equation}
where the cost function can be represented in matrix form as follows
\begin{center}
	\begin{tabular}{ c  c  c  c  c  }
		&& $Y_{n+1}$& & \\
		&& $y^{(1)}$ & \dots & $y^{(\text{dim}(\Omega_Y))}$ \\
		\cline{3-5}
		$U(\tilde{D})$ & $u^{(1)}$& \multicolumn{1}{|l}{$C(u^{(1)}, y^{(1)})$} &\multicolumn{1}{l}{\dots}&\multicolumn{1}{l|}{$C(u^{(1)}, y^{(\text{dim}(\Omega_Y))})$} \\
		& \vdots & \multicolumn{1}{|l}{\vdots} &\multicolumn{1}{l}{\vdots}&\multicolumn{1}{l|}{\vdots} \\
		& $u^{(\text{dim}(\Omega_U))}$ & \multicolumn{1}{|l}{$C(u^{(\text{dim}(\Omega_U))}, y^{(1)})$} &\multicolumn{1}{l}{\dots}&\multicolumn{1}{l|}{$C(u^{(\text{dim}(\Omega_U))}, y^{(\text{dim}(\Omega_Y)}))$} \\
		\cline{3-5}
	\end{tabular}
\end{center}
Note that the upper index represent realized values of $y_{n+1}$ whereas a lower index represent datapoints.

\begin{example}
	\label{ex:confusion}
	Consider a binary classification problem with action space $\Omega_U = \{u^{(1)},u^{(2)}\}$ and Nature's state space $\Omega_Y = \{y^{(1)}, y^{(2)}\}$, where $u^{(1)}$ corresponds to predicting class $y^{(1)}$ and $u^{(2)}$ to predicting class $y^{(2)}$. Let
	\begin{equation}
		D = \{(x_i,y_i)\}_{i=1}^n
	\end{equation}
	denote the data, where $y_i \in \Omega_Y$ are observed realizations of Nature's states. Let $U(x_{n+1},D)$ be a classifier based on the probability 
	\begin{equation}
		p_{Y_{n+1}\mid X_{n+1}, (X,Y)_{1\colon n}}(y_{n+1} | x_{n+1}, D).
	\end{equation}
	Define a threshold $k\in[0,1]$ and the decision rule
	\begin{equation}
		U_m(x_{n+1},D) =
		\begin{cases}
			u^{(1)}, & p_{Y_{n+1}\mid X_{n+1}, (X,Y)_{1\colon n}}(y^{(2)} | x_{n+1}, D) < m,\\
			u^{(2)}, & p_{Y_{n+1}\mid X_{n+1}, (X,Y)_{1\colon n}}(y^{(2)} | x_{n+1}, D) \ge m.
		\end{cases}
		\label{eq:decision_rule31}
	\end{equation}
	For a fixed threshold $m$, classifier performance is summarized in the confusion matrix
	\begin{center}
		\begin{tabular}{ c  c  c c}
			&& $Y_{n+1}$ &  \\
			&& $y^{(1)}$ & $y^{(2)}$ \\
			\cline{3-4}
			$U_m(x_{n+1},D)$ & $u^{(1)}$& \multicolumn{1}{|l}{TP(k)} & \multicolumn{1}{l|}{FP(k)}\\
			& $u^{(2)}$& \multicolumn{1}{|l}{FN(k)} & \multicolumn{1}{l|}{TN(k)}\\
			\cline{3-4}
		\end{tabular}
	\end{center}
\end{example}

\begin{example}
	Consider a binary classification problem with action space $\Omega_U = \{u^{(1)},u^{(2)}\}$ and Nature's state space $\Omega_Y = \{y^{(1)}, y^{(2)}\}$, where $u^{(1)}$ corresponds to predicting class $y^{(1)}$ and $u^{(2)}$ to predicting class $y^{(2)}$. Let
	\begin{equation}
		D = \{(x_i,y_i)\}_{i=1}^n
	\end{equation}
	denote the data, where $y_i \in \Omega_Y$ are observed realizations of Nature's states and let the cost function be defined by the matrix
	\begin{center}
		\begin{tabular}{ c  c  c  c }
			&& $Y_{n+1}$& \\
			&& $y^{(1)}$ & $y^{(2)}$  \\
			\cline{3-4}
			$U(x_{n+1,D})$ & $u^{(1)}$& \multicolumn{1}{|l}{$0$} &\multicolumn{1}{l|}{$\lambda_{12}$}  \\
			& $u^{(2)}$& \multicolumn{1}{|l}{$\lambda_{21}$} & \multicolumn{1}{l|}{$0$} \\
			\cline{3-4}
		\end{tabular}
	\end{center}
	where $\lambda_{12}$ denotes the cost of predicting $y^{(1)}$ when the true state is $y^{(2)}$, and $\lambda_{21}$ the cost of the reverse error.  The decision $U(\tilde{D})$ is determined by minimizing the conditional expected cost (\EQref{eq:conditional_cost_discrete})
	\begin{equation}
		\begin{split}
			\mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}] & = \sum_{y_{n+1}\in \Omega_Y}C(U(\tilde{D}),y_{n+1})p(y_{n+1}|\tilde{D})\\
			& = C(U(\tilde{D}),y^{(1)})p(y^{(1)}|\tilde{D})\\
			& \quad+C(U(\tilde{D}),y^{(2)})p(y^{(2)}|\tilde{D}).\\
		\end{split}
	\end{equation}
	For the different possible actions
	\begin{equation}
		\begin{split}
			\mathbb{E}[C(u^{(1)}, Y_{n+1})|\tilde{D}] &= \lambda_{12}p(y^{(2)}|\tilde{D}),\\
			\mathbb{E}[C(u^{(2)}, Y_{n+1})|\tilde{D}] &= \lambda_{21}p(y^{(1)}|\tilde{D}),\\
		\end{split}
	\end{equation}
	$U(\tilde{D})=u_1$ iff
	\begin{equation}
		\mathbb{E}[C(u^{(1)},Y_{n+1})|\tilde{D}]<\mathbb{E}[C(u^{(1)},Y_{n+1})|\tilde{D}])
	\end{equation}
	meaning
	\begin{equation}
		\begin{split}
			\lambda_{12}p(y^{(2)}|\tilde{D})&<\lambda_{21}p(y^{(1)}|\tilde{D})\\
			&=\lambda_{21}(1-p(y^{(2)}|\tilde{D}))
		\end{split}
	\end{equation}
	meaning $U(\tilde{D}) = u_1$ iff
	\begin{equation}
		p(y^{(2)}|\tilde{D})<\frac{\lambda_{21}}{\lambda_{12}+\lambda_{21}}=m.
		\label{eq:threshold31}
	\end{equation}
	\EQref{eq:threshold31} is equivalent to \EQref{eq:decision_rule31} from \exref{ex:confusion}. The difference between the two is that if $\lambda_{12}$ and $\lambda_{21}$ are specified, $m$ is specified. In \exref{ex:confusion} $m$ was left as a free parameter.
\end{example}


\begin{example}
	In many classification problems the Robot has the option of assigning $x_{n+1}$ to class $k\in K$ or, if the Robot is too uncertain, choosing a reject option. If the cost for rejection is less than the cost of falsely classifying the object, it may be the optimal action. Define the cost function as follows
	\begin{equation}
		C(U(\tilde{D}),y_{n+1})=\begin{cases}
			0 & \text{if correct classification ($U(\tilde{D})=y_{n+1}$)}\\
			\lambda_r & \text{if reject option ($U(\tilde{D})=\operatorname{reject}$)}\\
			\lambda_s & \text{if wrong classification ($U(\tilde{D})\neq y_{n+1}$)}\\
		\end{cases}.
	\end{equation}
	The conditional expected cost if the Robot does not pick the reject option, meaning $U(\tilde{D})\in \Omega_U\setminus\operatorname{reject}$, is
	\begin{equation}
		\begin{split}
			\mathbb{E}[C(U(\tilde{D}), Y_{n+1})|\tilde{D}] & = \sum_{y_{n+1}\in \Omega_Y} C(U(\tilde{D}),y_{n+1})p(y_{n+1}|\tilde{D})\\
			&= \lambda_s(1-p(U(\tilde{D})|\tilde{D})),
		\end{split}
		\label{eq:cost1a}
	\end{equation}
	where for the second equality it has been used that the cost of a correct classification is $0$. For the third equality it has been used that summing over all but $y_{n+1} =U(\tilde{D})$ is equal to $1-p(U(\tilde{D})|\tilde{D})$. The larger $p(U(\tilde{D})|\tilde{D})$, the smaller the expected cost (for $\lambda_s>0$), meaning the expected cost is minimized for the largest probability. The conditional expected cost if the Robot picks the reject option is
	\begin{equation}
		\mathbb{E}[C(\operatorname{reject}, Y_{n+1})|\tilde{D}]=\lambda_r,
		\label{eq:cost2a}
	\end{equation}
	and from \EQref{eq:cost1a} and \EQref{eq:cost2a} it follows that picking 
	\begin{equation}
		\argmax_{U(\tilde{D})\in \Omega_U\setminus \operatorname{reject}} p(U(\tilde{D})|\tilde{D})
	\end{equation}
	is the best option among classes $U(\tilde{D})\neq \operatorname{reject}$. To be the best option overall, it also needs to have lower cost than the reject option. Using \EQref{eq:cost1a} and \EQref{eq:cost2a} yields
	\begin{equation}
		(1-p(U(\tilde{D})|\tilde{D}))\lambda_s< \lambda_r,
	\end{equation}
	meaning
	\begin{equation}
		p(U(\tilde{D})|\tilde{D})\geq 1-\frac{\lambda_r}{\lambda_s}.
	\end{equation}
	Qualitatively, as $\frac{\lambda_r}{\lambda_s}$ is increased from $0$ to $1$, the behavior of the Robot changes smoothly. When
	\begin{equation}
		\frac{\lambda_r}{\lambda_s}=0,
	\end{equation}
	rejection is rated as a successful classification -- i.e., there is no cost associated with it -- and thus becomes the best option unless
	\begin{equation}
		p(U(\tilde{D})|\tilde{D})=1,
	\end{equation}
	corresponding to knowing the correct class with absolute certainty. In other words, in this limit rejection is optimal unless the Robot is completely certain of the correct class. Conversely, when
	\begin{equation}
		\frac{\lambda_r}{\lambda_s}=1,
	\end{equation}
	rejection is rated as a misclassification -- i.e., $\lambda_r=\lambda_s$ -- and thus always incurs a cost. In this case, rejection is never chosen. For values of $\frac{\lambda_r}{\lambda_s}$ between these limits, an interpolation of interpretations applies, where rejection is preferred only when the Robot's uncertainty exceeds the corresponding threshold.
\end{example}
