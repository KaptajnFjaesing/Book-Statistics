Let $(E,(\mathbb{P}_{\vec{\theta}})_{w\in \Omega_W})$ be a statistical model associated with a sample of i.i.d. random variables $X_1,\dots X_n$. Assume that there exists $w^*\in\Theta$ such that $X_1\sim \mathbb{P}_{\vec{\theta}^*}:\vec{\theta}^*$ is the true parameter. Given $X_1,\dots X_n$ the goal is to find an estimator $\hat{\vec{\theta}}=\hat{\vec{\theta}}(X_1,\dots X_n)$ such that $\mathbb{P}_{\hat{\vec{\theta}}}$ is close to $\mathbb{P}_{\vec{\theta}^*}$ for the true parameter $\vec{\theta}^*$. This means
\begin{equation}
	|\mathbb{P}_{\hat{\vec{\theta}}}(A)-\mathbb{P}_{\vec{\theta}^*}(A)|=\text{small},\quad \forall A\subset E.
\end{equation}
The \emph{total variation distance} (TV) between two probability measures $\mathbb{P}_{\hat{\vec{\theta}}}$ and $\mathbb{P}_{\vec{\theta}'}$ is defined by
\begin{equation}
	TV(\mathbb{P}_{\hat{\vec{\theta}}},\mathbb{P}_{\vec{\theta}'})=\max_{A\subset E}|\mathbb{P}_{\hat{\vec{\theta}}}(A)-\mathbb{P}_{\vec{\theta}'}(A)|.
\end{equation}
If TV is small, then $\mathbb{P}_{\hat{\vec{\theta}}}\simeq \mathbb{P}_{\vec{\theta} '}$ $\forall >\subset E$. The TV obey 
\begin{equation}
	\begin{split}
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})=TV(\mathbb{P}_{\vec{\theta}'},\mathbb{P}_{\vec{\theta}})\quad \text{(Symmetric)},\\
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})\geq 0,\\
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})=0\,\text{if } \mathbb{P}_{\vec{\theta}}=\mathbb{P}_{\vec{\theta}'}\quad \text{(definite)},\\
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})\leq TV(\mathbb{P}_{\vec{\theta}''},\mathbb{P}_{\vec{\theta}})+TV(\mathbb{P}_{\vec{\theta}''},\mathbb{P}_{\vec{\theta}'})\quad \text{(Triangle inequality)}.\\
	\end{split}
\end{equation}
These identities imply that the total variation is a distance between probability distributions. Graphically $2TV$ is the area of two distributions that does not overlap. 
\begin{example}
	If $TV(\mathbb{P}_{\hat{\vec{\theta}}},\mathbb{P}_{\vec{\theta}'})\leq k$ then
	\begin{equation}
		\mathbb{P}_{\hat{\vec{\theta}}}(A)\in [\mathbb{P}_{\vec{\theta}'}(A)\pm k], \quad \forall A\subset E.
	\end{equation}
\end{example}
In the case of discrete and continuous random variables TV takes the form
\begin{equation}
	\begin{split}
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta'}})=\frac{1}{2}\sum_{x_i\in E}|f_{\vec{\theta}}(x_i)-f_{\vec{\theta'}}(x_i)|\quad \text{For a discrete random variables},\\
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})=\frac{1}{2}\int_E|f_{\vec{\theta}}(x)-f_{\vec{\theta}'}(x)|\quad \text{For a continuous random variables}.
	\end{split}
\end{equation}
Ideally an estimator $\widehat{TV}(\mathbb{P}_{\hat{\vec{\theta}}},\mathbb{P}_{\vec{\theta}^*})$ for all $\vec{\theta}\in \Theta$ should now be constructed such that one could find $\hat{\vec{\theta}}$ that minimize the function $\vec{\theta}\mapsto \widehat{TV}(\mathbb{P}_{\hat{\vec{\theta}}},\mathbb{P}_{\vec{\theta}^*})$. However, there is no simple way of estimating $TV$, so a different, somewhat equivalent, measure (of which there are many) is considered; the Kullback-Leibler (KL) divergence. The KL divergence  between two probability measures $\mathbb{P}_{\vec{\theta}}$ and $\mathbb{P}_{\vec{\theta}'}$ is defined by
\begin{equation}
	KL(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})=\begin{cases}
		\sum_{x_i\in E}f_{\vec{\theta}}(x_i)log\bigg(\frac{f_{\vec{\theta}}(x_i)}{f_{\vec{\theta'}}(x_i)}\bigg) \quad \text{If $E$ is discrete}\\
		\int_Edx f_{\vec{\theta}}(x)log\bigg(\frac{f_{\vec{\theta}}(x)}{f_{\vec{\theta'}}(x)}\bigg) \quad \text{If $E$ is continuous}\\
	\end{cases},
\end{equation}
where $log$ has an unspecified base. The KL divergence obey
\begin{equation}
	\begin{split}
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})\neq TV(\mathbb{P}_{\vec{\theta}'},\mathbb{P}_{\vec{\theta}})\quad \text{(in general)},\\
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})\geq 0,\\
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})=0\,\text{if } \mathbb{P}_{\vec{\theta}}=\mathbb{P}_{\vec{\theta}'}\quad \text{(definite)} ,\\
		&TV(\mathbb{P}_{\vec{\theta}},\mathbb{P}_{\vec{\theta}'})\nleq  TV(\mathbb{P}_{\vec{\theta}''},\mathbb{P}_{\vec{\theta}})+TV(\mathbb{P}_{\vec{\theta}''},\mathbb{P}_{\vec{\theta}'})\quad \text{(in general)}.\\
	\end{split}
\end{equation}
From these identities it is clear that the KL divergence is actually not a distance but a divergence. The KL divergence can be written in terms of the expectation value
\begin{equation}
	\begin{split}
		KL(\mathbb{P}_{\vec{\theta}^*},\mathbb{P}_{\vec{\theta}})&=\mathbb{E}_{\vec{\theta}^*}\bigg[log\bigg(\frac{f_{\vec{\theta}^*}(X)}{f_{\vec{\theta}}(X)}\bigg)\bigg]\\
		&=\mathbb{E}_{\vec{\theta}^*}[log(f_{\vec{\theta}^*}(X))]-\mathbb{E}_{\vec{\theta}^*}[log(f_{\vec{\theta}}(X))].
	\end{split}
\end{equation}
So, the function $\vec{\theta}\mapsto KL(\mathbb{P}_{\vec{\theta}^*},\mathbb{P}_{\vec{\theta}})$ is on the form $const-\mathbb{E}_{\vec{\theta}^*}[log(f_{\vec{\theta}}(X))]$. This can be estimated 
\begin{equation}
	\widehat{KL}(\mathbb{P}_{\vec{\theta}^*},\mathbb{P}_{\vec{\theta}})=const-\frac{1}{n}\sum_{i=1}^nlog(f_{\vec{\theta}}(X_i)).
\end{equation}
This is a candidate estimator for $KL$. Minimizing $\widehat{KL}$ will result in $\vec{\theta}\simeq \vec{\theta}^*$, so consider
\begin{equation}
	\begin{split}
		\argmin_{\vec{\theta}}(\widehat{KL}(\mathbb{P}_{\vec{\theta}^*},\mathbb{P}_{\vec{\theta}}))&=\argmin_{\vec{\theta}}\bigg(const-\frac{1}{n}\sum_{i=1}^nlog(f_{\vec{\theta}}(X_i))\bigg)\\
		&=\argmin_{\vec{\theta}}\bigg(-\frac{1}{n}\sum_{i=1}^nlog(f_{\vec{\theta}}(X_i))\bigg)\\
		&=\argmax_{\vec{\theta}}\bigg(\frac{1}{n}\sum_{i=1}^nlog(f_{\vec{\theta}}(X_i))\bigg)\\
		&=\argmax_{\vec{\theta}}\bigg(\sum_{i=1}^nlog(f_{\vec{\theta}}(X_i))\bigg)\\
		&=\argmax_{\vec{\theta}}\bigg(log\bigg(\prod_{i=1}^nf_{\vec{\theta}}(X_i)\bigg)\bigg)\\
	\end{split}
\end{equation}
where it has been used that the additive constant and constant rescaling factors does not change the location of the minimum or maximum. Since $log(f)$ is an increasing function, maximizing $log(f)$ is equivalent to maximizing $f$. Therefore
\begin{equation}
	\begin{split}
		\argmin_{\vec{\theta}}(\widehat{KL}(\mathbb{P}_{\vec{\theta}^*},\mathbb{P}_{\vec{\theta}}))\Leftrightarrow \argmax_{\vec{\theta}}\bigg(\underbrace{\prod_{i=1}^nf_{\vec{\theta}}(X_i)}_{\equiv likelihood}\bigg).\\
	\end{split}
\end{equation}
The likelihood is defined viz.
\begin{equation}
	L(\vec{\theta}|X_1,\dots,X_n)\equiv\prod_{i=1}^nf_{\vec{\theta}}(X_i).
	\label{ll}
\end{equation}
$L(\vec{\theta})$ is the joint density of the vector $(X_1,\dots, X_n)$. The likelihood is a concave function, meaning that if it has a local maximum it must also be the global maximum. The formal definition of the maximum likelihood estimator is as follows; let $X_1,\dots X_n$ be an i.i.d. sample associated with a statistical model $(E,(\mathbb{P}_{\vec{\theta}})_{\vec{\theta}\in \Theta})$ and let $L$ be the corresponding likelihood. The maximum likelihood estimator of $\vec{\theta}$ is then defined as
\begin{equation}
	\begin{split}
		\hat{\vec{\theta}}^{MLE}_n&\equiv \argmax_{\vec{\theta}\in \Theta}\bigg(L(X_1,\dots, X_n,\vec{\theta})\bigg)\\
		&= \argmax_{\vec{\theta}\in \Theta}\bigg(log(L(X_1,\dots, X_n,\vec{\theta}))\bigg)
	\end{split}
\end{equation}
provided it exists. The performance of the maximum likelihood estimate is quantified by the \emph{Fischer information}. Define
\begin{equation}
	\ell(\vec{\theta})=log(L_1(X,\vec{\theta})), \quad \vec{\theta}\in\Theta\subset \mathbb{R}^d,
\end{equation}
where the subscript $1$ signifies that $X$ consist of only one random variable. The Fischer information of the statistical model is defined as
\begin{equation}
	I(\vec{\theta})=-\mathbb{E}[\vec{\nabla}^2\ell(\vec{\theta})].
\end{equation}
If this is large, the maximum likelihood estimate is good. Now let $\vec{\theta}^*\in \Theta$ (the true parameter) and assume
\begin{enumerate}
	\item The model is specified.
	\item $\forall \vec{\theta}\in \Theta$, the support of $\mathbb{P}_{\vec{\theta}}$ does not depend on $\vec{\theta}$.
	\item $\vec{\theta}^*$ is not on the boundary of $\Theta$.
	\item $I(\vec{\theta})$ is invertible in the neighborhood of $\vec{\theta}^*$.
	\item A few more technical conditions.
\end{enumerate}
Then, $\hat{\vec{\theta}}^{MLE}_n$ satisfies
\begin{equation}
	\hat{\vec{\theta}}_n^{MLE}\xrightarrow[]{n\rightarrow\infty}\vec{\theta}^* \quad (w.r.t. \, \mathbb{P}_{\vec{\theta}^*})\quad \wedge \quad \sqrt{n}(\hat{\vec{\theta}}_n^{MLE}-\vec{\theta}^*)\xrightarrow[]{n\rightarrow\infty}\mathcal{N}(0,I(\vec{\theta}^*)^{-1})\quad (w.r.t. \, \mathbb{P}_{\vec{\theta}^*}).
\end{equation}

\begin{example}
	\emph{Determine the maximum likelihood estimate of $p$ for the model $(\{0,1\},\{Ber(p)\}_{p\in(0,1)})$.}\newline
	
	\noindent In this case
	\begin{equation}
		\begin{split}
			L(p)=\prod_{i=1}^np^{x_i}(1-p^{x_i}).
		\end{split}
	\end{equation}
	Considering the form of the likelihood function it is expedient to maximize the logarithm of the likelihood, such that
	\begin{equation}
		\begin{split}
			argmax_p\bigg(log(L(p))\bigg)&= argmax_p\bigg(log\bigg(\prod_{i=1}^np^{x_i}(1-p^{x_i})\bigg)\bigg)\\
			&= argmax_{p}\bigg(\bigg(\sum_{i=1}^nx_i\bigg)log(p)+\bigg(n-sum_{i=1}^nx_i\bigg)log(1-p)\bigg).
		\end{split}
	\end{equation}
	Now 
	\begin{equation}
		\frac{dlog(L(p))}{dp}=\frac{1}{log(base)}\bigg[\frac{\sum_{i=1}^nx_i}{p}-\frac{n-\sum_{i=1}^nx_i}{1-p}\bigg].
	\end{equation}
	Requiring the derivative to vanish means the maximum likelihood estimate of $p$ is given by
	\begin{equation}
		\hat{p}=\frac{1}{n}\sum_{i=1}^nx_i.
	\end{equation}
\end{example}
\begin{example}
	\emph{Determine the maximum likelihood estimate of $\lambda$ for the model $([0,\infty),\{Exp(\lambda)\}_{\lambda>0})$.}\newline
	
	\noindent In this case
	\begin{equation}
		\begin{split}
			L(\lambda)=\prod_{i=1}^n\lambda e^{-\lambda x_i}.
		\end{split}
	\end{equation}
	Again maximize the logarithm of the likelihood function
	\begin{equation}
		\frac{dlog(L(\lambda))}{d\lambda}=\frac{n}{\lambda}-\sum_{i=1}^nx_i
	\end{equation}
	Requiring the derivative to vanish means the maximum likelihood estimate of $\lambda$ is given by
	\begin{equation}
		\hat{\lambda}=\frac{1}{\frac{1}{n}\sum_{i=1}^nx_i}.
	\end{equation}
\end{example}

\subsection{The Method of Least Squares}
The method of least squares (LS) is the standard approach to regression analysis in which a statistic, $g$, describes a measured, quantitative response, $Y$. That is
\begin{equation}
	Y=g(X_1\dots X_n)+\varepsilon,
\end{equation}  
where $\varepsilon$ is an error-term - often it is assumed that $\varepsilon\sim\mathcal{N}(0,\sigma^2)$. In this equation $g(X_1\dots X_n)$ represents the systematic variations in $Y$ and $\varepsilon$ represent the random. However, in the case where unaccounted systematic measurement uncertainties are present in $Y$, and $g$ is known, $\varepsilon$ can contain systematic effects as well. The measured response is estimated by estimating $f$
\begin{equation}
	\hat{Y}=\hat{g}(X_1\dots X_n,\vec{\theta}).
\end{equation}
In the method of least squares $\hat{\vec{\theta}}$ is determined by minimizing the sum of squared residuals. 

\subsection{Independent $y_i$ and $x_i$} In the case where the measurements, $y_i$ and $x_i$, are independent (no systematic uncertainties occur in $\delta y_i$ and $\delta x_i$)
\begin{equation}
	S(\vec{\theta})=\sum_{i=1}^n\bigg[\bigg(\frac{y_i-\hat{g}(\hat{x}_i,\vec{\theta})}{\delta y_i}\bigg)^2+\bigg(\frac{x_i-\hat{x}_i}{\delta x_i}\bigg)^2\bigg],
	\label{eq6}
\end{equation}
where $\delta y_i$ and $\delta x_i$ are the uncertainties (standard deviations) of the individual measured data points and $\hat{x}$ is the (unknown\footnote{There is by definition no function that estimates the independent variable.}) estimate of the independent variable. $\hat{\vec{\theta}}$ is estimated via the least squares method viz
\begin{equation}
	\hat{\vec{ \theta}}_n^{LS}=\argmin_{\vec{\theta},\hat{x}}(S).
\end{equation}
The minimization can be be carried out via numerical iteration. However, the job can be simplified by carrying out the minimization over $\hat{x}$ analytically. Consider
\begin{equation}
	\begin{split}
		\frac{\partial S}{\partial \hat{x}_i}&=-2\frac{y_i-\hat{g}(\hat{x}_i,\vec{\theta})}{\delta y_i^2}\frac{\partial \hat{g}(\hat{x}_i,\vec{\theta})}{\partial \hat{x}_i}-2\frac{x_i-\hat{x}_i}{\delta x_i^2}\\
		&=0
	\end{split}
	\label{eq3}
\end{equation}
Now, let
\begin{equation}
	\hat{g}(x_i,\vec{\theta})=\hat{g}(\hat{x}_i,\vec{\theta})+(x_i-\hat{x}_i)\frac{\partial \hat{g}(x_i,\vec{\theta})}{\partial x_i}\bigg|_{x_i=\hat{x}_i}+\mathcal{O}(\partial^2\hat{g}).
	\label{eq4}
\end{equation}
Using equation \eqref{eq4} in \eqref{eq3}
\begin{equation}
	\begin{split}
		-\frac{y_i-[\hat{g}(x_i,\vec{\theta})-(x_i-\hat{x}_i)\frac{\partial \hat{g}(x_i,\vec{\theta})}{\partial x_i}\bigg|_{x_i=\hat{x}_i}]}{\delta y_i^2}\frac{\partial \hat{g}(\hat{x}_i,\vec{\theta})}{\partial \hat{x}_i}-\frac{x_i-\hat{x}_i}{\delta x_i^2}+\mathcal{O}(\partial^2\hat{g})=0.
	\end{split}
\end{equation}
Now, define $\hat{g'}\equiv \frac{\partial \hat{g}(x_i,\vec{\theta})}{\partial x_i}\bigg|_{x_i=\hat{x}_i}=\frac{\partial \hat{g}(\hat{x}_i,\vec{\theta})}{\partial \hat{x}_i}$. Hereby
\begin{equation}
	\begin{split}
		\hat{x}_i=x_i+\frac{\delta x_i^2}{\delta_i^2}(y_i-\hat{g}(x_i,\vec{\theta}))+\mathcal{O}(\partial^2\hat{g}),
	\end{split}
	\label{eq5}
\end{equation}
where
\begin{equation}
	\delta_i^2=\delta y_i^2+\delta x_i^2\hat{g'}^2.
\end{equation}
Using equation \eqref{eq5} and \eqref{eq4} in equation \eqref{eq6}
\begin{equation}
	S(\vec{\theta})\simeq\sum_{i=1}^n\bigg(\frac{y_i-\hat{g}(x_i,\vec{\theta})}{\delta_i}\bigg)^2.
	\label{eq7}
\end{equation}
Often the uncertainty of the independent variable can be neglected. In this case $\delta x_i\simeq 0$ and equation \eqref{eq7} becomes
\begin{equation}
	S(\vec{\theta})=\sum_{i=1}^n\bigg(\frac{y_i-\hat{g}(x_i,\vec{\theta})}{\delta y_i}\bigg)^2,
	\label{eq8}
\end{equation}
which is the famous equation of least squares. Least squares estimates for the parameters can now be obtained from
\begin{equation}
	\hat{\vec{ \theta}}_n^{LS}=\argmin_{\vec{\theta}\in \Theta}(S).
\end{equation}
In the case where $\delta x_i\neq 0$ equation \eqref{eq7} in its entirely must be applied. In doing so it should be noted that $\hat{g'}$ depends on $\hat{x}$ which is an unknown quantity. The minimization of equation \eqref{eq7} therefore proceeds via an iterative procedure
\begin{enumerate}
	\item Guess some value of $\hat{g'}$.
	\item Calculate $\delta_i$.
	\item Minimize $S$ with respect to $\vec{\theta}$.
	\item Use the value of $\vec{\theta}$ obtained from minimization along with $\hat{x}_i\approx x_i$ to calculate $\hat{g'}$.
	\item Go to step 2.\newline
\end{enumerate}
The estimations performed by the least squares can be controlled further by introducing terms that penalize the function estimate, $\hat{g}$, further such that
\begin{equation}
	S(\vec{\theta},\lambda)=\sum_{i=1}^n\bigg(\frac{y_i-\hat{g}(x_i,\vec{\theta})}{\delta y_i}\bigg)^2+\lambda J(\hat{g}),
	\label{eq8p}
\end{equation} 
where $\lambda\geq 0$ is a parameter controlling the magnitude of the penalty and $J$ is a user-selected functional.
\begin{example}
	An example of the user-selected functional is the one used for smoothing splines
	\begin{equation}
		J(\hat{g})=\int dx \bigg(\frac{\partial ^2 \hat{g}(x,\vec{ \theta})}{\partial x^2}\bigg)^2.
	\end{equation}
\end{example}


\subsection{Least Squares vs. Maximum Likelihood}
The least squares estimation procedure is identical to the maximum likelihood estimation procedure in the case where the \emph{residuals are normally distributed}\footnote{Normally it is required that the random variables is normally distributed. However, in this case $\hat{g}\rightarrow \hat{\mu}$ is the mean and so by definition $X-\mu\sim \mathcal{N}$ if $X\sim \mathcal{N}$.} and the parameter to be estimated is the population mean. In this case
\begin{equation}
	\begin{split}
		L(\vec{\theta})&=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\delta y_i}e^{-\frac{1}{2}\big(\frac{y_i-\hat{g}(\hat{x}_i,\vec{\theta})}{\delta y_i}\big)^2}\frac{1}{\sqrt{2\pi}\delta x_i}e^{-\frac{1}{2}\big(\frac{x_i-\hat{x}_i}{\delta x_i}\big)^2}.
	\end{split}
\end{equation}
The maximum likelihood estimate of $\vec{\theta}$ is obtained by maximizing the logarithm of the likelihood function\footnote{The terms $\sim ln(\sqrt{2\pi}\delta y)$ does not affect the minimization over the parameters.} 
\begin{equation}
	\begin{split}
		\hat{\vec{\theta}}^{MLE}_n&=\argmin_{\vec{\theta}\in \Theta}\bigg[\sum_{i=1}^n\bigg[\bigg(\frac{y_i-\hat{g}(\hat{x}_i,\vec{\theta})}{\delta y_i}\bigg)^2+\bigg(\frac{x_i-\hat{x}_i}{\delta x_i}\bigg)^2\bigg]\bigg]\\
		&=  \argmin_{\vec{\theta}\in \Theta}[S(\vec{ \theta})].
	\end{split}
\end{equation}
The key point here is that the estimates of the two procedures only equal in the case where the residuals are normally distributed.
\begin{example}
	The least squares estimate of data ($\hat{g}\rightarrow \hat{\theta}$) in the case where $\delta x_i\simeq 0$ and $\delta y_i$ contain no systematic uncertainties can be determined viz
	\begin{equation}
		\frac{\partial S}{\partial \hat{\theta}}=2\sum_i\frac{y_i-\hat{g}}{\sigma_i^2}=0\Rightarrow \hat{\theta	}=\frac{\sum_i\frac{y_i}{\delta y_i^2}}{\sum_i\frac{1}{\delta y_i^2}}.
	\end{equation}
	This is the error-weighted mean. Hence, the error weighted mean is the least squares estimator of data\footnote{The mean is an attempt at assigning one value to data. Hence, the mean is an estimate of data.}. In the case where the residuals are normally distributed, the error-weighted mean will also be the maximum likelihood estimate of data.
\end{example}