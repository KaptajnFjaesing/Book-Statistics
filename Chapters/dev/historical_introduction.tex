\chapter{Historical Introduction}
\label{chp:intro}
Statistics is a mathematical discipline that use probability theory to extract insights from information (data). Probability theory is a branch of pure mathematics -- probabilistic questions can be posed and solved using axiomatic reasoning, and therefore there is one correct answer to any probability question. Statistical questions can be converted to probability questions by the use of probability models. Given certain assumptions about the mechanism generating the data, statistical questions can be answered using probability theory. This highlights the dual nature of statistics, comprised of two integral parts.
\begin{enumerate}
	\item The first part involves the formulation and evaluation of probabilistic models, a process situated within the realm of the philosophy of science. This phase grapples with the foundational aspects of constructing models that accurately represent the problem at hand.
	\item The second part concerns itself with extracting answers after assuming a specific model. Here, statistics becomes a practical application of probability theory, involving not only theoretical considerations but also numerical analysis in real-world scenarios.
\end{enumerate}
This duality underscores the interdisciplinary nature of statistics, bridging the gap between the conceptual and the applied aspects of probability theory.\newline

Probability theory was a relative latecomer in intellectual human history. It began to take shape in the context of games of chance and gambling in the 16th century. Italian writers of the fifteenth and sixteenth centuries, notably Pacioli (1494), Tartaglia (1556), and Cardan (1545), had discussed the problem of the division of a stake between two players whose game was interrupted before its close. The problem was proposed to Pascal and Fermat in 1654. The correspondence which ensued between Fermat and Pascal, was fundamental in the development of modern concepts of probability. Their inspiration came from a problem about games of chance, proposed by the chevalier de Mere. De Mere inquired about the proper division of the stakes when a game of chance is interrupted. Suppose two players, A and B, are playing a three-point game, each having wagered 32 pistoles, and are interrupted after A has two points and B has one. How much should each receive? Fermat and Pascal proposed somewhat different solutions, though they agreed about the numerical answer. Each undertook to define a set of equal or symmetrical cases, then to answer the problem by comparing the number for A with that for B. Fermat, however, gave his answer in terms of the chances, or probabilities. He reasoned that two more games would suffice in any case to determine a victory. There are four possible outcomes, each equally likely in a fair game of chance. A might win twice, AA; or first A then B might win; or B then A; or BB. Of these four sequences, only the last would result in a victory for B. Thus, the odds for A are 3:1, implying a distribution of 48 pistoles for A and 16 pistoles for B.\newline
Pascal thought Fermat's solution unwieldy, and he proposed to solve the problem not in terms of chances but in terms of the quantity now called “expectation”. Suppose B had already won the next round. In that case, the positions of A and B would be equal, each having won two games, and each would be entitled to 32 pistoles. A should receive his portion in any case. B's 32, by contrast, depend on the assumption that he had won the first round. This first round can now be treated as a fair game for this stake of 32 pistoles, so that each player has an expectation of 16. Hence A's lot is 32 + 16, or 48, and B's is just 16.\newline
In the 17th century, Pascal's strategy for solving problems of chance became the standard one. It was, for example, used by the Dutch mathematician Christiaan Huygens in his short treatise on games of chance, published in 1657. Huygens refused to define equality of chances as a fundamental presumption of a fair game but derived it instead from what he saw as a more basic notion of an equal exchange. Most questions of probability in the 17th century were solved, as Pascal solved his, by redefining the problem in terms of a series of games in which all players have equal expectations. The new theory of chances was not, in fact, simply about gambling but also about the legal notion of a fair contract. A fair contract implied equality of expectations, which served as the fundamental notion in these calculations. Measures of chance or probability were derived secondarily from these expectations.\newline
During the 18th-century Enlightenment probability came to be seen as a mathematical version of sound reasoning. In 1677 the German mathematician Gottfried Wilhelm Leibniz imagined a utopian world in which disagreements would be met by this challenge: "Let us calculate, Sir". The French mathematician Pierre-Simon de Laplace, in the early 19th century, called probability "good sense reduced to calculation". This ambition, bold enough, was not quite so scientific as it may first appear. For there were some cases where a straightforward application of probability mathematics led to results that seemed to defy rationality. One example, proposed by Nicolas Bernoulli and made famous as the St. Petersburg paradox, involved a bet with an exponentially increasing payoff. A fair coin is to be tossed until the first time it comes up heads. If it comes up heads on the first toss, the payment is 2 ducats; if the first time it comes up heads is on the second toss, 4 ducats; and if on the nth toss, 2n ducats. The mathematical expectation of this game is infinite, but no sensible person would pay a very large sum for the privilege of receiving the payoff from it. The disaccord between calculation and reasonableness created a problem, addressed by generations of mathematicians. Prominent among them was Nicolas's cousin Daniel Bernoulli, whose solution depended on the idea that a ducat added to the wealth of a rich man benefits him much less than it does a poor man (a concept now known as decreasing marginal utility).\newline
Many 18th-century ambitions for probability theory involved reasoning from effects to causes. Jakob Bernoulli formulated and proved a law of large numbers to give formal structure to such reasoning. This was published in 1713 from a manuscript, the Ars conjectandi, left behind at his death in 1705. There he showed that the observed proportion of, say, tosses of heads or of male births will converge as the number of trials increases to the true probability p, supposing that it is uniform. His theorem was designed to give assurance that when p is not known in advance, it can properly be inferred by someone with sufficient experience.\newline
The English physician and philosopher David Hartley announced in his Observations on Man (1749) that a certain “ingenious Friend” had shown him a solution of the "inverse problem" of reasoning from the occurrence of an event p times and its failure q times to the "original Ratio" of causes. But Hartley named no names, and the first publication of the formula he promised occurred in 1763 in a posthumous paper of Thomas Bayes, communicated to the Royal Society by the British philosopher Richard Price. This has come to be known as Bayes's theorem. But it was the French, especially Laplace, who put the theorem to work as a calculus of induction, and it appears that Laplace's publication of the same mathematical result in 1774 was entirely independent. The result was perhaps more consequential in theory than in practice. An exemplary application was Laplace's probability that the sun will come up tomorrow, based on 6000 years or so of experience in which it has come up every day. Laplace also argued in his Philosophical Essay on Probabilities (1825) that man's dependence on probability was simply a consequence of imperfect knowledge. A being who could follow every particle in the universe, and who had unbounded powers of calculation, would be able to know the past and to predict the future with perfect certainty. The same issues were discussed also in physics. Statistical understandings first gained an influential role in physics at just this time, in consequence of papers by the German mathematical physicist Rudolf Clausius from the late 1850s and, especially, of one by the Scottish physicist James Clerk Maxwell published in 1860. \newline
Although the foundation of probability theory was laid already in the 16th century, its axiomatization had to wait until Kolmogorov's classic Foundations of the Theory of Probability (1933)~\citep{kolmogorov1950foundations}. Within this axiomatic framework there is some freedom related to the interpretation of probability and as a consequence of this there exist different branches of statistics, related to how probability is interpreted and what it means. 