

\begin{equation}
	\begin{split}
		\rm BF &= \frac{p(D|A,I)}{\prod_{s=1,2}p(D_s|B,I)}\\
		&=\frac{\int p(D|A,\theta,I) p(\theta|A,I) d\theta}{\prod_{s=1,2}\int p(D_s|B,\theta_s,I) p(\theta_s|B,I)d\theta_s}
	\end{split},
	\label{eq:prob}
\end{equation}
where according to hypothesis $B$, $p(D|B,I) =p(x_1^{(1:n_1)}|B,\delta x_1^{(1:n_1)},I)p(x_2^{(1:n_2)}|B,\delta x_2^{(1:n_2)},I)$. The only difference between the three integrals in equation \eqref{eq:prob} is the data used. For this reason the remainder of the theoretical section will focus on the integral in the nominator
\begin{equation}
	I = \int p(x_0^{(1:n)}|A,\delta x_0^{(1:n)},\theta,I) p(\theta|A,\delta x_0^{(1:n)},I) d\theta
\end{equation}
and the generalization to the denominator should be straightforward. For ease of notation the index on $x_0$ will be omitted from hereon. Since there is an uncertainty $\delta x_j$ related to measurements $x_j$, the exact value of $x_j$ is unknown. Let $\tilde{x}_j$ denote this unknown exact value of $x_j$. The likelihood function ($p(x^{(1:n)}|A,\delta x^{(1:n)},\theta,I)$) can then be expressed as a product of marginal integrals over $\tilde{x}_j$ viz
\begin{equation}
	\begin{split}
		p(x^{(1:n)}|A,\theta,\delta x^{(1:n)}, I) &= \prod_{j=1}^{n}\int p(x_j,\tilde{x}_j|A,\theta,\delta x_j, I)d \tilde{x}_j\\
		&= \prod_{j=1}^{n}\int p(x_j|A,\theta,\delta x_j, \tilde{x}_j, I)p(\tilde{x}_j|A,\theta,\delta x, I)d \tilde{x}_j\\
	\end{split}
\end{equation}
with
\begin{equation}
	\begin{split}
		p(x_j|A,\theta,\delta x_j, \tilde{x}_j, I)
		& = p(x_j|A,\delta x_j, \tilde{x}_j, I),\\
		p(\tilde{x}_j|A,\theta,\delta x, I)
		&=p(\tilde{x}_j|A,\theta, I).\\
	\end{split}
\end{equation}
In practice the integrals associated to the Bayes factor is often intractable, meaning a numerical sampling technique like e.g. Nested sampling (see appendix \ref{app:NS}) have to be used.

======================


\begin{definition}[Discrete Probability Space]
	A discrete probability space is characterized by a sample space $\Omega$ that is at most countable. Probabilities are assigned to individual points in $\Omega$ through a probability mass function $p: \Omega \rightarrow [0,1]$, satisfying $\sum_{\omega\in \Omega} p(\omega) = 1$. In this context, all subsets of $\Omega$ are treated as events, and the event space is $F = 2^\Omega$, the power set of $\Omega$. The probability measure takes the form
	\begin{equation}
		p(A) = \sum_{\omega \in A} p(\omega) \quad \forall A \subseteq \Omega.
		\label{eq:disc}
	\end{equation}
\end{definition}
The case $p(\omega) = 0$ is permitted by equation \eqref{eq:disc} but is seldomly used, as such $\omega$ can be excluded from the sample space.

\begin{definition}[Continuous Probability Space]
	A continuous probability space is characterized by a sample space \( \Omega \) that is continuous, often represented as an interval on the real line. In this context, probabilities are described using a probability density function (PDF) \( p: \Omega \rightarrow \mathbb{R} \), with the property that \( \int_{\Omega} p(\omega) \, d\omega = 1 \). Events in a continuous probability space are subsets of \( \Omega \), and the event space is \( F \), a collection of subsets of \( \Omega \). The probability measure is given by integrating the PDF over the events viz
	\begin{equation}
		p(A) = \int_{A} p(\omega) \, d\omega \quad \forall A \subseteq \Omega.
		\label{eq:cont}
	\end{equation}
\end{definition}
The PDF \( p(\omega) \) (see section \ref{sec:notation} for a discussion on notation) represents the likelihood of different outcomes within the continuous sample space.	However, it is important to note that \( f(X=x) = 0 \) for any individual point \( x\in \mathbb{X} \). This is a consequence of the fact that probabilities are calculated as integrals over intervals. The implications of \( f(X=x) = 0 \) are that the probability of any single, exact outcome in a continuous sample space is infinitesimally small. Intuitively, in a continuous probability space, the probability of a specific, exact point is effectively zero. The focus is on the probability of ranges or intervals, capturing the idea that events in a continuous space are best described in terms of intervals rather than individual points.



==========================


\chapter{Derivation}
\label{app:derivation}
Writing out the expectation value of equation \eqref{pp}
\begin{equation}
	\begin{split}
		\mathbb{E}[n|n',I] &= \sum_i n_ip(n_i|n',I)\\
		& = \sum_i n_i\int d\theta d\lambda  \frac{p(n'|n_i,\theta,\lambda,I)p(n_i|\lambda,\theta,I)p(\lambda|\theta,I)p(\theta|I)}{p(n'|I)}.\\
	\end{split}
\end{equation}
The pror distributions on $\theta,\lambda$ are assumed to be Beta and Gamma, respectively, meaning
\begin{equation}
	\begin{split}
		p(\theta|I)& = \text{Beta}(\theta|a,b)= \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)},\\
		p(\lambda|\theta,I) & = p(\lambda|I) = \text{Gam}(\lambda|\alpha,\beta)=\frac{\beta^\alpha\lambda^{\alpha-1}e^{-\lambda \beta}}{\Gamma(\alpha)},\\
		p(n_i|\lambda,\theta,I) &= \text{Poi}(n_i|\lambda)= \frac{e^{-\lambda}\lambda^{n_i}}{(n_i)!}.
	\end{split}
\end{equation}
Note that the likelihood for future $n_i$ counts does not appear like a typical likelihood ($\text{likelihood}\sim \prod_ip_i$). the reason for this is that the rate parameter is unique and independent between each time interval $m$. The likelihood term
\begin{equation}
	\begin{split}
		p(n'|n_i,\theta,\lambda,I) & = p(n'|\lambda,\theta,I)\\
		& = \sum_r p(n',w_r|\lambda,\theta,I)\\
		& = \sum_r \text{Bin}(n'|\theta,w_r) \text{Poi}(w_r|\lambda),\\
	\end{split}
\end{equation}
The expectation value can then be written
\begin{equation}
	\begin{split}
		\mathbb{E}[n|n',I] & = \sum_i n_i\int d\theta d\lambda  \frac{\sum_r \text{Bin}(n'|\theta,w_r) \text{Poi}(w_r|\lambda)\text{Poi}(n_i|\lambda)\text{Gam}(\lambda|\alpha,\beta)\text{Beta}(\theta|a,b)}{p(n'|I)}\\
		&=\frac{1}{p(n'|I)}\int d\theta d\lambda \lambda \text{Poi}(n'|\theta\lambda)\text{Gam}(\lambda|\alpha,\beta)\text{Beta}(\theta|a,b)\\
		&=\frac{1}{p(n'|I)}\int d\theta d\lambda \lambda \frac{(\theta\lambda)^{n'}e^{-\lambda\theta}}{n'!}\frac{\beta^\alpha\lambda^{\alpha-1}e^{-\lambda \beta}}{\Gamma(\alpha)}\frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}\\
		&=\frac{1}{p(n'|I)}\frac{\beta^\alpha}{\Gamma(\alpha)B(a,b)n'!}\int d\theta\theta^{a+n'-1}(1-\theta)^{b-1} \int d\lambda  \lambda^{\alpha+n'}e^{-\lambda (\beta+\theta)}\\
		&=\frac{1}{p(n'|I)}\frac{\beta^\alpha\Gamma(\alpha+n'+1)}{\Gamma(\alpha)B(a,b)n'!}\int d\theta\theta^{a+n'-1}(1-\theta)^{b-1} (\beta+\theta)^{-(\alpha+n'+1)}.\\
	\end{split}
\end{equation}
Similarly
\begin{equation}
	\begin{split}
		p(n'|I) &= \sum_i \int d\theta d\lambda  \sum_r \text{Bin}(n'|\theta,w_r) \text{Poi}(w_r|\lambda)\text{Poi}(n_i|\lambda)\text{Gam}(\lambda|\alpha,\beta)\text{Beta}(\theta|a,b)\\
		&=  \int d\theta d\lambda  \text{Poi}(n'|\lambda\theta)\text{Gam}(\lambda|\alpha,\beta)\text{Beta}(\theta|a,b)\\
		&=\int d\theta d\lambda  \frac{(\theta\lambda)^{n'}e^{-\lambda\theta}}{n'!}\frac{\beta^\alpha\lambda^{\alpha-1}e^{-\lambda \beta}}{\Gamma(\alpha)}\frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}\\
		&=\frac{\beta^\alpha}{B(a,b)\Gamma(\alpha)n'!}\int d\theta \theta^{a+n'-1}(1-\theta)^{b-1} \int d\lambda  \lambda^{\alpha+n'-1}e^{-\lambda (\beta+\theta)}\\
		&=\frac{\beta^\alpha\Gamma(\alpha+n')}{B(a,b)\Gamma(\alpha)n'!}\int d\theta \theta^{a+n'-1}(1-\theta)^{b-1} (\beta+\theta)^{-(\alpha+n')}\\
	\end{split}
\end{equation}
where it has been used that
\begin{equation}
	\begin{split}
		\sum_i\text{Poi}(n_i|\lambda)&=1,\\
		\sum_in_i\text{Poi}(n_i|\lambda)&=\lambda,\\
		\int_0^\infty d\lambda  \lambda^{\alpha+n'}e^{-\lambda (\beta+\theta)} &= (\beta+\theta)^{-(\alpha+n'+1)}\Gamma(\alpha+n'+1)\\
	\end{split}
\end{equation}
\begin{equation}
	\begin{split}
		\sum_r\text{Bin}(n'|\theta,w_r)\text{Poisson}(w_r|\lambda) &= \sum_{r=n'}^\infty \begin{bmatrix}
			r \\
			n' \\
		\end{bmatrix}
		\theta^{n'}(1-\theta)^{r-n'}\frac{\lambda^re^{-\lambda}}{r!}\\
		&=\frac{\theta^{n'}e^{-\lambda}}{n'!}\sum_{r=n'}^{\infty} \frac{\lambda^r	(1-\theta)^{r-n'}}{(r-n')!}\\
	\end{split}
\end{equation}
Let $q= r-n'$
\begin{equation}
	\begin{split}
		\sum_r\text{Bin}(n'|\theta,w_r)\text{Poi}(w_r|\lambda) &=\frac{\theta^{n'}e^{-\lambda}}{n'!}\sum_{q=0}^{\infty} \frac{\lambda^{q+n'}(1-\theta)^{q}}{q!}\\
		&=\frac{(\theta\lambda)^{n'}e^{-\lambda}}{n'!}\sum_{q=0}^{\infty} \frac{(\lambda(1-\theta))^{q}}{q!}\\
		&=\frac{(\theta\lambda)^{n'}e^{-\lambda}}{n'!}e^{\lambda(1-\theta)}\\
		&=\frac{(\theta\lambda)^{n'}e^{-\lambda\theta}}{n'!}\\
		&= \text{Poi}(n'|\lambda\theta)
	\end{split}
\end{equation}

Collecting the results
\begin{equation}
	\begin{split}
		\mathbb{E}[n|D,I] &=\frac{\frac{\beta^\alpha\Gamma(\alpha+n'+1)}{\Gamma(\alpha)B(a,b)n'!}\int d\theta\theta^{a+n'-1}(1-\theta)^{b-1} (\beta+\theta)^{-(\alpha+n'+1)}}{\frac{\beta^\alpha\Gamma(\alpha+n')}{B(a,b)\Gamma(\alpha)n'!}\int d\theta \theta^{a+n'-1}(1-\theta)^{b-1} (\beta+\theta)^{-(\alpha+n')}}\\
		&=\frac{\Gamma(\alpha+n'+1)\int d\theta\theta^{a+n'-1}(1-\theta)^{b-1} (\beta+\theta)^{-(\alpha+n'+1)}}{\Gamma(\alpha+n')\int d\theta \theta^{a+n'-1}(1-\theta)^{b-1} (\beta+\theta)^{-(\alpha+n')}}\\
		&=\frac{\Gamma(\alpha+n'+1)}{\beta\Gamma(\alpha+n')}\frac{ \,_2\tilde{F}_1(a+n',\alpha+n'+1,a+n'+b,-\beta^{-1})}{\,_2\tilde{F}_1(a+n',\alpha+n',a+n'+b,-\beta^{-1})}\\
		&=\frac{\alpha+n'}{\beta}\frac{ \,_2\tilde{F}_1(a+n',\alpha+n'+1,a+n'+b,-\beta^{-1})}{\,_2\tilde{F}_1(a+n',\alpha+n',a+n'+b,-\beta^{-1})}\\
	\end{split}
	\label{e3}
\end{equation}
The variance 
\begin{equation}
	\text{Var}[n|D,I] = \mathbb{E}[n^2|D,I]-\mathbb{E}[n|D,I]^2,
\end{equation}
with
\begin{equation}
	\begin{split}
		\mathbb{E}[n^2|D,I] &=\frac{\int d\theta d\lambda (\lambda+\lambda^2)\text{Poi}(n'|\theta\lambda)\text{Gam}(\lambda|\alpha,\beta)\text{Beta}(\theta|a,b)}{p(D|I)}\\
		&=\frac{\int d\theta d\lambda \lambda^2\text{Poi}(n'|\theta\lambda)\text{Gam}(\lambda|\alpha,\beta)\text{Beta}(\theta|a,b)}{p(D|I)}+\mathbb{E}[n|D,I]\\
		&=\frac{\beta^\alpha\int d\theta\theta^{a+n'-1}(1-\theta)^{b-1} \int d\lambda \lambda^{\alpha+n'+1}e^{-\lambda(\theta+\beta)} }{B(a,b)\Gamma(\alpha)n'!p(D|I)}+\mathbb{E}[n|D,I]\\
		&=\frac{\beta^\alpha\Gamma(\alpha+n'+2)\int d\theta\theta^{a+n'-1}(1-\theta)^{b-1} (\beta+\theta)^{-(\alpha+n'+2)} }{B(a,b)\Gamma(\alpha)n'!p(D|I)}+\mathbb{E}[n|D,I]\\
		&=\frac{\Gamma(\alpha+n'+2)}{\Gamma(\alpha+n')}\frac{\int d\theta\theta^{a+n'-1}(1-\theta)^{b-1} (\beta+\theta)^{-(\alpha+n'+2)} }{\,_2\tilde{F}_1(a+n',\alpha+n',a+n'+b,-\beta^{-1})}+\mathbb{E}[n|D,I]\\
		&=\frac{\Gamma(\alpha+n'+2)}{\beta^2\Gamma(\alpha+n')}\frac{\,_2\tilde{F}_1(a+n',\alpha+n'+2,a+n'+b,-\beta^{-1}) }{\,_2\tilde{F}_1(a+n',\alpha+n',a+n'+b,-\beta^{-1})}+\mathbb{E}[n|D,I]\\
	\end{split}
\end{equation}
where it has been used that
\begin{equation}
	\begin{split}
		\sum_in_i^2\text{Poi}(n_i|\lambda)&=\lambda+\lambda^2.\\
	\end{split}
\end{equation}

\section{Normal Distribution Derivation}
\begin{equation}
	\begin{split}
		\mathbb{E}[n|n',I] &= \int dn np(n|n',I)\\
		& = \int dnd\lambda n  \frac{p(n'|n,\lambda,I)p(n|\lambda,I)p(\lambda|I)}{p(n'|I)}.\\
	\end{split}
\end{equation}
The pror distribution on $\lambda$ is assumed to be Gamma, meaning
\begin{equation}
	\begin{split}
		p(\lambda|I) &= \text{Gam}(\lambda|\alpha,\beta)=\frac{\beta^\alpha\lambda^{\alpha-1}e^{-\lambda \beta}}{\Gamma(\alpha)},\\
		p(n|\lambda,I) &= \text{N}(n_i|\mu = \lambda,\sigma = \sqrt{\lambda})= \frac{e^{-\frac{1}{2}\big(\frac{n_i-\lambda}{\sqrt{\lambda}}\big)^2}}{\sqrt{2\pi\lambda}}.
	\end{split}
\end{equation}
The likelihood term
\begin{equation}
	\begin{split}
		p(n'|n,\lambda,I) & = p(n'|\lambda,I)\\
		& =\frac{e^{-\frac{1}{2}\big(\frac{n'-\lambda}{\sqrt{\lambda}}\big)^2}}{\sqrt{2\pi\lambda}}
	\end{split}
\end{equation}
Meaning
\begin{equation}
	\begin{split}
		\mathbb{E}[n|n',I] & = \frac{1}{p(n'|I)}\int dnd\lambda n  \text{N}(n'| \lambda,\sqrt{\lambda})\text{N}(n|\lambda, \sqrt{\lambda})\text{gam}(\lambda|\alpha,\beta).\\
	\end{split}
\end{equation}
\begin{equation}
	\begin{split}
		\int dn n \text{N}(n|\lambda, \sqrt{\lambda})  & =\mathbb{E}[n|\lambda]\\
		& = \lambda
	\end{split}
\end{equation}
\begin{equation}
	\begin{split}
		\mathbb{E}[n|n',I] & = \frac{1}{p(n'|I)}\int d\lambda  \lambda \text{N}(n'| \lambda,\sqrt{\lambda})\text{gam}(\lambda|\alpha,\beta)
	\end{split}
\end{equation}
In the limit of $\alpha,\beta \rightarrow 0$ this yields $\mathbb{E}[n|n',I] = n'$. For the variance
\begin{equation}
	\begin{split}
		\mathbb{E}[n^2|n',I] & = \frac{1}{p(n'|I)}\int dnd\lambda n^2  \text{N}(n'| \lambda,\sqrt{\lambda})\text{N}(n|\lambda, \sqrt{\lambda})\text{gam}(\lambda|\alpha,\beta).\\
	\end{split}
\end{equation}

\begin{equation}
	\begin{split}
		\int dn n^2 \text{N}(n|\lambda, \sqrt{\lambda})  & =\mathbb{E}[n^2|\lambda]\\
		& = \text{Var}[n|\lambda]+\mathbb{E}[n|\lambda]^2\\
		&= \lambda+\lambda^2
	\end{split}
\end{equation}
\begin{equation}
	\begin{split}
		\mathbb{E}[n^2|n',I] & = \frac{1}{p(n'|I)}\int d\lambda  \lambda^2 \text{N}(n'| \lambda,\sqrt{\lambda})\text{gam}(\lambda|\alpha,\beta)+\mathbb{E}[n|n',I]
	\end{split}
\end{equation}
In the limit of $\alpha,\beta \rightarrow 0$ this yields $\text{Var}[n|n',I]=2n'$.



\section{Normal Distribution Derivation Take 2}
\begin{equation}
	\begin{split}
		\mathbb{E}[n|n',I] &= \int dn np(n|n',I)\\
		& = \int dnd\lambda d\mu d\sigma n p(n',\lambda,\mu,\sigma|n,I)\\
		& = \int dnd\lambda d\mu d\sigma n p(n',\lambda,\mu,\sigma|n,I)\\
		& = \int dnd\lambda d\mu d\sigma n  \frac{p(n'|n,\lambda,\mu,\sigma,I)p(n|\lambda,\mu,\sigma,I)p(\lambda|\mu,\sigma,I)p(\xi|I)}{p(n'|I)}.\\
	\end{split}
\end{equation}
The pror distributions on $\xi,\lambda$ are assumed to be Gamma and normal, respectively, meaning
\begin{equation}
	\begin{split}
		p(\xi|I) &= \text{Gam}(\xi|\alpha,\beta)\\
		&=\frac{\beta^\alpha\xi^{\alpha-1}e^{-\xi \beta}}{\Gamma(\alpha)},\\
		p(\lambda|\xi,I) &= \text{N}(\lambda|\alpha,\beta)=\frac{\beta^\alpha\xi^{\alpha-1}e^{-\xi \beta}}{\Gamma(\alpha)},\\
		p(n|\lambda,\xi,I) &= \text{N}(n_i|\mu = \lambda,\sigma = \sqrt{\lambda})= \frac{e^{-\frac{1}{2}\big(\frac{n_i-\lambda}{\sqrt{\lambda}}\big)^2}}{\sqrt{2\pi\lambda}}.
	\end{split}
\end{equation}
The likelihood term
\begin{equation}
	\begin{split}
		p(n'|n,\lambda,I) & = p(n'|\lambda,I)\\
		& =\frac{e^{-\frac{1}{2}\big(\frac{n'-\lambda}{\sqrt{\lambda}}\big)^2}}{\sqrt{2\pi\lambda}}
	\end{split}
\end{equation}

\section{Negative Binomial Derivation}
Let $x=x_1+x_2+\dots x_M$ be the sum of future event counts and $y=\{y_1,y_2,y_3,\dots y_N\}$ be observed event counts. With this notation the expected number of future events, $x$, and the associated uncertainty can be written
\begin{equation}
	\mathbb{E}[x|y,I]\pm \sqrt{\text{Var}[x|y,I]},
\end{equation}
where
\begin{equation}
	\begin{split}
		\mathbb{E}[x|y,I] &= \sum_i x_ip(x_i|y,I),\\
		\text{Var}[x|y,I] &=  \sum_i x_i^2p(x_i|y,I)-\mathbb{E}[x|y,I]^2.
	\end{split}
	\label{h1}
\end{equation}
The distribution over counts can be expanded by marginalizing over a set of unknown parameters from underlying distributions. The details depend on the statistical assumptions. The most common assumption is to assume data follow a Poisson distribution with an unknown rate parameter which will then be marginalized over. An abvious choice of prior would be the conjugate gamma distribution, meaning
\begin{equation}
	\begin{split}
		p(x_i|y,I) &= \int d\lambda p(x_i,\lambda|y,I)\\
		&=\int d\lambda p(x_i|\lambda,y,I)p(\lambda|y,I)\\
		&= \int d\lambda p(x_i|\lambda,y,I)\frac{p(y|\lambda,I)p(\lambda|I)}{p(y|I)}.
	\end{split}
	\label{h4}
\end{equation}
Assuming each $x_j\in x$ is Poisson distributed
\begin{equation}
	\begin{split}
		p(x_i|\lambda,y,I) &= \text{Poi}(x_i|M\lambda),\\
		p(y|\lambda,I) &= \prod_{j=1}^N\text{Poi}(y_j|\lambda)\\
		&\propto \lambda^{N\bar{y}}e^{-N\lambda},\\
		p(\lambda|I) &= \text{Ga}(\lambda|\alpha,\beta).\\
	\end{split}
\end{equation}
With this selection $p(y|\lambda,I)p(\lambda|I)\sim \text{Ga}(\alpha+N\bar{y},\beta +N)$ and so
\begin{equation}
	\begin{split}
		\mathbb{E}[x|y,I] &= \sum_ix_i\int d\lambda \text{Poi}(x_i|M\lambda)\text{Ga}(\alpha+N\bar{y},\beta +N)\\
		&= M\int d\lambda\lambda\text{Ga}(\alpha+N\bar{y},\beta +N)\\
		&= M \frac{\alpha+N\bar{y}}{\beta +N},
	\end{split}
\end{equation}
where for the second equality it has been used that $\sum_ix_i \text{Poi}(x_i|M\lambda)=M\lambda$ and for the second equality it has been realized that the second line denoted the expectation of a Gamma distribution with modified parameters. Similarly for the variance
\begin{equation}
	\begin{split}
		\sum_i x_i^2p(x_i|y,I) &=  \sum_ix_i^2\int d\lambda \text{Poi}(x_i|M\lambda)\text{Ga}(\alpha+N\bar{y},\beta +N)\\
		&=M\int d\lambda (\lambda +M\lambda^2)\text{Ga}(\alpha+N\bar{y},\beta +N)\\
		&=\mathbb{E}[x|y,I]+M^2\int d\lambda \lambda^2\text{Ga}(\alpha+N\bar{y},\beta +N)\\
		&=\mathbb{E}[x|y,I]+M^2\frac{\alpha+N\bar{y}}{(\beta +N)^2}+\mathbb{E}[x|y,I]^2\\
	\end{split}
	\label{h2}
\end{equation}
where for the second equality it has been used that $\sum_ix_i^2\text{Poi}(x_i|M\lambda)=\text{Var}[x_i|M\lambda]+\mathbb{E}[x_i|M\lambda]^2=M\lambda +M^2\lambda^2$ and similarly for the fourth equality. Combining equations \eqref{h1} and \eqref{h2}, the variance can be written
\begin{equation}
	\begin{split}
		\text{Var}[x|y,I] & = \mathbb{E}[x|y,I]+M^2\frac{\alpha+N\bar{y}}{(\beta +N)^2}\\
		& = M\bigg(1+\frac{M}{\beta+N}\bigg)\frac{\alpha+N\bar{y}}{\beta +N}.\\
	\end{split}
\end{equation}
In the limit of $\beta,\alpha\rightarrow 0$ then
\begin{equation}
	\begin{split}
		\lim\limits_{\alpha,\beta\rightarrow 0}\big(\mathbb{E}[x|y,I]\pm \sqrt{\text{Var}[x|y,I]}\big) &\approx M\bar{y}\pm \sqrt{M(1+\frac{M}{N})\bar{y}}.\\ 
	\end{split}
	\label{h3}
\end{equation}
Equation \eqref{h3} informs that the expected number of future events is equal to the mean number of observed events per day multiplied with the number of days -- That makes sense. The variance of equation \eqref{h3} is proportional to the square root of the observed mean event count, which is to be expected from a Poisson distribution. It means $\mathbb{E}[x|y,I]\gg \sqrt{\text{Var}[x|y,I]}$ for $\bar{y}\gg 1$ and hence that the uncertainty approaches a negligible quantity. This is a strong implicit statement which easily can be broken. This can for example happen if the number of events is expected to vary due to a rate parameter that changes every day due to underlying dynamics, e.g. weather conditions. In this case it is intuitively clear that there will be a relatively large variation between observed counts from day to day -- due to a changing rate parameter. Consequently, the uncertainty of the predicted number of events should be relatively large, meaning intuitively it is clear that equation \eqref{h3} significantly underestimate the uncertainty. This underestimation of uncertainty stems from the fact that the Poisson distribution assume a tight correlation between the variance and mean of the distribution. This assumption can be broken by assuming data instead follow a Negative Binomial distribution, meaning
\begin{equation}
	\begin{split}
		p(x_j|\lambda,\phi) &= \int d\zeta  \text{Poi}(x_j|\zeta)\text{Ga}(\zeta|\phi,\frac{\lambda}{\phi})\\
		&=\text{NB}(x_j|\phi,\frac{\lambda}{\lambda+\phi})
	\end{split}
\end{equation}
where $x_j\subset x$ is one of the variables in $x$. This means equation \eqref{h4} can be rewritten viz
\begin{equation}
	\begin{split}
		p(x_i|y,I) &= \int d\lambda d\phi p(x_i,\phi,\lambda|y,I)\\
		&=\int d\lambda d\phi p(x_i|\lambda,\phi,y,I)p(\lambda,\phi|y,I)\\
		&= \int d\lambda d\phi p(x_i|\lambda,y,I)\frac{p(y|\lambda,\phi,I)p(\lambda|\phi,I)p(\phi|I)}{p(y|I)}.
	\end{split}
	\label{h5}
\end{equation}
The sum of Negative-binomial random variables is another negative-binomial random variable on the form
\begin{equation}
	p(x_i|\lambda,\phi,y,I) = \text{NB}(x_i|M\phi,\frac{\lambda}{\lambda+\phi}).
\end{equation}
The priors $p(\lambda|\phi,I), p(\phi|I)$ could both be assigned gamma distributions with specified parameters with, subsequently to the computation of the integrals, the limit of these going to zero being considered. However, the Negative-Binomial likelihood function in combination with any realistic priors yields an intractable integral without an analytical solution. To remedy this the Negative-Binomial distribution can, according to the central limit theorem, be approximated to normal for a large number of counts, meaning
\begin{equation}
	\begin{split}
		\lim\limits_{x_j\rightarrow \infty}(\text{NB}(x_j|\phi,\frac{\lambda}{\lambda+\phi})) &=N(x_j|\lambda,\lambda+\frac{\lambda^2}{\phi})\\
		&\approx N(x_j|\mu = \lambda,\sigma^2=\frac{1}{\tau}),
	\end{split}
\end{equation}
where for the second equality it has been used that for unknown $\lambda,\phi$ the variance can be decoupled from the mean. With this decoupling assumed, equation \eqref{h5} can instead be written
\begin{equation}
	p(x_i|y,I) \approx \int d\lambda d\tau p(x_i|\lambda,\tau,y,I)\frac{p(y|\lambda,\tau,I)p(\lambda|\tau,I)p(\tau|I)}{p(y|I)}.
	\label{h6}
\end{equation}
According to the principle of maximum entropy, the Normal and Gamma distribution best represent an unknown mean ($\lambda$) and precision ($\tau$) parameter, respectively, meaning the priors can be written
\begin{equation}
	\begin{split}
		p(\lambda|\tau,I) &=N(\lambda|\mu=\lambda_0,\sigma^2=\frac{1}{c\tau}),\\
		p(\tau|I)&= \text{Ga}(\tau|\alpha,\beta).
	\end{split}
\end{equation}
Using that $p(x_i|\lambda,\tau,y,I)=N(x_i|M\lambda,M\tau^{-1})$ and
\begin{equation}
	\begin{split}
		p(y|\lambda,\tau,I) &= \bigg(\frac{\tau}{2\pi}\bigg)^{\frac{N}{2}}\prod_{j=1}^{N}e^{-\frac{\tau}{2}(\lambda-y_j)^2}\\
		&=\bigg(\frac{\tau}{2\pi}\bigg)^{\frac{N}{2}}e^{-\frac{\tau}{2}\sum_{j=1}^{N}(\lambda-y_j)^2}\\
	\end{split}
\end{equation}
the expectation can be written
\begin{equation}
	\begin{split}
		\mathbb{E}[x|y,I] &\approx\frac{M\beta^{\alpha}}{p(y|I)\Gamma(\alpha)}\int d\lambda d\tau \lambda \bigg(\frac{\tau}{2\pi}\bigg)^{\frac{N}{2}}e^{-\frac{\tau}{2}\sum_{j=1}^{N}(\lambda-y_j)^2}\sqrt{\frac{c\tau}{2\pi}}e^{-\frac{c\tau}{2}(\lambda-\lambda_0)^2}\tau^{\alpha-1}e^{-\tau \beta}\\
		&=M\frac{Z_1}{Z_0},\\
	\end{split}
	\label{h11}
\end{equation}
with
\begin{equation}
	\begin{split}
		Z_1&\equiv \int d\tau \tau^{\alpha-\frac{1}{2}+\frac{N}{2}}e^{-\tau \beta} \int d\lambda \lambda e^{-\frac{\tau}{2}\sum_{j=1}^{N}(\lambda-y_j)^2-\frac{c\tau}{2}(\lambda-\lambda_0)^2},\\
		Z_0 &\equiv\int d\tau \tau^{\alpha-\frac{1}{2}+\frac{N}{2}}e^{-\tau \beta} \int d\lambda e^{-\frac{\tau}{2}\sum_{j=1}^{N}(\lambda-y_j)^2-\frac{c\tau}{2}(\lambda-\lambda_0)^2}.
	\end{split}
	\label{h9}
\end{equation}
For the identification of $Z_1$ and $Z_0$ it has been used that all constants cancel out due to $p(y|I)$.	The exponents involving $\lambda$ can be rewritten 
\begin{equation}
	\begin{split}
		-\frac{\tau}{2}\sum_{j=1}^{N}(\lambda-y_j)^2-\frac{c\tau}{2}(\lambda-\lambda_0)^2 &= -\frac{\tau}{2}\sum_{j=1}^{N}(\lambda-\bar{y}+\bar{y}-y_j)^2-\frac{c\tau}{2}(\lambda-\lambda_0)^2\\
		&=-\frac{\tau}{2}\sum_{j=1}^{N}(\bar{y}-y_j)^2-\frac{N\tau(\lambda-\bar{y})^2}{2}-\tau(\lambda-\bar{y})\sum_{j=1}^{N}(\bar{y}-y_j)-\frac{c\tau}{2}(\lambda-\lambda_0)^2\\
		&=-\frac{\tau}{2}\sum_{j=1}^{N}(\bar{y}-y_j)^2-\frac{N\tau(\lambda-\bar{y})^2}{2}-\frac{c\tau}{2}(\lambda-\lambda_0)^2\\
		&=-\frac{\tau}{2}\sum_{j=1}^{N}(\bar{y}-y_j)^2-\frac{\tau}{2}(N+c)\bigg[\bigg(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2 +\frac{cN\tau(\bar{y}-\lambda_0)^2}{(c+N)^2}\bigg].\\
	\end{split}
	\label{h7}
\end{equation}
Only the second term in equation \eqref{h7} depend on $\lambda$, so
\begin{equation}
	e^{-\beta\tau}e^{-\frac{\tau}{2}\sum_{j=1}^{N}(\lambda-y_j)^2-\frac{c\tau}{2}(\lambda-\lambda_0)^2} = e^{-\beta_0\tau}e^{-\frac{\tau}{2}(N+c)(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2},
	\label{h8}
\end{equation}	
with 
\begin{equation}
	\beta_0\equiv \beta+\frac{1}{2}\sum_{j=1}^{N}(\bar{y}-y_j)^2+\frac{cN(\bar{y}-\lambda_0)^2}{2(c+N)}.
\end{equation}
Using equation \eqref{h8} equation \eqref{h9} can be written
\begin{equation}
	\begin{split}
		Z_1 &= \int d\tau \tau^{\alpha_0-\frac{1}{2}}e^{-\tau\beta_0} \int d\lambda \lambda e^{-\frac{\tau}{2}(N+c)(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2}\\
		&=\sqrt{\frac{2\pi}{N+c}}\frac{c\lambda_0+N\bar{y}}{N+c}\beta_0^{-\alpha_0}\Gamma(\alpha_0), \\ 
		Z_0 &= \int d\tau \tau^{\alpha_0-\frac{1}{2}}e^{-\tau\beta_0} \int d\lambda e^{-\frac{\tau}{2}(N+c)(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2}\\
		&=\sqrt{\frac{2\pi}{N+c}}\beta_0^{-\alpha_0}\Gamma(\alpha_0).
	\end{split}
	\label{h10}
\end{equation} 
Combining equation \eqref{h11} with \eqref{h10} yields
\begin{equation}
	\mathbb{E}[x|y,I]\approx M\frac{c\lambda_0+N\bar{y}}{c+N}
\end{equation}
For the variance
\begin{equation}
	\begin{split}
		\mathbb{E}[x^2|y,I] &\approx \sum_i x_i^2\int d\lambda d\tau p(x_i|\lambda,\tau,y,I)\frac{p(y|\lambda,\tau,I)p(\lambda|\tau,I)p(\phi|I)}{p(y|I)}\\
		&=\frac{M}{p(y|I)}\int d\lambda d\tau (M\lambda^2+\tau^{-1}) p(y|\lambda,\tau,I)p(\lambda|\tau,I)p(\phi|I)\\
		&=M\frac{MZ_2+Z_3}{Z_0}
	\end{split}
	\label{h12}
\end{equation}
where
\begin{equation}
	\begin{split}
		Z_2 &= \int d\tau \tau^{\alpha_0-\frac{1}{2}}e^{-\tau\beta_0} \int d\lambda \lambda^2 e^{-\frac{\tau}{2}(N+c)(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2}\\
		& = \sqrt{\frac{2\pi}{(N+c)}}\bigg[\frac{1}{N+c}\beta_0^{1-\alpha_0}\Gamma(\alpha_0-1)+\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2\beta_0^{-\alpha_0}\Gamma(\alpha_0) \bigg],\\
		Z_3 &= \int d\tau \tau^{\alpha_0-\frac{3}{2}}e^{-\tau\beta_0} \int d\lambda  e^{-\frac{\tau}{2}(N+c)(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2}\\
		&=\sqrt{\frac{2\pi}{(N+c)}}\beta_0^{1-\alpha_0}\Gamma(\alpha_0-1)
	\end{split}
	\label{h13}
\end{equation}
Combining equations \eqref{h1}, \eqref{h12} and \eqref{h13}
\begin{equation}
	\text{Var}[x|y,I]=M\bigg(\frac{M}{N+c}+1\bigg)\frac{\beta_0}{\alpha_0-1}
\end{equation}
In the limit of $\alpha,\beta,c\rightarrow 0$ then
\begin{equation}
	\begin{split}
		\lim\limits_{\alpha,\beta,c\rightarrow 0}\big(\mathbb{E}[x|y,I]\pm \sqrt{\text{Var}[x|y,I]}\big) &\approx M\bar{y}\pm \sqrt{\frac{M(N-1)}{N-2}\bigg(\frac{M}{N}+1\bigg)\frac{\sum_{j=1}^{N}(\bar{y}-y_j)^2}{N-1}}\\ 
	\end{split}
	\label{h14}
\end{equation}
Comparing equations \eqref{h3} and \eqref{h14} it is clear that the expectations are identical but the variances are different -- As designed. The variance of equation \eqref{h14} depends now on the sample variance of observed event counts meaning the uncertainty of equation \eqref{h14} scale with the variation of observed counts, stemming from underlying dynamics in e.g. the rate parameter. In the case of events affected by weather phenomena, equation \eqref{h14} is expected to be a better approximation.


\chapter{trashed}

Collecting the results, equation \eqref{e1a} (yielding the probability a new data point belonging to the $k$'th class) can be written on the form
\begin{equation}
	\begin{split}
		p(y^{(n+1)}_k|x^{(n+1)},\mathcal{D},I) &= \int p(y^{(n+1)}_k,\theta|x^{(n+1)},\mathcal{D},I)d\theta d\lambda\\
	\end{split},
	\label{e2a}
\end{equation}
where $N$ is the number of samples from $p(\theta|x^{(n+1)},\mathcal{D},I)$ used in the sum.



===================


can be approximated by the output of a function with a probabilistic output (e.g. a classic feedforward ANN) on the form~\citep{Saerens2002}
\begin{equation}
	p(y_k^{(n+1)}|x^{(n+1)},\theta,I)\approx f_k(x^{(n+1)},\theta).
	\label{out}
\end{equation}


\begin{equation}
	\approx  \lim\limits_{N\rightarrow \infty}\bigg(\frac{1}{N}\sum_{q\in \tilde{p}_2}f_k(x^{(n+1)},\theta_q)\bigg)
\end{equation}






Equation \eqref{e2a} can be evaluated by using the second equality and obtaining the samples frfom $p(\theta|x^{(n+1)},\mathcal{D},I)$ via HMC -- Similarly to the case of Bayesian regression. 


======================================

According to equation \eqref{h10} it is clear that the integrals over $\lambda$ are unnormalized Gaussian integrals with normalization factor $\sqrt{\frac{2\pi}{\tau(N+c)}}$. Hereby





Define
\begin{equation}
	\begin{split}
		Z_1 &= \int d\tau \tau^{\alpha-\frac{1}{2}+\frac{N}{2}} \int d\lambda \lambda e^{-\frac{\tau}{2}\sum_{j=1}^{N}(\lambda-y_j)^2-\frac{c\tau}{2}(\lambda-\lambda_0)^2-\tau \beta}\\
		&= \int d\tau \tau^{\alpha-\frac{1}{2}+\frac{N}{2}}e^{-\tau(\beta+\frac{1}{2}\sum_{j=1}^{N}(\bar{y}-y_j)^2)} \int d\lambda \lambda e^{-\frac{N\tau(\lambda-\bar{y})^2}{2}-\frac{c\tau}{2}(\lambda-\lambda_0)^2}
	\end{split}
\end{equation}
Now
\begin{equation}
	\begin{split}
		-\frac{N\tau(\lambda-\bar{y})^2}{2}-\frac{c\tau}{2}(\lambda-\lambda_0)^2 &= -\frac{N\tau\lambda^2}{2}-\frac{N\tau\bar{y}^2}{2}+N\tau\lambda\bar{y}-\frac{c\tau\lambda^2}{2}-\frac{c\tau\lambda_0^2}{2}+c\tau\lambda\lambda_0\\
		&=\tau\lambda(c\lambda_0+N\bar{y}) -\frac{\lambda^2\tau}{2}(N+c)-\frac{N\tau\bar{y}^2}{2}-\frac{c\tau\lambda_0^2}{2}\\
		&=-\frac{\tau}{2}(N+c)\bigg[ \lambda^2-2\lambda\frac{c\lambda_0+N\bar{y}}{N+c}+\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2-\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2+\frac{N\bar{y}^2+c\lambda_0^2}{N+c}\bigg]\\
		&=-\frac{\tau}{2}(N+c)\bigg[\bigg(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2 -\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2+\frac{N\bar{y}^2+c\lambda_0^2}{N+c}\bigg]\\
		&=-\frac{\tau}{2}(N+c)\bigg[\bigg(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2 +\frac{cN\tau(\bar{y}-\lambda_0)^2}{(c+N)^2}\bigg]\\
	\end{split}
\end{equation}
\begin{equation}
	\begin{split}
		Z_1 &= \int d\tau \tau^{\alpha_0-\frac{1}{2}}e^{-\tau\beta_0} \int d\lambda \lambda e^{-\frac{\tau}{2}(N+c)(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2}
	\end{split}
\end{equation}
with
\begin{equation}
	\begin{split}
		\alpha_0&\equiv \alpha+\frac{N}{2},\\
		\beta_0&\equiv \beta+\frac{1}{2}\sum_{j=1}^{N}(\bar{y}-y_j)^2+\frac{cN(\bar{y}-\lambda_0)^2}{2(c+N)}
	\end{split}
\end{equation}
Now 
\begin{equation}
	\int d\lambda \lambda e^{-\frac{\tau}{2}(N+c)[(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2 ]} = \sqrt{\frac{2\pi}{\tau(N+c)}}\frac{c\lambda_0+N\bar{y}}{N+c}
\end{equation}
\begin{equation}
	\begin{split}
		Z_1 &=\sqrt{\frac{2\pi}{N+c}}\frac{c\lambda_0+N\bar{y}}{N+c} \int d\tau \tau^{\alpha_0-1}e^{-\tau \beta_0}\\
		&=\sqrt{\frac{2\pi}{N+c}}\frac{c\lambda_0+N\bar{y}}{N+c}\beta_0^{-\alpha_0}\Gamma(\alpha_0) \\  
	\end{split}
\end{equation}
Similarly
\begin{equation}
	\begin{split}
		p(y|I)&=\frac{\beta^{\alpha}\sqrt{c}}{\Gamma(\alpha)(2\pi)^{\frac{1}{2}(1-N)}}Z_0
	\end{split}
\end{equation}
with
\begin{equation}
	Z_0\equiv \int d\tau \tau^{\alpha_0-\frac{1}{2}}e^{-\tau\beta_0} \int d\lambda  e^{-\frac{\tau}{2}(N+c)(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2}.
\end{equation}
Then
\begin{equation}
	\mathbb{E}[x|y,I]\approx M\frac{Z_1}{Z_0}
\end{equation}
where the approximation is due to the normal approximation, not the representation -- which is exact (hence, no further approximations). Now 
\begin{equation}
	\int d\lambda e^{-\frac{\tau}{2}(N+c)[(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2 ]} = \sqrt{\frac{2\pi}{\tau(N+c)}}
\end{equation}
meaning
\begin{equation}
	\begin{split}
		Z_0&=\sqrt{\frac{2\pi}{N+c}} \int d\tau \tau^{\alpha_0-1}e^{-\tau(\beta+\frac{1}{2}\sum_{j=1}^{N}(\bar{y}-y_j)^2)}\\
		&=\sqrt{\frac{2\pi}{N+c}}\beta_0^{-\alpha_0}\Gamma(\alpha_0)
	\end{split}
\end{equation}
\begin{equation}
	\mathbb{E}[x|y,I]\approx M\frac{c\lambda_0+N\bar{y}}{c+N}
\end{equation}



=====================


Now
\begin{equation}
	\int d\lambda \lambda^2 e^{-\frac{\tau}{2}(N+c)(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2}=\bigg(\frac{1}{\tau(N+c)}+\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2\bigg)\sqrt{\frac{2\pi}{\tau(N+c)}}
\end{equation}	
So
\begin{equation}
	\begin{split}
		Z_2 & = \sqrt{\frac{2\pi}{(N+c)}}\int d\tau \tau^{\alpha_0-1}e^{-\tau\beta_0} \bigg(\frac{1}{\tau(N+c)}+\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2\bigg)\\
		& = \sqrt{\frac{2\pi}{(N+c)}}\bigg[\frac{1}{N+c}\int d\tau \tau^{\alpha_0-2}e^{-\tau\beta_0}+\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2\int d\tau \tau^{\alpha_0-1}e^{-\tau\beta_0}\bigg]\\
		& = \sqrt{\frac{2\pi}{(N+c)}}\bigg[\frac{1}{N+c}\beta_0^{1-\alpha_0}\Gamma(\alpha_0-1)+\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2\beta_0^{-\alpha_0}\Gamma(\alpha_0) \bigg]\\
	\end{split}
\end{equation}
\begin{equation}
	\begin{split}
		Z_3 &= \int d\tau \tau^{\alpha_0-\frac{3}{2}}e^{-\tau\beta_0} \int d\lambda  e^{-\frac{\tau}{2}(N+c)(\lambda-\frac{c\lambda_0+N\bar{y}}{N+c})^2}\\
		&=\sqrt{\frac{2\pi}{(N+c)}}\int d\tau \tau^{\alpha_0-2}e^{-\tau\beta_0}\\
		&=\sqrt{\frac{2\pi}{(N+c)}}\beta_0^{1-\alpha_0}\Gamma(\alpha_0-1)
	\end{split}
\end{equation}






\begin{equation}
	\begin{split}
		MZ_2+Z_3&=M\sqrt{\frac{2\pi}{(N+c)}}\bigg[\frac{1}{N+c}\beta_0^{1-\alpha_0}\Gamma(\alpha_0-1)+\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2\beta_0^{-\alpha_0}\Gamma(\alpha_0) \bigg]+\sqrt{\frac{2\pi}{(N+c)}}\beta_0^{1-\alpha_0}\Gamma(\alpha_0-1)\\
		&=\sqrt{\frac{2\pi}{(N+c)}}\beta_0^{1-\alpha_0}\Gamma(\alpha_0-1)\bigg(\frac{M}{N+c}+1\bigg)+M\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2Z_0 \\
		&=\bigg(\frac{M}{N+c}+1\bigg)\frac{\beta_0}{\alpha_0-1}Z_0+M\bigg(\frac{c\lambda_0+N\bar{y}}{N+c}\bigg)^2Z_0 \\
	\end{split}
\end{equation}



===================


\begin{example}
	Let $\{t,j,k,g\}$ be the sufficient statistics for denote an event of class $k\in K$ occuring at time $t$, in room $j\in J$ in geographical region $g\in G$. Let $\tilde{n}_{j,k,m,g}$ denote the number events counted in discrete time intervals $m$ of duration $\Delta t$. The event count relevant for this study will be the sum of events over discrete time intervals, $m$, and rooms, $j$, meaning 
	\begin{equation}
		n_{k,q,g} = \sum_{j,m\in T_q}\tilde{n}_{j,k,m,g},
	\end{equation} 
	where $T_q$ is the $q$'th time period with a generic duration, e.g. $\Delta t=$ 1 hour and $T_q=1$ day $\forall q$. The mean and variance of the posterior predictive distribution for $n_{k,q,g}$ describe the best guess, given data, of the expected value of counts and the associated uncertainty for a repeat of the process behind the data. Denoting the number of events measured over time period $q$ in geographical region, $g$, for any class of events, $k$, by $n'_{k,q,g}$, the expected value and associated uncertainty can be written~\citep{Sivia2006,murphy2013}
	\begin{equation}
		\mathbb{E}[n_{k,q,g}|n'_{k,q,g},I]\pm \sqrt{\text{Var}[n_{k,q,g}|n'_{k,q,g},I]}.
		\label{pp}
	\end{equation}	
	Equation \eqref{pp} depend on the satistical distribution of $n$, which in general vary across the event classes, $k$. For this study the event classes are separated into $2$ subcategories
	\begin{enumerate}
		\item $s_1\subset K$: Events that happen randomly during each discrete time interval $\Delta t$ (manual events). In this case the number, $\tilde{n}_{j,k\in s_1,m,g}$, of events in $s_1\subset K$ over time $\Delta t$ will be assumed to follow a Poisson distribution with rate parameter $\tilde{\lambda}$, meaning
		\begin{equation}
			\tilde{n}_{j,k\in s_1,m,g} \sim \text{Poi}(\tilde{n}_{j,k\in s_1,m,g}|\tilde{\lambda}_{j,k\in s_1,m,g}).
		\end{equation}
		Since the sum of Poisson random variables is another Poisson random variable
		\begin{equation}
			n_{k\in s_1,q,g} \sim \text{Poi}(n_{k\in s_1,q,g}|\lambda_{k\in s_1,q,g})
		\end{equation}
		where $\lambda_{k\in s_1,q,g} \equiv \sum_{j,m\in T_q}\tilde{\lambda}_{j,k\in s_1,m,g}$.
		
		\item $s_2\subset K$: Events that happen with a binary probability for each discrete time interval $\Delta t$ (algorithmic events). In this case the number, $\tilde{n}_{j,k\in s_2,m,g}$, of events in $s_2\subset K$ over time $\Delta t$ will be assumed to follow a Bernoulli distribution with probability $\phi_{j,k\in s_2,m,g}$ of an event. Let $M = len(T_q)$ (the training data cover $M$ trials and we consider $M$ new trials) and $W=len(J)$, then
		\begin{equation}
			n_{k\in s_2,q,g} \sim \text{Poi-Bern}(n_{k\in s_2,q,g}|\{\phi\},W,M,I)\approx \text{Poi}(n_{k\in s_2,q,g}|\phi_{k\in s_2,q,g}),
		\end{equation}
		where Le Cams theorem has been used for the approximation and $\phi_{k\in s_2,q,g}\equiv \sum_{j,m}\phi_{j,k\in s_2,m,g}$. Note that since $\phi_{j,k\in s_2,m,g}$ are probabilities, $0\leq \phi_{k\in s_2,q,g}\leq WM$. For this study $WM$ will be a large number, so $0\leq \phi_{k\in s_2,q,g}\lesssim \infty$ and by replacing $\phi\rightleftarrows \lambda$, the analysis for events in $s_2$ becom identical to that of events in $s_1$. 
	\end{enumerate}
	
	Since the event count for both classes of events can be approximated to be identically distributed, the event count for time duration, $T_q$ and geographical region, $g$, can be modeled viz
	\begin{equation}
		n_{k,q,g} \sim \text{Poi}(n_{k,q,g}|\lambda_{k,q,g}),
	\end{equation}
	Leading to (omitting indices for ease of notation, see appendix \ref{app:derivation} for details)
	\begin{equation}
		\begin{split}
			\mathbb{E}[n|n',I] &=\frac{\alpha+n'}{\beta}\frac{ K_1(a,b,\alpha,\beta,n')}{K_0(a,b,\alpha,\beta,n')}, \\
			\text{Var}[n|n',I]&=\frac{(\alpha+n'+1)(\alpha+n')}{\beta^2}\frac{K_2(a,b,\alpha,\beta,n') }{K_0(a,b,\alpha,\beta,n')}+\mathbb{E}[n|n',I]-\mathbb{E}[n|n',I]^2,
		\end{split}
	\end{equation}
	where
	\begin{equation}
		\begin{split}
			K_0(a,b,\alpha,\beta,n') &\equiv \,_2\tilde{F}_1(a+n',\alpha+n',a+n'+b,-\beta^{-1})\\
			K_1(a,b,\alpha,\beta,n') &\equiv  \,_2\tilde{F}_1(a+n',\alpha+n'+1,a+n'+b,-\beta^{-1})\\
			K_2(a,b,\alpha,\beta,n') &\equiv \,_2\tilde{F}_1(a+n',\alpha+n'+2,a+n'+b,-\beta^{-1})\\
		\end{split}
	\end{equation}
	and $a,b$ are parameters for a Beta prior distribution describing the probability of an event being registered (i.e. the complementary to an event being lost due to a data pipeline failure) and $\alpha, \beta$ are parameters for a Gamma prior distribution describing the distribution of the rate parameter, $\lambda$. Taking $a>b, b=2$ describes a pipeline that is neither perfect nor useless. The limit of $a\rightarrow \infty, \beta \lesssim 2$ corresponds to an almost perfect pipeline, in which case
	\begin{equation}
		\begin{split}
			\lim\limits_{a\rightarrow \infty}\bigg(\frac{K_1(a,b,\alpha,\beta,n')}{K_0(a,b,\alpha,\beta,n')}\bigg)\bigg|_{b\lesssim 2}&\approx \frac{\beta}{\beta+1},\\
			\lim\limits_{a\rightarrow \infty}\bigg(\frac{K_2(a,b,\alpha,\beta,n')}{K_0(a,b,\alpha,\beta,n')}\bigg)\bigg|_{b\lesssim 2}&\approx \bigg(\frac{\beta}{\beta+1}\bigg)^2.\\
		\end{split}
	\end{equation}
	Letting $\alpha,\beta \rightarrow 0$ constitutes a vague prior, so for a pipeline approaching perfect data quality
	\begin{equation}
		\begin{split}
			\bigg(\mathbb{E}[n_{k,q,g}|n'_{k,q,g},I]\pm \sqrt{\text{Var}[n_{k,q,g}|n'_{k,q,g},I]}\bigg)\bigg|_{\sim \text{perfect data pipeline}}&\approx n'_{k,q,g}\pm \sqrt{2n'_{k,q,g}},
		\end{split}
		\label{pp2}
	\end{equation}	
	meaning the expected value is equal to the observed count and the uncertainty is proportional to the square root of the count. For an imperfect data pipeline, the effect of $K_{0,1,2}$ must be included. Given $K_{0,1,2}$ are numerically unstable for $n\gtrsim 50$ and theoretically impenetrable, further analysis is beyond the scope of this study.
\end{example}

