\section{Frequentist Statistics}
\label{chp:freq}
The Frequentist paradigm (\dfref{def:frequentist_statistics}) trace back to seminal works such as those of Neyman and Pearson \citep{Neyman1928OnSR} and Fisher \citep{fisher1925statistical}, who laid the groundwork for much of its methodology. Subsequent developments by Wald \citep{Wald1945Sequential}, Neyman \citep{Neyman1948Consistent}, and Lehmann \citep{lehmann1986testing} further refined its theories and techniques.\newline
In the Frequentist paradigm, it is assumed that Nature's decisions can be captured by a statistical model with fixed, unknown parameters and an objective probability measure. In this setting, the optimal decision rule can be expressed as
\begin{equation}
	U^*(x,w).
\end{equation}
Since $w$ is not known to the Robot, the central task becomes to estimate $w$ from past data $D$. This gives rise to a nested decision problem with two levels:
\begin{enumerate}
	\item[\textit{i)}] Parameter estimation: use past data $D$ to construct an estimator $\hat{w}(D)$ of the fixed but unknown parameters $w$.
	\item[\textit{ii)}] Prediction/decision: given a new observation $x$ and the parameter estimate $\hat{w}(D)$, apply the decision rule $U$ to determine an action.
\end{enumerate}
The practical decision rule for a new observation $x_{n+1} \in \Omega_X$ therefore takes the form\footnote{\EQref{eq:decision_rule_frequentist} justifies the unified notation $U(\tilde{D})$ for the decision rule in \dfref{def:statistical_game} and \rmref{rem:frequentist_bayesian_expected_cost}.}
\begin{equation}
	U^*(x_{n+1}, \hat{w}^*(D)),
	\label{eq:decision_rule_frequentist}
\end{equation}
where $\hat{w}^*(D)$ denotes the optimal parameter decision rule, obtained from past data $D$, and the action $U^*$ is determined by minimizing the expected cost as specified in \dfref{def:statistical_game} and \rmref{rem:frequentist_bayesian_expected_cost}.


\subsection{Frequentist Regression}
\label{chp:frequentist_regression}
Regression involves the Robot constructing a model,
\begin{equation}
	f\colon \Omega_\Theta \times \Omega_X \to \Omega_Y,
\end{equation}
with associated parameters $\theta \in \Omega_\Theta\subseteq \Omega_W$, that estimates Nature's actions $y_{n+1}\in \Omega_Y$ based on observed data $x_{n+1}\in \Omega_X=\mathbb{R}$. The model $f$ acts as a proxy for the Robot in that it on behalf of the Robot estimates the action of Nature given an input. Hence, in providing an estimate, the model must make a choice, similar to the Robot and thus the Robot must pick a cost function for the model. In this study, the quadratic cost function from \dfref{def:quadratic_cost} will be considered to review the subject.


Assuming the actions of Nature follow a normal distribution with the function $f$ as mean and an unknown precision $\xi \in \Omega_{W}$
\begin{equation}
	p(y_{n+1}\mid x_{n+1},w)=\sqrt{\frac{\xi}{2\pi}} e^{-\frac{\xi}{2}(f(\theta,x_{n+1})-y_{n+1})^2},
	\label{freq:dist}
\end{equation}
where $w = (\theta, \xi,\dots)$ denotes the collection of fixed, unknown parameters. Under the quadratic cost function from \dfref{def:quadratic_cost}, the optimal decision rule is the conditional expectation of $Y_{n+1}$ given $(x_{n+1},w)$ (\thref{theorem:expectation_decision_rule}),
\begin{equation}
	\begin{split}
		U^*(x_{n+1},\hat{w}^*(D)) &= \mathbb{E}[Y_{n+1} \mid x_{n+1}, \hat{w}^*(D)]\\
		&= \int y_{n+1} p(y_{n+1} \mid x_{n+1}, \hat{w}^*(D)) \mathrm{d}y_{n+1}\\
		& = f(\hat{\theta}^*(D),x_{n+1}).
	\end{split}
	\label{freq:decision}
\end{equation}
\EQref{freq:decision} represents the Frequentist optimal decision rule, defined conditional on an estimate of the model parameters. This can be directly contrasted with \EQref{eq:q3}, which expresses the Bayesian optimal decision rule obtained by averaging over the posterior distribution of the model parameters (and latent variables) given the observed data. From \EQref{freq:decision}, it follows that within the Frequentist paradigm, regression becomes a problem of parameter estimation.

\subsection{Frequentist Classification}
\label{chp:frequentist_classification}
The setup for Frequentist classification initially mirrors that of Bayesian classification (\secref{chp:baycl}), but begins to diverge at the level of the cost function,
\begin{equation}
	C(U(x_{n+1},w), y_{n+1}) = 1 - \delta_{U(x_{n+1},w),y_{n+1}} + (\psi-1)\delta_{U(x_{n+1},w), \operatorname{reject}}.
\end{equation}
Let $\hat{w}^*(D)$ denote the optimal Frequentist estimator of the model parameters obtained from past data $D$. The optimal decision rule for a new observation $x_{n+1}$ is (\thref{theorem:opt_decision_rule} and \EQref{eq:conditional_cost_discrete})
\begin{equation}
	\begin{split}
		U^*(x_{n+1}, \hat{w}^*(D)) &= \argmin_{U(\tilde{D})} \mathbb{E}[C(U(\tilde{D}), Y_{n+1}) \mid x_{n+1}, \hat{w}(D)] \\
		&= \argmin_{U(\tilde{D})} \sum_{y_{n+1}\in \Omega_Y}C(U(\tilde{D}),y_{n+1})p(y_{n+1}\mid x_{n+1}, \hat{w}(D)) \\
		&= \argmin_{U(\tilde{D})} \Big(1 - f_{U(\tilde{D})}(\hat{\theta}^*(D), x_{n+1}) + (\psi-1)\delta_{U(\tilde{D}), \operatorname{reject}}\Big).
	\end{split}
	\label{freq:decision_classification}
\end{equation}
From \EQref{freq:decision_classification}, it follows that within the Frequentist paradigm, classification reduces to a problem of parameter estimation.


\begin{remark}[Bayesian versus Frequentist regression and classification]
	Contrasting \EQref{freq:decision} and \EQref{eq:q3} for regression and \EQref{freq:decision_classification} and \EQref{eq:expected_cost1} for classification, the mathematical difference in relation to prediction between the two paradigms can be written as shown in Table \ref{tab:comare}.
	\begin{table}[h!]
		\centering
		\caption{Comparison between Frequentist and Bayesian prediction}
		\begin{tabular}{ll}
			\toprule
			\textbf{Paradigm} & \textbf{Predictive model} \\
			\midrule
			Frequentist & $f(\hat{\theta}^*(D), x_{n+1})$ \\[3pt]
			Bayesian    & $\mathbb{E}[f(\theta, x_{n+1}) \mid D]$ \\[3pt]
			\bottomrule
			\label{tab:comare}
		\end{tabular}
	\end{table}
\end{remark}

\subsection{Frequentist Parameter Estimation}
\label{chp:frequentist_parameter_estimation}
As shown in \secref{chp:frequentist_regression} and \secref{chp:frequentist_classification}, both regression and classification in the Frequentist paradigm can be reframed as problems of parameter estimation. This makes parameter estimation the central focus of Frequentist statistics. Unlike in Bayesian statistics, where parameters are intermediate quantities to be marginalized over, in the Frequentist framework the parameters are fixed but unknown, and their determination carries substantive interpretational and practical importance. Estimators of these parameters serve as decision rules that summarize past observations into actionable predictions.

\begin{definition}[Sampling distribution]
	Let $D$ denote the observed dataset and let $\hat{w}(D)$ be a decision rule (estimator) for the fixed-but-unknown parameter $w\in\Omega_W$. The sampling distribution\index{Sampling distribution} of $\hat{w}$ is the probability distribution of the random variable $\hat{w}(D)$ induced by repeated sampling of $D$ from the data-generating mechanism $p(D \mid w)$.	
\end{definition}

\begin{remark}[Bayesian versus Frequentist perspective]
	The sampling distribution of an estimator $\hat{w}(D)$ is central to the Frequentist paradigm, since all uncertainty arises from the randomness of the data $D \sim p(D \mid w)$ while the parameter $w$ is treated as a fixed but unknown constant. In Bayesian statistics, by contrast, uncertainty about $w$ is represented by a posterior distribution $p(w \mid D)$ after observing data. Both approaches yield distributions over possible parameter values or estimates, but their conceptual origin differs: in the Frequentist case, the distribution is over repeated samples of data, whereas in the Bayesian case, the distribution is over the parameter itself given the observed data.
\end{remark}

\begin{example}
	In practice, the true sampling distribution of an estimator $\hat{w}(D)$ is rarely available in closed form. The bootstrap\index{Bootstrapping} provides an approximation technique based solely on the observed dataset. Let $D = \{(x_i,y_i)\}_{i=1}^n$ be the dataset. A bootstrap sample $D^{b}$ is constructed by sampling $n$ observations with replacement from $D$. Repeating this procedure $B$ times yields bootstrap replicates $\hat{w}(D^{1}), \ldots, \hat{w}(D^{B})$, whose empirical distribution approximates the sampling distribution of $\hat{w}(D)$. 
	
	Common quantities derived from the bootstrap include:
	\begin{itemize}
		\item The bootstrap estimate of variance:
		\begin{equation}
			\widehat{\operatorname{Var}}_{\mathrm{boot}}[\hat{w}] = \frac{1}{B-1}\sum_{b=1}^B \left(\hat{w}(D^{b}) - \overline{\hat{w}}\right)^2,
		\end{equation}
		where
		\begin{equation}
			\overline{\hat{w}} = \frac{1}{B}\sum_{b=1}^B \hat{w}(D^{b}).
		\end{equation}
		\item The bootstrap confidence interval, constructed from quantiles of the bootstrap distribution of $\hat{w}$.
	\end{itemize}
\end{example}

\begin{definition}[Fisher information]
	\label{def:fisher_information}
	Let $D_y = \{y_i\}_{i=1}^n$ denote Nature’s actions, $D_x = \{x_i\}_{i=1}^n$ the corresponding observations, and let $w \in \Omega_W$ be an unknown parameter of the model.  
	Let $p(D_y \mid D_x, w)$ denote the likelihood\index{Likelihood} of observing $D_y$ given $D_x$ and $w$.  
	The Fisher information\index{Fisher information} is defined as
	\begin{equation}
		\begin{split}
			\mathcal{I}(w)
			&\equiv \mathbb{E}\bigg[\bigg(\frac{\partial}{\partial w} \ln p(D_y \mid D_x, w)\bigg)^2 \,\Bigg|\, D_x, w\bigg] \\
			&= \operatorname{Var}\bigg[\frac{\partial}{\partial w} \ln p(D_y \mid D_x, w) \,\Bigg|\, D_x, w\bigg].
		\end{split}
	\end{equation}
\end{definition}


\begin{proof}
	In general
	\begin{equation}
		\mathbb{E}\bigg[\bigg(\frac{\partial}{\partial w} \ln p\bigg)^2\bigg] 
		= \operatorname{Var}\bigg[\frac{\partial}{\partial w} \ln p\bigg] + 
		\bigg(\mathbb{E}\bigg[\frac{\partial}{\partial w} \ln p\bigg]\bigg)^2.
	\end{equation}
	Now
	\begin{align}
		\mathbb{E}\bigg[\frac{\partial}{\partial w} \ln p\bigg] 
		&= \int \bigg(\frac{\partial}{\partial w} \ln p\bigg)  p \mathrm{d}D_y \\
		&= \int \frac{\partial}{\partial w} p \mathrm{d}D_y\\
		&= \frac{\partial}{\partial w} \int  p \mathrm{d}D_y \\
		&= 0,
	\end{align}
	since $\int p(D_y \mid D_x, w) \mathrm{d}D_y = 1$. Therefore
	\begin{equation}
		\mathcal{I}(w) = \operatorname{Var}\bigg[\frac{\partial}{\partial w} \ln p(D_y \mid D_x, w) \,\bigg|\,  D_x, w\bigg].
	\end{equation}
\end{proof}

\begin{theorem}[Fisher information for independent observations]
	\label{thm:fisher_sample}
	Let $D_y = \{y_i\}_{i=1}^n$ denote Nature’s actions and $D_x = \{x_i\}_{i=1}^n$ the corresponding observations, and let $w \in \Omega_W$ be the parameters of Nature’s model. If the data are independently and identically distributed (IID)\index{IID}, the likelihood\index{Likelihood} factorizes as
	\begin{equation}
		p(D_y \mid D_x, w) = \prod_{i=1}^{n} p(y_i \mid x_i, w).
	\end{equation}
	Then, the Fisher information\index{Fisher information} of the full dataset is
	\begin{equation}
		\begin{split}
			\mathcal{I}(w) 
			&= \mathbb{E}\Bigg[\Bigg(\frac{\partial}{\partial w} \ln p(D_y \mid D_x, w)\Bigg)^2 \,\Bigg|\, D_x, w \Bigg] \\
			&= \sum_{i=1}^{n} \mathcal{I}_i(w),
		\end{split}
	\end{equation}
	where $\mathcal{I}_i(w)$ denotes the Fisher information contributed by the $i$th observation:
	\begin{equation}
		\mathcal{I}_i(w) = \mathbb{E}\Bigg[\Bigg(\frac{\partial}{\partial w} \ln p(y_i \mid x_i, w)\Bigg)^2 \,\Bigg|\, x_i, w\Bigg].
	\end{equation}
\end{theorem}

\newpage
\begin{definition}[Maximum likelihood estimator (MLE) decision rule]
	\label{def:MLE}
	Let $D_y = \{y_i\}_{i=1}^n$ denote Nature’s actions, $D_x = \{x_i\}_{i=1}^n$ the corresponding observations, and let $w \in \Omega_W$ be the parameters of Nature’s model.  
	The Maximum Likelihood Estimator (MLE)\index{Maximum likelihood estimator} decision rule $\hat{w}_{\mathrm{MLE}}$ is the value of $w$ that maximizes the likelihood of observing $D_y$ given $D_x$:
	\begin{equation}
		\hat{w}_{\mathrm{MLE}}(D) \equiv \argmax_{w \in \Omega_W} p(D_y \mid D_x, w).
	\end{equation}
\end{definition}

\begin{theorem}[Asymptotic sampling distribution of the MLE]
	\label{thm:unbiased_mle}
	Let $\hat{w}_{\mathrm{MLE}}(D)$ denote the Maximum Likelihood Estimator (MLE)\index{Maximum likelihood} of the fixed-but-unknown parameter $w$.  
	Under standard regularity conditions, the sampling distribution of $\hat{w}_{\mathrm{MLE}}$ satisfies
	\begin{equation}
		\sqrt{n}\,\big(\hat{w}_{\mathrm{MLE}} - w\big) \xrightarrow{\text{d}} \operatorname{Norm}\big(0, \mathcal{I}(w)^{-1}\big),
	\end{equation}
	where $\mathcal{I}(w)$ is the Fisher information matrix evaluated at $w$, and $\xrightarrow{\text{d}}$ denotes convergence in distribution as $n \to \infty$.  
	That is, the sampling distribution of the MLE becomes approximately normal, centered at the true parameter $w$ with variance given by the inverse Fisher information.
\end{theorem}

\begin{definition}[Minimax decision rule]
	\label{def:minimax}
	A decision rule $\hat{w}'$ is said to be minimax if it minimizes the maximum\index{Minimax} expected cost, that is,
	\begin{equation}
		\begin{split}
			\hat{w}' &\equiv \inf_{\hat{w}} \sup_{w \in \Omega_W} \mathbb{E}[C(\hat{w}, w) \mid w] \\
			&= \inf_{\hat{w}} \sup_{w \in \Omega_W} \int C(\hat{w}(D), w)\, p(D \mid w)\, \mathrm{d}D.
		\end{split}
	\end{equation}
\end{definition}

\begin{theorem}[Mean squared error (MSE)]
	\label{theorem:MSE}
	The expectation of the quadratic cost function for a parameter estimate can be written as
	\begin{equation}
		\begin{split}
			\mathbb{E}[C(\hat{w}, w)\mid w] &= \mathbb{E}[(\hat{w}-w)^2\mid w]\\ 
			&= \mathbb{E}[(\hat{w}-\mathbb{E}[\hat{w}])^2\mid w] + (w-\mathbb{E}[\hat{w}])^2\\
			&= \operatorname{Var}[\hat{w}\mid w] + \operatorname{Bias}[\hat{w}\mid w]^2,
		\end{split}
		\label{eq:MSE}
	\end{equation}
	where the bias of the estimator $\hat{w}$ is defined as
	\begin{equation}
		\operatorname{Bias}[\hat{w}\mid w] \equiv w - \mathbb{E}[\hat{w}].
	\end{equation}
	If $\mathbb{E}[C(\hat{w}, w)\mid w] \to 0$ as $n \to \infty$, then $\hat{w}$ is a weakly consistent estimator\index{Weakly consistent estimator} of $w$, i.e., $\hat{w} \xrightarrow{p} w$.  
	Different consistent estimators may converge to $w$ at different rates.  
	It is desirable for an estimator to be consistent and have a small (quadratic) cost, meaning that both its bias and variance should be small.\index{Bias-Variance tradeoff}  
	In practice, however, these two quantities often cannot be minimized simultaneously.
\end{theorem}

\begin{corollary}[MLE is approximately minimax for quadratic loss]
	\label{cor:MLE_minimax}
	Under standard regularity conditions, the Maximum Likelihood Estimator (MLE)\index{Maximum likelihood} $\hat{w}_{\text{MLE}}$ is approximately minimax for the quadratic cost function, meaning it approximately minimizes the maximum expected cost\index{Minimax}.
	\begin{proof}
		From \thref{theorem:MSE},
		\begin{equation}
			\mathbb{E}[(\hat{w}-w)^2\mid w] = \operatorname{Var}[\hat{w}\mid w] + \operatorname{Bias}[\hat{w}\mid w]^2.
		\end{equation}
		Under the regularity conditions where the MLE\index{Maximum likelihood} is unbiased and asymptotically efficient, the bias term vanishes, meaning $\operatorname{Bias}[\hat{w}_{\text{MLE}}\mid w] = 0$, and the variance term $\operatorname{Var}[\hat{w}_{\text{MLE}}\mid w]$ is asymptotically minimal among a class of estimators.  
		Thus, the expected quadratic cost for the MLE can be approximated by
		\begin{equation}
			\begin{split}
				\mathbb{E}[(\hat{w}_{\text{MLE}} - w)^2\mid w] &\approx \operatorname{Var}[\hat{w}_{\text{MLE}}\mid w] \\
				&\approx \frac{\operatorname{tr}[\mathcal{I}(w)^{-1}]}{n},
			\end{split}
		\end{equation}
		where the second line follows from \thref{thm:unbiased_mle}.  
		The Cramér–Rao lower bound~\citep{Rao1973Linear} states that
		\begin{equation}
			\operatorname{Var}[\hat{w}\mid w] \geq \frac{\operatorname{tr}[\mathcal{I}(w)^{-1}]}{n},
		\end{equation}
		implying that the MLE achieves the smallest possible variance asymptotically. Therefore,
		\begin{equation}
			\sup_{w \in \Omega_W} \mathbb{E}[(\hat{w}_{\text{MLE}} - w)^2 \mid w]
			\approx \inf_{\hat{w}} \sup_{w \in \Omega_W} \mathbb{E}[(\hat{w} - w)^2 \mid w],
		\end{equation}
		meaning the MLE decision rule is approximately minimax under quadratic cost.
	\end{proof}
\end{corollary}

\begin{remark}[Over/under-fitting]
	The bias-variance decomposition (\thref{theorem:MSE}) is a concept relevant to Frequentist statistics, where a single point estimate of the parameters is used.\index{Bias-variance tradeoff} This decomposition illustrates the tradeoff between underfitting and overfitting\index{Overfitting}\index{Underfitting}: high bias corresponds to underfitting, while high variance corresponds to overfitting. \newline
	
	In Bayesian statistics, predictions are obtained by integrating over the posterior distribution of parameters, rather than relying on a single point estimate. This integration inherently regularizes the model, mitigating overfitting and underfitting.
\end{remark}

\begin{remark}[Frequentist regression]
	\label{rm:frequentist_regression}
	According to \EQref{freq:decision}, the optimal decision rule for the Robot in the Frequentist regression setting depends on the observed data $x_{n+1}$ and on the optimal decision rule for parameter estimation, $\hat{w}^*(D)$. As defined in \dfref{def:minimax}, the optimal parameter decision rule under the quadratic cost of \EQref{freq:decision} coincides, according to \corref{cor:MLE_minimax}, approximately with the maximum likelihood estimate of \dfref{def:MLE}.
	
	To illustrate this concretely, consider a dataset $D = \{(x_i, y_i)\}_{i=1}^n$ consisting of IID pairs\index{IID}, where each observation follows the probabilistic model in \EQref{freq:dist}. The parameters of interest are $\theta \in \Omega_\Theta \subset \Omega_W$, representing the regression coefficients. The precision parameter $\xi \in \Omega_W$, denoting the inverse variance of the noise, is treated as a nuisance parameter, as it does not directly affect the decision rule in \EQref{freq:decision}.
	
	By equation \EQref{freq:dist}\index{Normal distribution}, the likelihood\index{Likelihood} is given by
	\begin{equation}
		p(D_y \mid D_x, \theta, \xi) 
		= \bigg(\frac{\xi}{2\pi}\bigg)^{\!\frac{n}{2}}
		\prod_{i=1}^n e^{-\frac{\xi}{2}\big(f(\theta, x_i) - y_i\big)^2}.
		\label{eq:freq_reg_likelihood}
	\end{equation}
	Since the natural logarithm is a monotonic transformation, the MLE can equivalently be obtained by minimizing the negative log-likelihood\index{Log-likelihood}, also called the loss function\index{Loss function},
	\begin{equation}
		\begin{split}
			\ell(\theta, \xi)
			&= -\ln p(D_y \mid D_x, \theta, \xi) \\
			&= \frac{n}{2}[\ln(2\pi) -\ln\xi]
			+ \frac{\xi}{2}\sum_{i=1}^n \big(f(\theta, x_i) - y_i\big)^2.
		\end{split}
	\end{equation}
	
	The loss function $\ell$ plays a role analogous to the Hamiltonian in the Bayesian formulation of \EQref{eqh2}. Indeed, one can view $\ell$ as a subset of $H$, in the sense that it captures the same data-fit term but omits the prior and momentum contributions. Despite this structural similarity, their purposes differ: $\ell$ is minimized to obtain a single optimal parameter estimate, whereas $H$ is used to generate an ensemble of parameter values for computing the expected value of $f$.
	
	Minimization with respect to $\theta$, for fixed $\xi$, is equivalent to minimizing the sum of squared residuals,
	\begin{equation}
		\hat{\theta}_{\text{MLE}}
		= \argmin_{\theta}
		\sum_{i=1}^n \big(f(\theta, x_i) - y_i\big)^2.
	\end{equation}
\end{remark}

\begin{remark}[Philosophical tension within Frequentist statistics]
	\label{rm:philosophical_challenge}
	The structural similarity between the loss function and the Hamiltonian, described in \rmref{rm:frequentist_regression}, extends naturally through the introduction of regularization and ensemble learning. Bayesian models inherently mitigate overfitting and promote model stability through prior distributions that constrain parameter uncertainty. In contrast, the Frequentist framework must introduce analogous mechanisms explicitly. For example, ridge regression\citep{Plaut1986} corresponds directly to imposing a Gaussian prior on the regression coefficients, making it equivalent to Bayesian linear regression with a conjugate prior. Likewise, ensemble methods such as bagging approximate Bayesian model averaging by combining predictions from multiple fitted models, thereby capturing parameter uncertainty through resampling rather than explicit probabilistic inference.
	
	In such cases, the Frequentist decision rule (\EQref{freq:decision}) increasingly resembles the Bayesian one (\EQref{eq:q3}), blurring the practical distinction between the two paradigms. This convergence exposes a philosophical tension within Frequentist statistics: in seeking to replicate the robustness of Bayesian methods, it implicitly incorporates subjective elements--through modeling choices, regularization assumptions, or ensemble constructions--thus challenging its foundational claim of objectivity (\dfref{def:objective_probability}). 
	
	To be clear, this incorporation of subjectivity does not undermine the mathematical validity of the Frequentist definition of probability as a long-run relative frequency (\dfref{def:objective_probability}). Rather, it challenges the philosophical claim of epistemic objectivity, since regularization and ensemble methods implicitly encode subjective choices that parallel the role of priors in Bayesian inference.
\end{remark}


\begin{example}
	\label{ex:bernoulli_quad_cost}
	Consider a statistical experiment in which the actions of Nature are IID\index{IID} Bernoulli\index{Bernoulli distribution} random variables with unknown parameter $w \in \Omega_W = [0,1]$. Each observed outcome $y_i \in \{0,1\}$ satisfies
	\begin{equation}
		p(1 \mid w) = w, \qquad p(0 \mid w) = 1-w,
	\end{equation}
	and the Robot observes the dataset
	\begin{equation}
		D_y = \{y_i\}_{i=1}^n.
	\end{equation}
	The goal of the Robot is to estimate the unknown parameter $w$ based on $D_y$.
	\begin{itemize}
		\item \textbf{Arithmetic mean:} 
		\(
		\hat{w}(D_y) = \frac{1}{n}\sum_{i=1}^n y_i
		\)
		with 
		\begin{equation} 
			\begin{split} 
				\mathbb{E}[\hat{w}(D_y)\mid w] &= \int \hat{w}(D_y) p(D_y\mid w) \mathrm{d}D_y\\ & = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[Y_{i}\mid w]\\ & = w,\\ \operatorname{Var}[\hat{w}(D_y)\mid w] & =\int (\hat{w}(D_y)-\mathbb{E}[\hat{w}(D_y)\mid w])^2 p(D_y\mid w) \mathrm{d}D_y\\ &= \frac{1}{n^2} \sum_{i=1}^n \operatorname{Var}[Y_{i}\mid w]\\ & = \frac{w(1-w)}{n},\\ \mathbb{E}[(\hat{w}(D_y)-w)^2\mid w] &= \operatorname{Var}[\hat{w}(D_y)\mid w]+(\mathbb{E}[\hat{w}(D_y)\mid w]-w)^2\\ & = \frac{w(1-w)}{n}. 
			\end{split} 
		\end{equation}
		
		\item \textbf{Constant estimator:} 
		\(
		\hat{w}(D_y) = 0.5
		\)
		with 
		\begin{equation} 
			\begin{split} 
				\mathbb{E}[\hat{w}\mid w] &= 0.5,\\ \operatorname{Var}[\hat{w}\mid w] &= 0,\\ \mathbb{E}[(\hat{w}-w)^2\mid w] &= (0.5 - w)^2. 
			\end{split} 
			\end{equation}
		
		\item \textbf{First observation:} 
		\(
		\hat{w}(D_y) = y_1
		\)
		with 
		\begin{equation} 
			\begin{split} 
				\mathbb{E}[\hat{w}(D_y)\mid w] &= \mathbb{E}[Y_1\mid w]\\ & = w,\\ \operatorname{Var}[\hat{w}(D_y)\mid w] &= \operatorname{Var}[Y_1\mid w] +(\mathbb{E}[\hat{w}(D_y)\mid w]-w)^2\\ &= w(1-w),\\ \mathbb{E}[(\hat{w}(D_y)-w)^2\mid w] &= w(1-w).
			\end{split} 
		\end{equation}
	\end{itemize}
	The arithmetic mean minimizes the quadratic cost across all $w$, whereas the constant estimator may perform better only for particular values of $w$. The variance of the first-observation estimator does not decrease with sample size $n$, making it sub-optimal as $n$ increases.
\end{example}

\begin{example}
	\label{ex:bernoulli_mle}
	The likelihood function for \exref{ex:bernoulli_quad_cost} is
	\begin{equation} 
		\begin{split} 
			p(D_y\mid D_x,w) & =p(D_y\mid w)\\ & = \prod_{i=1}^nw^{y_i}(1-w)^{1-y_i}. 
		\end{split} 
	\end{equation} 
	Let $\ell(w)\equiv -\ln p(D_y\mid D_x,w)$, then 
	\begin{equation} 
		\begin{split} 
			\argmin_w \ell(w) & = \argmax_wp(D_y\mid w)\\ &= \argmax_w\ln \bigg(\prod_{i=1}^nw^{y_i}(1-w)^{1-y_i}\bigg)\\ &=\argmin_w \bigg[ \ln(1-w)\sum_{i=1}^n(y_i-1)-\ln w\sum_{i=1}^ny_i\bigg] 
		\end{split} 
	\end{equation}  
	Requiring
	\begin{equation} 
		\frac{\partial }{\partial w}\ell(w)=\frac{n-\sum_{i=1}^ny_i}{1-w}-\frac{\sum_{i=1}^ny_i}{w}
	\end{equation}
	 to vanish means the maximum likelihood estimate\index{Maximum likelihood} of $w$ is given by 
	\begin{equation} 
		\hat{w}_{\text{MLE}}(D_y)=\frac{1}{n}\sum_{i=1}^ny_i. 
	\end{equation} 
\end{example} 

\begin{example}
	\label{ex:exponential_mle}
	Consider a statistical experiment in which the actions of Nature are IID\index{IID} random variables with exponential distribution\index{Exponential distribution} and unknown rate parameter $w>0$. Each observed outcome $y_i \in \mathbb{R}_{>0}$ satisfies
	\begin{equation}
		p(y_i \mid w) = \frac{e^{-\frac{y_i}{w}}}{w}, \quad i = 1, \dots, n,
	\end{equation}
	and the Robot observes the dataset
	\begin{equation}
		D_y = \{y_i\}_{i=1}^n.
	\end{equation}
	The goal of the Robot is to estimate the unknown parameter $w$ based on $D_y$. The likelihood is
	\begin{equation}
		p(D_y \mid w) = \prod_{i=1}^n \frac{e^{-\frac{y_i}{w}}}{w}.
	\end{equation}
	The loss function is
	\begin{equation}
		\ell(w) = \frac{\sum_{i=1}^n y_i}{w}-n \ln w.
	\end{equation}
	Differentiating and setting to zero gives the maximum likelihood estimate\index{Maximum likelihood}:
	\begin{equation}
		\hat{w}_{\mathrm{MLE}}(D_y) = \frac{\sum_{i=1}^n y_i}{n}.
	\end{equation}
\end{example}

\begin{example}
	The posterior mean derived in \exref{ex:heteroscedastic_bayes} coincides with the maximum likelihood estimate of the common mean. This equivalence arises because the prior on the parameter was assumed to be uniform, and therefore does not influence the posterior. In general, for a non-informative prior, the Bayes estimator under a quadratic loss reduces to the maximum likelihood estimator.
\end{example}

