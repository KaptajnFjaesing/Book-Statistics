\section{Frequentist Statistics}
\label{chp:freq}
The Frequentist paradigm (\dfref{def:frequentist_statistics}) trace back to seminal works such as those of Neyman and Pearson \citep{Neyman1928OnSR} and Fisher \citep{fisher1925statistical}, who laid the groundwork for much of its methodology. Subsequent developments by Wald \citep{Wald1945Sequential}, Neyman \citep{Neyman1948Consistent}, and Lehmann \citep{lehmann1986testing} further refined its theories and techniques.\newline
In the Frequentist paradigm, it is assumed that Nature's decisions can be captured by a statistical model with fixed, unknown parameters and an objective probability measure. In this setting, the optimal decision rule can be expressed as
\begin{equation}
	U^*(x,w).
\end{equation}
Since $w$ is not known to the Robot, the central task becomes to estimate $w$ from past data $D$. This gives rise to a nested decision problem with two levels:
\begin{enumerate}
	\item[\textit{i)}] Parameter estimation: use past data $D$ to construct an estimator $\hat{w}(D)$ of the fixed but unknown parameters $w$.
	\item[\textit{ii)}] Prediction/decision: given a new observation $x$ and the parameter estimate $\hat{w}(D)$, apply the decision rule $U$ to determine an action.
\end{enumerate}
To avoid notational ambiguity, a distinction is made between the decision rule used for prediction, denoted $U$, and the decision rule used for parameter estimation, denoted $\hat{w}$. The practical decision rule for a new observation $x_{n+1} \in \Omega_X$ therefore takes the form\footnote{\EQref{eq:decision_rule_frequentist} justifies the unified notation $U(\tilde{D})$ for the decision rule in \dfref{def:statistical_game} and \rmref{rem:frequentist_bayesian_expected_cost}.}
\begin{equation}
	U^*(x_{n+1}, \hat{w}^*(D)),
	\label{eq:decision_rule_frequentist}
\end{equation}
where $\hat{w}^*(D)$ denotes the optimal parameter decision rule, obtained from past data $D$, and the final action is determined by minimizing the expected cost as specified in \dfref{def:statistical_game} and \rmref{rem:frequentist_bayesian_expected_cost}.


\subsection{Frequentist Regression}
\label{chp:frequentist_regression}
Regression involves the Robot constructing a model,
\begin{equation}
	f: \Omega_\Theta \times \Omega_X \to \Omega_Y,
\end{equation}
with associated parameters $\theta \in \Omega_\Theta\subseteq \Omega_W$, that estimates Nature's actions $y_{n+1}\in \Omega_Y$ based on observed data $x_{n+1}\in \Omega_X=\mathbb{R}$. The model $f$ acts as a proxy for the Robot in that it on behalf of the Robot estimates the action of Nature given an input. Hence, in providing an estimate, the model must make a choice, similar to the Robot and thus the Robot must pick a cost function for the model. In this study, the quadratic cost function from \dfref{def:quadratic_cost} will be considered to review the subject.


Assuming the actions of Nature follow a normal distribution with the function $f$ as mean and an unknown precision $\xi \in \Omega_{W}$
\begin{equation}
	p(y_{n+1}|x_{n+1},w)=\sqrt{\frac{\xi}{2\pi}} e^{-\frac{\xi}{2}(f(\theta,x_{n+1})-y_{n+1})^2},
	\label{freq:dist}
\end{equation}
where $w = \{\theta, \xi,\dots\}$ denotes the collection of fixed, unknown parameters. Under the quadratic cost function from \dfref{def:quadratic_cost}, the optimal decision rule is the conditional expectation of $Y_{n+1}$ given $(x_{n+1},w)$ (\thref{theorem:expectation_decision_rule}),
\begin{equation}
	\begin{split}
		U^*(x_{n+1},\hat{w}^*(D)) &= \mathbb{E}[Y_{n+1} \mid x_{n+1}, \hat{w}^*(D)]\\
		&= \int y_{n+1} p(y_{n+1} \mid x_{n+1}, \hat{w}^*(D)) \mathrm{d}y_{n+1}\\
		& = f(\hat{\theta}^*(D),x_{n+1}).
	\end{split}
	\label{freq:decision}
\end{equation}
\EQref{freq:decision} represents the Frequentist optimal decision rule, defined conditional on an estimate of the model parameters. This can be directly contrasted with \EQref{eq:q3}, which expresses the Bayesian optimal decision rule obtained by averaging over the posterior distribution of the model parameters (and latent variables) given the observed data. From \EQref{freq:decision}, it follows that within the Frequentist paradigm, regression becomes a problem of parameter estimation.

\subsection{Frequentist Classification}
\label{chp:frequentist_classification}
The setup for Frequentist classification initially mirrors that of Bayesian classification (\secref{chp:baycl}), but start diverging at the level of the cost function
\begin{equation}
	C(U(x_{n+1},w), y_{n+1}) = 1 - \delta_{U(x_{n+1},w),y_{n+1}} + (\lambda-1)\delta_{U(x_{n+1},w), \operatorname{reject}}.
\end{equation}
Let $\hat{w}^*(D)$ denote the optimal Frequentist estimator of the model parameters obtained from past data $D$. The optimal decision rule for a new observation $x_{n+1}$ is (\thref{theorem:opt_decision_rule} and \EQref{eq:conditional_cost_discrete})
\begin{equation}
	\begin{split}
		U^*(x_{n+1}, \hat{w}^*(D)) &= \argmin_{U(\tilde{D})} \mathbb{E}[C(U(\tilde{D}), Y_{n+1}) \mid x_{n+1}, \hat{w}(D)] \\
		&= \argmin_{U(\tilde{D})} \sum_{y_{n+1}\in \Omega_Y}C(U(\tilde{D}),y_{n+1})p(y_{n+1}|x_{n+1}, \hat{w}(D)) \\
		&= \argmin_{U(\tilde{D})} \Big(1 - f_{U(\tilde{D})}(\hat{\theta}^*(D), x_{n+1}) + (\lambda-1)\delta_{u, \operatorname{reject}}\Big).
	\end{split}
	\label{freq:decision_classification}
\end{equation}
From \EQref{freq:decision_classification}, it follows that within the Frequentist paradigm, classification becomes a problem of parameter estimation.

\begin{remark}[Bayesian versus Frequentist Regression and Classification]
	Contrasting \EQref{freq:decision} and \EQref{eq:q3} for regression and \EQref{freq:decision_classification} and \EQref{eq:expected_cost1} for classification, the mathematical difference between the two paradigms can be written as shown in Table \ref{tab:comare}.
	\begin{table}[h!]
		\centering
		\caption{Comparison between Frequentist and Bayesian prediction}
		\begin{tabular}{ll}
			\toprule
			\textbf{Paradigm} & \textbf{Predictive model} \\
			\midrule
			Frequentist & $f(\hat{\theta}^*(D), x_{n+1})$ \\[3pt]
			Bayesian    & $\mathbb{E}[f(\theta, x_{n+1}) \mid D]$ \\[3pt]
			\bottomrule
			\label{tab:comare}
		\end{tabular}
	\end{table}
\end{remark}

\subsection{Frequentist Parameter Estimation}
\label{chp:frequentist_parameter_estimation}
As shown in \secref{chp:frequentist_regression} and \secref{chp:frequentist_classification}, both regression and classification in the Frequentist paradigm can be reframed as problems of parameter estimation. This makes parameter estimation the central focus of Frequentist statistics. Unlike in Bayesian statistics, where parameters are intermediate quantities to be marginalized over, in the Frequentist framework the parameters are fixed but unknown, and their determination carries substantive interpretational and practical importance. Estimators of these parameters serve as decision rules that summarize past observations into actionable predictions.

\begin{definition}[Sampling distribution]
	Let $D$ denote the observed dataset and let $\hat{w}(D)$ be a decision rule (estimator) for the fixed-but-unknown parameter $w\in\Omega_W$. The sampling distribution\index{Sampling distribution} of $\hat{w}$ is the probability distribution of the random variable $\hat{w}(D)$ induced by repeated sampling of $D$ from the data-generating mechanism $p(D \mid w)$.	
\end{definition}

\begin{remark}[Bayesian versus Frequentist perspective]
	The sampling distribution of an estimator $\hat{w}(D)$ is central to the Frequentist paradigm, since all uncertainty arises from the randomness of the data $D \sim p(D \mid w)$ while the parameter $w$ is treated as a fixed but unknown constant. In Bayesian statistics, by contrast, uncertainty about $w$ is represented by a posterior distribution $p(w \mid D)$ after observing data. Both approaches yield distributions over possible parameter values or estimates, but their conceptual origin differs: in the Frequentist case, the distribution is over repeated samples of data, whereas in the Bayesian case, the distribution is over the parameter itself given the observed data.
\end{remark}

\begin{example}
	In practice, the true sampling distribution of an estimator $\hat{w}(D)$ is rarely available in closed form. The \emph{bootstrap} provides an approximation technique based solely on the observed dataset. Let $D = \{(x_i,y_i)\}_{i=1}^n$ be the dataset. A bootstrap\index{Bootstrapping} sample $D^{b}$ is constructed by sampling $n$ observations with replacement from $D$. Repeating this procedure $B$ times yields bootstrap replicates $\hat{w}(D^{1}), \ldots, \hat{w}(D^{B})$, whose empirical distribution approximates the sampling distribution of $\hat{w}(D)$. 
	
	Common quantities derived from the bootstrap include:
	\begin{itemize}
		\item The bootstrap estimate of variance:
		\begin{equation}
			\widehat{\operatorname{Var}}_{\mathrm{boot}}[\hat{w}] = \frac{1}{B-1}\sum_{b=1}^B \left(\hat{w}(D^{b}) - \overline{\hat{w}}^{*}\right)^2,
		\end{equation}
		where $\overline{\hat{w}}^{*} = \frac{1}{B}\sum_{b=1}^B \hat{w}(D^{b})$.
		\item The bootstrap confidence interval, constructed from quantiles of the bootstrap distribution of $\hat{w}$.
	\end{itemize}
\end{example}


\begin{definition}[Fisher Information]
	\label{def:fisher_information}
	Take $D_y= \{y_i\}_{i=1}^n$, $D_x= \{x_i\}_{i=1}^n$ and let $w \in \Omega_W$ be an unknown parameter of the model. Let $p(D_y | D_x, w, I)$ denote the likelihood of observing Natures actions $D_y$ given observed data $D_x$ and $w$. The Fisher information\index{Fisher information} is defined as
	\begin{equation}
		\begin{split}
			\mathcal{I}(w) &\equiv \mathbb{E} \bigg[\bigg(\frac{\partial}{\partial w} \ln p(D_y|D_x, w)\bigg)^2 \Bigg| D_x, w\bigg]\\
			&= \operatorname{Var}\bigg[\frac{\partial}{\partial w} \ln p(D_y|D_x, w) \Bigg| D_x, w\bigg].
		\end{split}
	\end{equation}
\end{definition}

\begin{proof}
	In general
	\begin{equation}
		\mathbb{E}\bigg[\bigg(\frac{\partial}{\partial w} \ln p\bigg)^2\bigg] 
		= \operatorname{Var}\bigg[\frac{\partial}{\partial w} \ln p\bigg] + 
		\bigg(\mathbb{E}\bigg[\frac{\partial}{\partial w} \ln p\bigg]\bigg)^2.
	\end{equation}
	Now
	\begin{align}
		\mathbb{E}\bigg[\frac{\partial}{\partial w} \ln p\bigg] 
		&= \int \bigg(\frac{\partial}{\partial w} \ln p\bigg)  p \mathrm{d}D_y \\
		&= \int \frac{\partial}{\partial w} p \mathrm{d}D_y\\
		&= \frac{\partial}{\partial w} \int  p \mathrm{d}D_y \\
		&= 0,
	\end{align}
	since $\int p(D_y \mid D_x, w) \mathrm{d}D_y = 1$. Therefore
	\begin{equation}
		\mathcal{I}(w) = \operatorname{Var}\bigg[\frac{\partial}{\partial w} \ln p(D_y \mid D_x, w) \bigg| D_x, w\bigg].
	\end{equation}
\end{proof}

\begin{theorem}[Fisher Information for Independent Observations]
	\label{thm:fisher_sample}
	Take $D_y= \{y_i\}_{i=1}^n$ , $D_x = \{x_i\}_{i=1}^n$ and let $w \in \Omega_W$ be a parameter of the model. Assume the likelihood factorizes as
	\begin{equation}
		p(D_y | D_x, w) = \prod_{i=1}^{n} p(y_i | x_i, w).
	\end{equation}
	Then, the Fisher information of the full dataset is
	\begin{equation}
		\mathcal{I}(w) 
		= \mathbb{E}\Bigg[\Big(\frac{\partial}{\partial w} \ln p(D_y | D_x, w)\Big)^2 \Bigg| D_x, w \Bigg] 
		= \sum_{i=1}^{n} \mathcal{I}_i(w),
	\end{equation}
	where $\mathcal{I}_i(w)$ is the Fisher information of the $i$-th observation:
	\begin{equation}
		\mathcal{I}_i(w) = \mathbb{E}\Bigg[\Bigg(\frac{\partial}{\partial w} \ln p(y_i | x_i, w)\Bigg)^2 \Bigg| x_i, w\Bigg].
	\end{equation}
\end{theorem}

\begin{definition}[Maximum Likelihood Estimator (MLE) Decision Rule]
	\label{def:MLE}
	Take $D_y= \{y_i\}_{i=1}^n$ , $D_x = \{x_i\}_{i=1}^n$ and let $w \in \Omega_W$ be a fixed but unknown parameter. The Maximum Likelihood Estimator (MLE) decision rule \index{Maximum likelihood estimator} $\hat{w}_{\mathrm{MLE}}$ is the value of $w$ that maximizes the likelihood of observing $D_y$ given $D_x$
	\begin{equation}
		\hat{w}_{\mathrm{MLE}}(D) \equiv \argmax_{w \in \Omega_W} p(D_y | D_x, w).
	\end{equation}
\end{definition}

\begin{theorem}[Asymptotic Sampling Distribution of the MLE]
	\label{thm:unbiased_mle}
	Let $\hat{w}_{\mathrm{MLE}}(D)$ denote the Maximum Likelihood Estimator (MLE) of the fixed-but-unknown parameter $w$. Under standard regularity conditions, the sampling distribution of $\hat{w}_{\mathrm{MLE}}$ satisfies
	\begin{equation}
		\sqrt{n}\,\big(\hat{w}_{\mathrm{MLE}} - w\big) \xrightarrow{\text{d}} \operatorname{Norm}\big(0, \mathcal{I}(w)^{-1}\big),
	\end{equation}
	where $\mathcal{I}(w)$ is the Fisher information matrix evaluated at $w$ and $\xrightarrow{\text{d}}$ denotes convergence in distribution as $n \to \infty$. 
	That is, the sampling distribution of the MLE becomes approximately normal, centered at the true parameter $w$ with variance given by the inverse Fisher information.
\end{theorem}


\begin{definition}[Minimax Decision Rule]
	\label{def:minimax}
	A decision rule $\hat{w}'$ is said to be minimax if it minimize the maximum\index{Minimax} expected cost, meaning (\EQref{eq:conditional_expected_cost})
	\begin{equation}
		\begin{split}
			\hat{w}' &\equiv \inf_{\hat{w}}\sup_{w\in \Omega_W}\mathbb{E}[C(\hat{w},w)|w]\\
			& = \inf_{\hat{w}}\sup_{w\in \Omega_W}\int  C(\hat{w}(D),w) p(D|w) \mathrm{d}D.
		\end{split}
	\end{equation}
\end{definition}


\begin{theorem}[Mean Squared Error (MSE)]
	\label{theorem:MSE}
	The expectation of the quadratic cost function (\dfref{def:quadratic_cost}) can be written
	\begin{equation}
		\begin{split}
			\mathbb{E}[C(\hat{w}, w)|w] &= \mathbb{E}[(\hat{w}-w)^2|w]\\ 
			&= \mathbb{E}[(\hat{w}-\mathbb{E}[\hat{w}])^2|w]+(w-\mathbb{E}[\hat{w}])^2\\
			&=\operatorname{Var}[\hat{w}|w]+\operatorname{Bias}[\hat{w}|w]^2\\
		\end{split}
		\label{eq:MSE}
	\end{equation}
	where the bias of the estimator of $\hat{w}$ is defined as follows
	\begin{equation}
		\operatorname{Bias}[\hat{w}|w]\equiv w-\mathbb{E}[\hat{w}].
	\end{equation}
	If $\mathbb{E}[C(\hat{w}, w)|w] \to 0$ as $n \to \infty$, then $\hat{w}$ is a weakly consistent estimator of $w$, i.e., $\hat{w} \xrightarrow{p} w$. There can be different consistent estimates that converge towards $w$ at different speeds. It is desirable for an estimate to be consistent and with small (quadratic) cost, meaning that both the bias and variance of the estimator should be small. In many cases, however, there is bias-variance which means that both cannot be minimized at the same time. 
\end{theorem}

\begin{corollary}[MLE is Approximately Minimax for quadratic Loss]
	\label{cor:MLE_minimax}
	Under certain regularity conditions, the Maximum Likelihood decision rule (MLE) $\hat{w}_{\text{MLE}}$ is approximately minimax for the quadratic cost function (\dfref{def:quadratic_cost}), meaning it approximately minimizes the maximum expected cost\index{Minimax}.
	
	\begin{proof}
		From theorem \thref{theorem:MSE}
		\begin{equation}
			\mathbb{E}[(\hat{w}-w)^2|w] = \operatorname{Var}[\hat{w}|w]+\operatorname{Bias}[\hat{w}|w]^2.
		\end{equation}
		Under the regularity conditions where the MLE is unbiased and has asymptotically minimal variance, the bias term vanish, meaning $\operatorname{Bias}[\hat{w}_{\text{MLE}}|w] = 0$ and the variance term $\operatorname{Var}[\hat{w}_{\text{MLE}}|w]$ is minimized among a class of estimators. Thus, the expected quadratic cost for the MLE can be approximated by
		\begin{equation}
			\begin{split}
				\mathbb{E}[(\hat{w}_{\text{MLE}}-w)^2|w] &\approx \operatorname{Var}[\hat{w}_{\text{MLE}}|w]\\
				&\approx \frac{\operatorname{tr}[\mathcal{I}(w)^{-1}]}{n},
			\end{split}
		\end{equation}
		where \thref{thm:unbiased_mle} was used for the second line. The Cramer-Rao lower bound~\citep{Rao1973Linear} for variance states that 
		\begin{equation}
			\operatorname{Var}[\hat{w}|w]\geq \frac{\text{tr}[\mathcal{I}(w)^{-1}]}{n},
		\end{equation}
		implying that the MLE decision rule acheives the smallest possible variance asymptotically and therefore that 
		\begin{equation}
			\sup_{w\in \Omega_W}\mathbb{E}[(\hat{w}_{\text{MLE}}-w)^2|w]\approx \inf_{\hat{w}} \sup_{w \in \Omega_W} \mathbb{E}[(\hat{w} - w)^2|w],
		\end{equation}
		meaning the MLE decision rule is approximately the minimax decision rule under quadractic cost.
	\end{proof}
\end{corollary}

\begin{example}
	The bias-variance decomposition (\thref{theorem:MSE}) is a concept relevant to Frequentist statistics, where a single point estimate of the parameters is used. This decomposition illustrates the tradeoff between underfitting and overfitting: high bias corresponds to underfitting, while high variance corresponds to overfitting. \newline
	
	In Bayesian statistics, predictions are obtained by integrating over the posterior distribution of parameters, rather than relying on a single point estimate. This integration inherently regularizes the model, mitigating overfitting and underfitting.
\end{example}


\begin{example}
	Take $D_y= \{y_i\}_{i=1}^n$ with $Y_i \sim \mathrm{Ber}(w)$, and let $w\in [0,1]$ be the unknown parameter. Determine the quadratic cost of three different decision rules for estimating $w$: the arithmetic sample mean, the constant $0.5$, and the first observation $y_1$.
	
	\begin{itemize}
		\item Arithmetic mean:
		\begin{equation}
			\hat{w}(D_y) = \frac{1}{n} \sum_{i=1}^n y_i
		\end{equation}
		with
		\begin{equation}
			\begin{split}
				\mathbb{E}[\hat{w}(D_y)|w] &=  \int \hat{w}(D_y) p(D_y|w) \mathrm{d}D_y\\
				& = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[Y_{i}|w]\\
				& = w,\\
				\operatorname{Var}[\hat{w}(D_y)|w,I] & =\int  (\hat{w}(D_y)-\mathbb{E}[\hat{w}(D_y)|w])^2 p(D_y|w) \mathrm{d}D_y\\
				 &= \frac{1}{n^2} \sum_{i=1}^n \operatorname{Var}[Y_{i}|w]\\
				 & = \frac{w(1-w)}{n},\\
				\mathbb{E}[(\hat{w}(D_y)-w)^2|w,I] &= \operatorname{Var}[\hat{w}(D_y)|w]+(\mathbb{E}[\hat{w}(D_y)|w]-w)^2\\
				& = \frac{w(1-w)}{n}.
			\end{split}
		\end{equation}
		
		\item Constant estimate:
		\begin{equation}
			\hat{w} = 0.5
		\end{equation}
		with
		\begin{equation}
			\begin{split}
				\mathbb{E}[\hat{w}|w] &= 0.5,\\
				\operatorname{Var}[\hat{w}|w] &= 0,\\
				\mathbb{E}[(\hat{w}-w)^2|w] &= (0.5 - w)^2.
			\end{split}
		\end{equation}
		
		\item First observation:
		\begin{equation}
			\hat{w}(D_y) = y_1
		\end{equation}
		with
		\begin{equation}
			\begin{split}
				\mathbb{E}[\hat{w}(D_y)|w] &= \mathbb{E}[Y_1|w]\\
				& = w,\\
				\operatorname{Var}[\hat{w}(D_y)|w] &= \operatorname{Var}[Y_1|w] +(\mathbb{E}[\hat{w}(D_y)|w]-w)^2\\
				&= w(1-w),\\
				\mathbb{E}[(\hat{w}(D_y)-w)^2|w] &= w(1-w).
			\end{split}
		\end{equation}
	\end{itemize}
	The arithmetic mean minimizes the quadratic cost over the entire range of $w$, while the constant $0.5$ performs better for specific values of $w$. The cost of using $y_1$ is independent of $n$, making it less favorable as the sample size increases.
\end{example}


\begin{example}
	Take $D_y= \{y_i\}_{i=1}^n$ with $Y_i\sim \mathrm{Ber}(w)$, and let $w\in [0,1]$ be the unknown parameter. Determine the maximum likelihood estimate of $w$.\newline
	
	\noindent In this case
	\begin{equation}
		\begin{split}
			p(D_y|D_x,w) & =p(D_y|w)\\
			& = \prod_{i=1}^nw^{y_i}(1-w)^{1-y_i}.
		\end{split}
	\end{equation}
	Let $l(w)\equiv \ln p(D_y|D_x,w)$, then
	\begin{equation}
		\begin{split}
			\argmax_wl(w) & = \argmax_wp(D_y|w)\\
			&= \argmax_w\ln \bigg(\prod_{i=1}^nw^{y_i}(1-w)^{1-y_i}\bigg)\\
			&=\argmax_w \bigg[\ln w\sum_{i=1}^ny_i + \ln(1-w)\sum_{i=1}^n(1-y_i)\bigg]
		\end{split}
	\end{equation}
	Now 
	\begin{equation}
		\frac{d}{dw}l(w)=\frac{\sum_{i=1}^ny_i}{w}-\frac{n-\sum_{i=1}^ny_i}{1-w}
	\end{equation}
	Requiring the derivative to vanish means the maximum likelihood estimate of $w$ is given by
	\begin{equation}
		\hat{w}_{\text{MLE}}(D_y)=\frac{1}{n}\sum_{i=1}^ny_i.
	\end{equation}
\end{example}
\begin{example}
	Take $D_y= \{y_i\}_{i=1}^n$ with $Y_i\sim \mathrm{Exp}(w)$, and let $w> 0$ be the unknown parameter. Determine the maximum likelihood estimate of $w$.\newline
	
	\noindent In this case
	\begin{equation}
		\begin{split}
			p(D_y|D_x,w)& =p(D_y|w)\\
			& =\prod_{i=1}^nw e^{-w y_i}.
		\end{split}
	\end{equation}
	Let $l(w)\equiv \ln p(D_y|D_x,w)$, then
	\begin{equation}
		\frac{d}{dw}l(w)=\frac{n}{w}-\sum_{i=1}^ny_i
	\end{equation}
	Requiring the derivative to vanish means the maximum likelihood estimate of $w$ is given by
	\begin{equation}
		\hat{w}_{\text{MLE}}(D_y)=\frac{1}{\frac{1}{n}\sum_{i=1}^ny_i}.
	\end{equation}
\end{example}

